"""
Housler - Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½ĞµĞ´Ğ²Ğ¸Ğ¶Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
Ğ’ĞµĞ±-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ñ 3-ÑĞºÑ€Ğ°Ğ½Ğ½Ñ‹Ğ¼ wizard UX
"""

# Load .env before any other imports
from dotenv import load_dotenv
load_dotenv()

from flask import Flask, render_template, request, jsonify, session
import os
import uuid
import logging
import json
from typing import Dict, List, Optional
from datetime import datetime
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from flask_wtf.csrf import CSRFProtect

# Ğ¦ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ
from src.config import settings
from src.config.regions import detect_region_from_url, detect_region_from_address

# Ğ¦ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ€Ğ²Ğ¸ÑÑ‹
from src.services.validation import validate_url, sanitize_string
from src.exceptions import URLValidationError, SSRFError

logging.basicConfig(level=getattr(logging, settings.LOG_LEVEL, logging.INFO))
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARSER SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€:
#   âœ… CianParser (Ğ¦Ğ˜ĞĞ) - Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³ Ğ¸ ĞœĞ¾ÑĞºĞ²Ğ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ parser infrastructure
    from src.parsers import get_global_registry
    from src.parsers.browser_pool import BrowserPool

    # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ñ€ÑĞµÑ€
    parsers_loaded = []

    try:
        from src.parsers import CianParser
        parsers_loaded.append('Ğ¦Ğ˜ĞĞ')
    except ImportError as e:
        logger.warning(f"CianParser Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½: {e}")

    PARSER_REGISTRY_AVAILABLE = True
    logger.info(f"âœ“ Parser available: {', '.join(parsers_loaded) if parsers_loaded else 'Ğ½ĞµÑ‚ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ¾Ğ²'}")

except ImportError as e:
    logger.error(f"Failed to import ParserRegistry: {e}")
    # Fallback Ğ½Ğ° ÑÑ‚Ğ°Ñ€Ñ‹Ğ¹ PlaywrightParser
    try:
        from src.parsers.playwright_parser import PlaywrightParser
        from src.parsers.browser_pool import BrowserPool
        PARSER_REGISTRY_AVAILABLE = False
        logger.warning("âš ï¸ Fallback: Using legacy PlaywrightParser (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¦Ğ˜ĞĞ)")
    except Exception as e2:
        logger.error(f"Playwright also not available: {e2}")
        from src.parsers.simple_parser import SimpleParser
        PARSER_REGISTRY_AVAILABLE = False
        BrowserPool = None

# Check if Playwright is available for PDF generation
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logger.warning("Playwright Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ - PDF ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ±ÑƒĞ´ĞµÑ‚ Ğ·Ğ°Ğ¼ĞµĞ½ĞµĞ½ Ğ½Ğ° Markdown")

from src.analytics.analyzer import RealEstateAnalyzer
from src.analytics.offer_generator import generate_housler_offer
from src.models.property import (
    TargetProperty,
    ComparableProperty,
    AnalysisRequest
)
from src.utils.session_storage import get_session_storage
from src.cache import init_cache, get_cache
from src.utils.duplicate_detector import DuplicateDetector


def safe_error_message(e: Exception, include_details: bool = False) -> str:
    """
    Return safe error message for API responses.

    In production: returns generic message to avoid leaking sensitive data
    In development: returns full error details for debugging

    Args:
        e: The exception to process
        include_details: Force include details (for known safe errors)
    """
    # Known safe error types that have controlled messages
    safe_types = (
        URLValidationError,
        SSRFError,
        ValueError,  # Usually controlled validation errors
    )

    if include_details or isinstance(e, safe_types):
        return str(e)

    # In production, hide internal error details
    if os.getenv('FLASK_ENV') == 'production':
        return "Internal server error. Please try again later."

    # In development, return full error for debugging
    return str(e)


# Task Queue (async operations)
try:
    from src.tasks import init_task_queue
    from src.api import task_api
    TASK_QUEUE_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Task queue not available: {e}")
    TASK_QUEUE_AVAILABLE = False

app = Flask(__name__)

# SECURITY: Secret key from configuration
app.secret_key = settings.SECRET_KEY
if not app.secret_key:
    logger.error("SECRET_KEY not set! Using temporary key for development only.")
    if settings.is_production:
        raise RuntimeError('SECRET_KEY must be set in production environment')
    # Development fallback (will be different on each restart)
    app.secret_key = os.urandom(24)

# SECURITY: CSRF Protection (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Cross-Site Request Forgery)
csrf = CSRFProtect(app)
app.config['WTF_CSRF_TIME_LIMIT'] = None  # Token doesn't expire (session-based)
app.config['WTF_CSRF_SSL_STRICT'] = settings.is_production
app.config['WTF_CSRF_METHODS'] = ['POST', 'PUT', 'PATCH', 'DELETE']
logger.info("CSRF protection enabled")

# SECURITY: Session cookie flags
app.config['SESSION_COOKIE_HTTPONLY'] = True  # Prevent JavaScript access
app.config['SESSION_COOKIE_SAMESITE'] = 'Lax'  # Prevent CSRF via third-party sites
app.config['SESSION_COOKIE_SECURE'] = settings.is_production  # HTTPS only in production
if settings.is_production:
    logger.info("Session cookies: Secure=True, HttpOnly=True, SameSite=Lax")

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Redis ĞºÑÑˆĞ° (Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸Ğ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ°)
property_cache = init_cache(
    host=settings.REDIS_HOST,
    port=settings.REDIS_PORT,
    db=settings.REDIS_DB,
    password=settings.REDIS_PASSWORD,
    namespace=settings.REDIS_NAMESPACE,
    enabled=settings.REDIS_ENABLED
)

# Ğ¥Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğµ ÑĞµÑÑĞ¸Ğ¹ Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹ Redis
session_storage = get_session_storage()

# SECURITY & PERFORMANCE: Browser Pool Ğ´Ğ»Ñ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ² Playwright
# ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ¾Ğ²
# Ğ—Ğ°Ñ‰Ğ¸Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ‚ DoS Ğ°Ñ‚Ğ°Ğº Ğ¸ ÑƒÑ‚ĞµÑ‡ĞµĞº Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸
browser_pool = None
if PARSER_REGISTRY_AVAILABLE and settings.USE_BROWSER_POOL:
    browser_pool = BrowserPool(
        max_browsers=settings.MAX_BROWSERS,
        max_age_seconds=3600,  # 1 Ñ‡Ğ°Ñ
        headless=settings.PARSER_HEADLESS,
        block_resources=True
    )
    browser_pool.start()
    logger.info(f"Browser pool initialized with max_browsers={settings.MAX_BROWSERS}")
else:
    logger.info("Browser pool disabled (for local dev or parsers not available)")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROXY ROTATOR INITIALIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞŸÑ€Ğ¾ĞºÑĞ¸ Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ IP ÑĞµÑ€Ğ²ĞµÑ€Ğ° Ğ¾Ñ‚ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ¾Ğº
proxy_rotator = None
PROXY_ENABLED = os.getenv('PROXY_ENABLED', 'false').lower() == 'true'
PROXY_POOL_FILE = os.getenv('PROXY_POOL_FILE', 'config/proxy_pool.json')

if PROXY_ENABLED and os.path.exists(PROXY_POOL_FILE):
    try:
        from src.parsers.proxy_rotator import ProxyRotator
        
        with open(PROXY_POOL_FILE) as f:
            proxy_pool = json.load(f)
        
        proxy_rotator = ProxyRotator(
            proxies=proxy_pool,
            strategy=os.getenv('PROXY_STRATEGY', 'round_robin'),
            max_failures=int(os.getenv('PROXY_MAX_FAILURES', '3')),
            cooldown_seconds=int(os.getenv('PROXY_COOLDOWN_SECONDS', '300'))
        )
        
        logger.info(f"ğŸ”’ ProxyRotator Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½: {len(proxy_pool)} Ğ¿Ñ€Ğ¾ĞºÑĞ¸")
    except Exception as e:
        logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ProxyRotator: {e}")
        proxy_rotator = None
elif PROXY_ENABLED:
    logger.warning(f"âš ï¸ PROXY_ENABLED=true Ğ½Ğ¾ Ñ„Ğ°Ğ¹Ğ» {PROXY_POOL_FILE} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
else:
    logger.info("ğŸŒ ĞŸÑ€Ğ¾ĞºÑĞ¸ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½ - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARSER REGISTRY INITIALIZATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ğ“Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞµÑÑ‚Ñ€ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ¾Ğ² Ñ ĞºÑÑˆĞµĞ¼
parser_registry = None
if PARSER_REGISTRY_AVAILABLE:
    parser_registry = get_global_registry(cache=property_cache, delay=1.0)
    logger.info(f"âœ“ Parser Registry Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ")
    logger.info(f"  Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸: {', '.join(parser_registry.get_all_sources())}")
else:
    logger.warning("âš ï¸ Parser Registry Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ - Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ fallback")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DUPLICATE DETECTOR INITIALIZATION
# Ğ”ĞµÑ‚ĞµĞºÑ†Ğ¸Ñ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ² Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²
# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸Ğ· Ñ†ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ° (settings)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
duplicate_detector = DuplicateDetector(
    strict_price_tolerance=settings.DUPLICATE_STRICT_PRICE_TOLERANCE,
    probable_price_tolerance=settings.DUPLICATE_PROBABLE_PRICE_TOLERANCE,
    possible_price_tolerance=settings.DUPLICATE_POSSIBLE_PRICE_TOLERANCE,
    area_tolerance=settings.DUPLICATE_AREA_TOLERANCE,
    possible_area_tolerance=settings.DUPLICATE_POSSIBLE_AREA_TOLERANCE
)
logger.info("âœ“ Duplicate Detector Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BLOG ROUTES REGISTRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    from blog_routes import register_blog_routes
    register_blog_routes(app)
    logger.info("âœ“ Blog routes Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹")
except ImportError as e:
    logger.warning(f"âš ï¸ Blog routes Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONTACTS BLUEPRINT REGISTRATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    from src.routes import contacts_bp
    # CSRF exempt Ğ´Ğ»Ñ Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼ (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ñ‡ĞµÑ€ĞµĞ· rate limiting Ğ¸ honeypot)
    csrf.exempt(contacts_bp)
    app.register_blueprint(contacts_bp)
    logger.info("âœ“ Contacts Blueprint Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")
except ImportError as e:
    logger.warning(f"âš ï¸ Contacts Blueprint Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½: {e}")


def get_parser_for_url(url: str, region: str = 'spb', proxy_config: Optional[Dict] = None):
    """
    ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ URL

    Ğ“Ğ¸Ğ±Ñ€Ğ¸Ğ´Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´:
    - Ğ”Ğ»Ñ Ğ¦Ğ˜ĞĞ: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ PlaywrightParser Ñ region Ğ¸ browser_pool (ÑĞ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ)
    - Ğ”Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ parser_registry (Ğ½Ğ¾Ğ²Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ)

    Args:
        url: URL Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ñ
        region: Ğ ĞµĞ³Ğ¸Ğ¾Ğ½ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ¦Ğ˜ĞĞ)
        proxy_config: ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ (Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾)

    Returns:
        ĞŸĞ°Ñ€ÑĞµÑ€ Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ°Ğ¼Ğ¸ parse_detail_page() Ğ¸ search_similar()
    """
    if not PARSER_REGISTRY_AVAILABLE:
        # Fallback: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¹ PlaywrightParser
        from src.parsers.playwright_parser import PlaywrightParser
        return PlaywrightParser(
            headless=True,
            delay=1.0,
            cache=property_cache,
            region=region,
            browser_pool=browser_pool,
            proxy_config=proxy_config
        )

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº
    source = parser_registry.detect_source(url) if parser_registry else None

    if source == 'cian':
        # Ğ”Ğ»Ñ Ğ¦Ğ˜ĞĞ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ÑÑ‚Ğ°Ñ€Ñ‹Ğ¹ PlaywrightParser Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¾Ğ¹
        from src.parsers.playwright_parser import PlaywrightParser
        return PlaywrightParser(
            headless=True,
            delay=1.0,
            cache=property_cache,
            region=region,
            browser_pool=browser_pool,
            proxy_config=proxy_config
        )
    elif source:
        # Ğ”Ğ»Ñ Ğ¾ÑÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ registry
        parser = parser_registry.get_parser(url=url)
        if parser:
            logger.info(f"âœ“ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ´Ğ»Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°: {source}")
            return parser
        else:
            logger.error(f"âŒ ĞŸĞ°Ñ€ÑĞµÑ€ Ğ´Ğ»Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° {source} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
            raise ValueError(f"ĞŸĞ°Ñ€ÑĞµÑ€ Ğ´Ğ»Ñ {source} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
    else:
        # Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½ - Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¦Ğ˜ĞĞ ĞºĞ°Ğº fallback
        logger.warning(f"âš ï¸ Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½ Ğ´Ğ»Ñ URL: {url}, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¦Ğ˜ĞĞ")
        from src.parsers.playwright_parser import PlaywrightParser
        return PlaywrightParser(
            headless=True,
            delay=1.0,
            cache=property_cache,
            region=region,
            browser_pool=browser_pool
        )


# Rate limiting configuration
# SECURITY: ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞºÑĞ¸
import hashlib

def get_rate_limit_key():
    """
    Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ»ÑÑ‡ Ğ´Ğ»Ñ rate limiting

    Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚: IP + User-Agent + Session (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
    Ğ­Ñ‚Ğ¾ Ğ·Ğ°Ñ‚Ñ€ÑƒĞ´Ğ½ÑĞµÑ‚ Ğ¾Ğ±Ñ…Ğ¾Ğ´ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ¸Ğ»Ğ¸ ÑĞ¼ĞµĞ½Ñƒ IP
    """
    # IP Ğ°Ğ´Ñ€ĞµÑ
    ip = get_remote_address()

    # User-Agent
    user_agent = request.headers.get('User-Agent', '')[:200]  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñƒ

    # Session ID (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
    session_id = session.get('id', '')

    # ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¸ Ñ…ÑÑˆĞ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ»Ñ privacy
    combined = f"{ip}:{user_agent}:{session_id}"
    key_hash = hashlib.sha256(combined.encode()).hexdigest()[:16]

    return key_hash

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Redis Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ rate limiting (ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½)
limiter = Limiter(
    app=app,
    key_func=get_rate_limit_key,
    storage_uri=settings.rate_limit_storage_uri,
    default_limits=[settings.RATELIMIT_DEFAULT],
    storage_options={"socket_connect_timeout": 30},
    strategy="moving-window"
)

logger.info(f"Rate limiting initialized with storage: {'redis' if settings.REDIS_ENABLED else 'memory'}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TASK QUEUE INITIALIZATION (Async Operations)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if TASK_QUEUE_AVAILABLE:
    try:
        task_queue = init_task_queue()
        if task_queue:
            logger.info("âœ… Task queue initialized successfully")
            # Register task API blueprint
            app.register_blueprint(task_api)
            logger.info("âœ… Task API endpoints registered")
        else:
            logger.warning("âš ï¸ Task queue initialization failed - running without async support")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize task queue: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PYDANTIC MODELS
# Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Pydantic
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Pydantic models Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
from pydantic import BaseModel, Field, validator, ValidationError as PydanticValidationError

class ManualPropertyInput(BaseModel):
    """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ½ĞµĞ´Ğ²Ğ¸Ğ¶Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸"""
    address: str = Field(..., min_length=5, max_length=500, description="ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ°Ğ´Ñ€ĞµÑ")
    price_raw: float = Field(..., gt=0, lt=1_000_000_000_000, description="Ğ¦ĞµĞ½Ğ° Ğ² Ñ€ÑƒĞ±Ğ»ÑÑ…")
    total_area: float = Field(..., gt=1, lt=10000, description="ĞĞ±Ñ‰Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ² Ğ¼Â²")
    rooms: str = Field(..., description="ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚")
    floor: str = Field(default='', max_length=20, description="Ğ­Ñ‚Ğ°Ğ¶ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ N/M")
    living_area: Optional[float] = Field(default=None, gt=0, lt=10000, description="Ğ–Ğ¸Ğ»Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ² Ğ¼Â²")
    kitchen_area: Optional[float] = Field(default=None, gt=0, lt=500, description="ĞŸĞ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ ĞºÑƒÑ…Ğ½Ğ¸ Ğ² Ğ¼Â²")
    repair_level: str = Field(default='ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ', max_length=50)
    view_type: str = Field(default='ÑƒĞ»Ğ¸Ñ†Ğ°', max_length=50)

    @validator('address')
    def validate_address(cls, v):
        """Ğ¡Ğ°Ğ½Ğ¸Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°Ğ´Ñ€ĞµÑĞ°"""
        v = sanitize_string(v, max_length=500)
        if not v or len(v) < 5:
            raise ValueError('ĞĞ´Ñ€ĞµÑ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹')
        # Ğ‘Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒĞµĞ¼ SQL injection Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹
        dangerous_patterns = ['<script', 'javascript:', 'onerror=', 'onclick=', 'drop table', 'union select']
        v_lower = v.lower()
        for pattern in dangerous_patterns:
            if pattern in v_lower:
                raise ValueError(f'ĞĞ´Ñ€ĞµÑ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ½ĞµĞ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ñ‹')
        return v

    @validator('rooms')
    def validate_rooms(cls, v):
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚"""
        allowed_values = ['Ğ¡Ñ‚ÑƒĞ´Ğ¸Ñ', '1', '2', '3', '4', '5', '5+']
        if v not in allowed_values:
            raise ValueError(f'ĞĞµĞ´Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚: {v}. Ğ Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ñ‹: {allowed_values}')
        return v

    @validator('living_area')
    def validate_living_area(cls, v, values):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‡Ñ‚Ğ¾ Ğ¶Ğ¸Ğ»Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ½Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¾Ğ±Ñ‰ĞµĞ¹"""
        if v and 'total_area' in values and v > values['total_area']:
            raise ValueError('Ğ–Ğ¸Ğ»Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¾Ğ±Ñ‰ĞµĞ¹')
        return v

    @validator('kitchen_area')
    def validate_kitchen_area(cls, v, values):
        """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ‡Ñ‚Ğ¾ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ ĞºÑƒÑ…Ğ½Ğ¸ Ğ½Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¾Ğ±Ñ‰ĞµĞ¹"""
        if v and 'total_area' in values and v > values['total_area']:
            raise ValueError('ĞŸĞ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ ĞºÑƒÑ…Ğ½Ğ¸ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ¾Ğ±Ñ‰ĞµĞ¹')
        return v


# Timeout decorator Ğ´Ğ»Ñ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ñ‹ Ğ¾Ñ‚ Ğ·Ğ°Ğ²Ğ¸ÑĞ°ÑÑ‰Ğ¸Ñ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹
import signal
import threading
from contextlib import contextmanager
from functools import wraps
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError

class TimeoutError(Exception):
    """Exception raised when operation times out"""
    pass


@contextmanager
def timeout_context(seconds: int, error_message: str = 'Operation timed out'):
    """
    Context manager Ğ´Ğ»Ñ Ğ¶ĞµÑÑ‚ĞºĞ¾Ğ³Ğ¾ timeout Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹
    Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ² Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ (signal) Ğ¸ Ğ² worker threads (ThreadPoolExecutor)

    Args:
        seconds: ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ Ğ² ÑĞµĞºÑƒĞ½Ğ´Ğ°Ñ…
        error_message: Ğ¡Ğ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¾Ğ± Ğ¾ÑˆĞ¸Ğ±ĞºĞµ

    Raises:
        TimeoutError: ĞµÑĞ»Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ğ»Ğ° timeout

    Example:
        with timeout_context(60):
            long_running_operation()
    """
    is_main_thread = threading.current_thread() is threading.main_thread()

    if is_main_thread:
        # Ğ’ Ğ³Ğ»Ğ°Ğ²Ğ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ signal (Ğ´Ğ»Ñ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸)
        def timeout_handler(signum, frame):
            raise TimeoutError(error_message)

        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(seconds)

        try:
            yield
        finally:
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_handler)
    else:
        # Ğ’ worker threads (Gunicorn) Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ threading timeout
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ°
        timeout_event = threading.Event()
        exception_holder = [None]

        def check_timeout():
            if not timeout_event.wait(seconds):
                # Ğ¢Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚ Ğ¸ÑÑ‚ĞµĞº, Ğ½Ğ¾ Ğ¼Ñ‹ Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµĞ¼ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ñ‚Ğ¾Ğº Ğ¸Ğ·Ğ²Ğ½Ğµ
                # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸
                logger.warning(f"Timeout ({seconds}s) exceeded: {error_message}")

        # Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ñ‚Ğ°Ğ¹Ğ¼ĞµÑ€ Ğ² Ñ„Ğ¾Ğ½Ğµ
        timer_thread = threading.Thread(target=check_timeout, daemon=True)
        timer_thread.start()

        try:
            yield
        finally:
            # Ğ¡Ğ¸Ğ³Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°
            timeout_event.set()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECURITY HEADERS (CRITICAL FIX)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.after_request
def set_security_headers(response):
    """
    Apply security headers to all responses

    Protection against:
    - XSS (Content-Security-Policy)
    - Clickjacking (X-Frame-Options)
    - MIME sniffing (X-Content-Type-Options)
    - Information leakage (Referrer-Policy)
    """

    # Content Security Policy - Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ XSS
    response.headers['Content-Security-Policy'] = (
        "default-src 'self'; "
        "script-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net https://unpkg.com https://*.yandex.ru https://*.yandex.com https://yastatic.net; "
        "style-src 'self' 'unsafe-inline' https://cdn.jsdelivr.net https://fonts.googleapis.com; "
        "img-src 'self' data: https: http:; "
        "font-src 'self' https://cdn.jsdelivr.net https://fonts.gstatic.com; "
        "connect-src 'self' https://*.yandex.ru https://*.yandex.com https://*.yandex.md wss://*.yandex.ru wss://*.yandex.com; "
        "frame-ancestors 'none'; "
        "base-uri 'self'; "
        "form-action 'self';"
    )

    # Ğ—Ğ°Ğ¿Ñ€ĞµÑ‚ Ğ½Ğ° MIME-sniffing
    response.headers['X-Content-Type-Options'] = 'nosniff'

    # Ğ—Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ clickjacking
    response.headers['X-Frame-Options'] = 'DENY'

    # XSS Protection (legacy, Ğ½Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ€Ñ‹Ñ… Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ¾Ğ²)
    response.headers['X-XSS-Protection'] = '1; mode=block'

    # Referrer Policy - Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ URL Ğ¿Ñ€Ğ¸ Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ°Ñ…
    response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'

    # HSTS - Ğ¿Ñ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ HTTPS (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² production)
    if os.getenv('FLASK_ENV') == 'production':
        response.headers['Strict-Transport-Security'] = 'max-age=31536000; includeSubDomains'

    return response


@app.route('/')
def index():
    """Landing page - Agency website"""
    # Load recent blog posts for preview
    recent_posts = []
    try:
        from blog_database import BlogDatabase
        blog_db = BlogDatabase()
        recent_posts = blog_db.get_recent_posts(limit=4)
    except Exception as e:
        logger.warning(f"Could not load blog posts: {e}")

    return render_template('index.html', recent_posts=recent_posts)


@app.route('/consent')
def consent():
    """Consent page for receiving promotional materials"""
    return render_template('consent.html')


@app.route('/doc/clients/politiki/')
@app.route('/doc/clients/politiki')
def privacy_policy():
    """Privacy policy page"""
    return render_template('privacy_policy.html')


@app.route('/doc/clients/soglasiya/advertising-agreement/')
@app.route('/doc/clients/soglasiya/advertising-agreement')
def advertising_consent_clients():
    """Advertising consent page for clients"""
    return render_template('advertising_consent.html', canonical_path='/doc/clients/soglasiya/advertising-agreement/')


@app.route('/doc/realtors/soglasiya/advertising-agreement/')
@app.route('/doc/realtors/soglasiya/advertising-agreement')
def advertising_consent_realtors():
    """Advertising consent page for realtors"""
    return render_template('advertising_consent.html', canonical_path='/doc/realtors/soglasiya/advertising-agreement/')


@app.route('/doc/agencies/soglasiya/advertising-agreement/')
@app.route('/doc/agencies/soglasiya/advertising-agreement')
def advertising_consent_agencies():
    """Advertising consent page for agencies"""
    return render_template('advertising_consent.html', canonical_path='/doc/agencies/soglasiya/advertising-agreement/')


@app.route('/doc')
@app.route('/doc/')
def docs_index():
    """Documents main page"""
    return render_template('docs_index.html')


@app.route('/doc/clients')
@app.route('/doc/clients/')
def docs_clients():
    """Documents for clients"""
    return render_template('docs_clients.html')


@app.route('/doc/realtors')
@app.route('/doc/realtors/')
def docs_realtors():
    """Documents for realtors"""
    return render_template('docs_realtors.html')


@app.route('/doc/agents')
@app.route('/doc/agents/')
def docs_agents():
    """Documents for agencies"""
    return render_template('docs_agents.html')


# Mining page translations
MINING_TRANSLATIONS = {
    'ru': {
        'meta_title': 'Ğ“Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ°',
        'meta_description': 'Ğ¡Ğ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ ĞºÑ€Ğ¸Ğ¿Ñ‚Ğ¾Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ°. 44 Ğ“ĞŸĞ£, 14.08 ĞœĞ’Ñ‚ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‚Ğ°Ñ€Ğ¸Ñ„ $0.05/ĞºĞ’Ñ‚Ñ‡.',
        'hero_title': 'Ğ¡Ğ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ°',
        'hero_alt': 'Ğ“Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ°',
        'summary_title': 'ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹',
        'summary_price': 'Ğ¡Ñ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ°ĞºÑ‚Ğ°: 5.3 Ğ¼Ğ»Ğ½ USD',
        'summary_1': '44 Ğ³Ğ°Ğ·Ğ¾Ğ¿Ğ¾Ñ€ÑˆĞ½ĞµĞ²Ñ‹Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ (Ğ“ĞŸĞ£) + 44 ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ğ°Ğ²Ñ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¸Ğ²Ğ° Ğ¼Ğ°ÑĞ»Ğ° Ğ·Ğ° 5.3 Ğ¼Ğ»Ğ½ USD',
        'summary_2': 'ĞĞ±Ñ‰Ğ°Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ°Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ: 14.08 ĞœĞ’Ñ‚, ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ: 17.6 ĞœĞ’Ñ‚',
        'summary_3': 'Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ‚Ğ°Ñ€Ğ¸Ñ„ Ğ½Ğ° ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞ½ĞµÑ€Ğ³Ğ¸Ñ: 0.05 USD/ĞºĞ’Ñ‚Ñ‡',
        'summary_4': 'Ğ ĞµÑÑƒÑ€Ñ Ğ³Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¼ĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ: Ğ±Ğ¾Ğ»ĞµĞµ 30 Ğ»ĞµÑ‚',
        'summary_5': 'Ğ Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½Ñ‹ Ğ² Ñ‚Ğ°Ñ€Ğ¸Ñ„',
        'summary_alt': 'ĞĞ±Ğ·Ğ¾Ñ€ Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³-Ñ„ĞµÑ€Ğ¼Ñ‹',
        'problem_title': 'ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°',
        'problem_1': 'Ğ”ĞµÑ„Ğ¸Ñ†Ğ¸Ñ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… ÑĞ»ĞµĞºÑ‚Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹',
        'problem_2': 'Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğµ Ñ‚Ğ°Ñ€Ğ¸Ñ„Ñ‹ Ğ½Ğ° ÑĞµÑ‚ĞµĞ²ÑƒÑ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞ½ĞµÑ€Ğ³Ğ¸Ñ',
        'problem_3': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸',
        'problem_4': 'ĞĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ±Ğ¸Ğ·Ğ½ĞµÑ-Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ…Ğ¾ÑÑ‚Ğ¸Ğ½Ğ³-Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²',
        'problem_5': 'Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ ÑÑ‚Ğ¾Ñ€Ğ¾Ğ½Ğ½Ğ¸Ñ… Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ñ‰Ğ¸ĞºĞ¾Ğ² ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸',
        'problem_6': 'ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ',
        'problem_7': 'ĞĞµĞ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹',
        'solution_title': 'Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ',
        'solution_1': 'ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¾Ñ‚ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞµÑ‚Ğ¸',
        'solution_2': 'ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞµĞ±ĞµÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸',
        'solution_3': 'ĞŸÑ€ÑĞ¼Ğ¾Ğ¹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¿Ñ€Ğ¸Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¼Ñƒ Ğ³Ğ°Ğ·Ñƒ',
        'solution_4': 'ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒĞµĞ¼Ñ‹Ğµ Ğ¸ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹',
        'solution_5': 'ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°',
        'solution_6': 'Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ ĞºĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ñ Ğ½Ğ°Ğ´ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°Ğ¼Ğ¸',
        'solution_7': 'ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ½Ñ‚Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ°Ğ¹Ğ½Ğ¸Ğ½Ğ³Ğ°',
        'equipment_title': 'ĞĞ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (Ğ“ĞŸĞ£)',
        'kw': 'ĞºĞ’Ñ‚',
        'units': 'ĞµĞ´.',
        'equipment_nominal': 'Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ',
        'equipment_effective': 'Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ°Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ',
        'equipment_note': 'Ğ“ĞŸĞ£ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° 320 ĞºĞ’Ñ‚ (80% Ğ¾Ñ‚ Ğ½Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ»Ğ°), Ğ¿Ğ¾ÑĞºĞ¾Ğ»ÑŒĞºÑƒ, ĞºĞ°Ğº Ğ¸ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ĞµĞ»ÑŒ, Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ¿Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ±ĞµĞ· ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ½Ğ¾ÑĞ°. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ² Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ Ğ½Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ, ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ´Ğ¾Ğ»Ğ³Ğ¾Ğ²ĞµÑ‡Ğ½Ğ¾ÑÑ‚ÑŒ.',
        'equipment_fuel': 'ĞĞ¸Ğ·ĞºĞ¸Ğ¹ Ñ€Ğ°ÑÑ…Ğ¾Ğ´ Ñ‚Ğ¾Ğ¿Ğ»Ğ¸Ğ²Ğ° (Ğ¾Ñ‚ 0.27 Ğ¼3/ĞºĞ’Ñ‚Ñ‡)',
        'equipment_resource': 'Ğ—Ğ°Ğ²Ğ¾Ğ´ÑĞºĞ¾Ğ¹ Ñ€ĞµÑÑƒÑ€Ñ Ğ´Ğ¾ ĞºĞ°Ğ¿Ñ€ĞµĞ¼Ğ¾Ğ½Ñ‚Ğ°: 5 Ğ»ĞµÑ‚',
        'equipment_maintenance': 'Ğ˜Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ: Ğ´Ğ¾ 2000 Ğ¼Ğ¾Ñ‚Ğ¾Ñ‡Ğ°ÑĞ¾Ğ²',
        'infra_title': 'Ğ˜Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°',
        'infra_1': 'ĞšĞ¾Ğ½Ñ‚ĞµĞ¹Ğ½ĞµÑ€Ñ‹ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑÑ‚ÑÑ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ¼',
        'infra_2': 'ASIC-Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ğ°Ğ´Ğ»ĞµĞ¶Ğ¸Ñ‚ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ',
        'infra_3': 'ĞĞ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ',
        'infra_4': 'Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° 24/7',
        'infra_5': 'Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚-ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ',
        'infra_6': 'ĞÑ…Ñ€Ğ°Ğ½ÑĞµĞ¼Ñ‹Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚',
        'infra_7': 'Ğ—ĞµĞ¼ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ‡Ğ°ÑÑ‚Ğ¾Ğº Ğ² ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°',
        'infra_8': 'Ğ—Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°',
        'gas_title': 'Ğ“Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğµ Ğ¼ĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ',
        'gas_wells': 'Ğ³Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… ÑĞºĞ²Ğ°Ğ¶Ğ¸Ğ½ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ğ°',
        'gas_cost': 'ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ³Ğ°Ğ·Ğ°',
        'gas_years': 'Ğ»ĞµÑ‚ Ğ·Ğ°Ğ¿Ğ°ÑĞ¾Ğ² Ğ¼ĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ñ',
        'gas_1': 'Ğ˜Ğ½Ğ´ĞµĞºÑĞ°Ñ†Ğ¸Ñ Ñ†ĞµĞ½ Ğ½Ğ° Ğ³Ğ°Ğ· Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ¤ĞĞ¡',
        'gas_2': 'ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ¾Ğ±Ñ‹Ñ‡Ğ° Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹: ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ñ‹ Ğ½Ğ° Ğ¼ĞµÑÑ‚Ğ¾Ñ€Ğ¾Ğ¶Ğ´ĞµĞ½Ğ¸Ğµ Ğ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ³Ğ°Ğ·Ğ°',
        'agreements_title': 'ĞĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹',
        'agreement_1_title': 'Ğ”Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ“ĞŸĞ£',
        'agreement_1_text': 'ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ²ÑĞµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ, ÑÑ€Ğ¾ĞºĞ¸ Ğ¸ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ´ÑƒÑ€Ñ‹ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ² ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ³Ğ°Ğ·Ğ¾Ğ¿Ğ¾Ñ€ÑˆĞ½ĞµĞ²Ñ‹Ñ… ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¾Ğº.',
        'agreement_2_title': 'Ğ”Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ Ğ°Ñ€ĞµĞ½Ğ´Ñ‹ Ğ“ĞŸĞ£',
        'agreement_2_text': 'ĞŸĞ¾ÑĞ»Ğµ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ¸ Ğ“ĞŸĞ£ Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ĞµĞ»ÑŒ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ°Ñ€ĞµĞ½Ğ´Ñƒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€Ñƒ. Ğ”Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ° ÑĞºÑĞ¿Ğ»ÑƒĞ°Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ.',
        'agreement_3_title': 'Ğ”Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ ÑĞ½ĞµÑ€Ğ³Ğ¾ÑĞ½Ğ°Ğ±Ğ¶ĞµĞ½Ğ¸Ñ',
        'agreement_3_text': 'ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸, Ñ‚Ğ°Ñ€Ğ¸Ñ„Ñ‹, Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ¾Ğ² Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ.',
        'agreement_4_title': 'Ğ”Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ Ñ…Ğ¾ÑÑ‚Ğ¸Ğ½Ğ³Ğ°',
        'agreement_4_text': 'Ğ ĞµĞ³ÑƒĞ»Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ASIC-Ğ¼Ğ°Ğ¹Ğ½ĞµÑ€Ğ¾Ğ²: ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ, Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ Ğº Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ, ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ñ‹ Ğ°Ğ¿Ñ‚Ğ°Ğ¹Ğ¼Ğ°, Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ, Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ñ‚ĞµÑ…Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ.',
        'legal_title': 'Ğ®Ñ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ',
        'legal_highlight': 'Ğ“ĞŸĞ£ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾ĞºÑƒĞ¿Ğ°Ñ‚ĞµĞ»Ñ Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ»ĞµĞºÑ‚Ñ€Ğ¾ÑĞ½ĞµÑ€Ğ³Ğ¸Ğ¸.',
        'legal_1': 'ĞĞ°ÑˆĞ° ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ Ğ² ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ğ¾Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾Ğ¼. ĞœÑ‹ Ğ¸Ğ¼ĞµĞµĞ¼ Ğ²ÑĞµ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ñ€Ğ°Ğ·Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ, Ğ»Ğ¸Ñ†ĞµĞ½Ğ·Ğ¸Ğ¸, Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ñ‹ Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ.',
        'legal_2': 'Ğ’ÑĞµ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ»ĞµĞ³Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾, Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ¾ Ñ‚Ğ°Ğ¼Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğµ Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ Ğ² Ğ Ğ¤ Ğ¸ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ»ĞµĞ³Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ² Ğ Ğ¾ÑÑĞ¸Ğ¸.',
        'legal_3': 'ĞœÑ‹ Ğ¾Ğ±ĞµÑĞ¿ĞµÑ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğ½ÑƒÑ ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ÑĞµÑ… Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ².',
        'cta_title': 'Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ¾Ğ±ÑÑƒĞ´Ğ¸Ñ‚ÑŒ?',
        'cta_subtitle': 'ĞÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ğ·Ğ°ÑĞ²ĞºÑƒ Ğ¸ Ğ¼Ñ‹ ÑĞ²ÑĞ¶ĞµĞ¼ÑÑ Ñ Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ´ĞµÑ‚Ğ°Ğ»ĞµĞ¹',
        'cta_button': 'ĞÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°ÑĞ²ĞºÑƒ',
        'footer_rights': 'Ğ’ÑĞµ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ·Ğ°Ñ‰Ğ¸Ñ‰ĞµĞ½Ñ‹.',
        'form_title': 'ĞÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ·Ğ°ÑĞ²ĞºÑƒ',
        'form_name': 'Ğ˜Ğ¼Ñ',
        'form_name_placeholder': 'ĞšĞ°Ğº Ğº Ğ²Ğ°Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‰Ğ°Ñ‚ÑŒÑÑ?',
        'form_phone': 'Ğ¢ĞµĞ»ĞµÑ„Ğ¾Ğ½',
        'form_message': 'ĞšĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹',
        'form_message_placeholder': 'ĞĞ¿Ğ¸ÑˆĞ¸Ñ‚Ğµ Ğ²Ğ°Ñˆ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¸Ğ»Ğ¸ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹...',
        'form_contact_method': 'Ğ£Ğ´Ğ¾Ğ±Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± ÑĞ²ÑĞ·Ğ¸',
        'form_call': 'ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ½Ğ¸Ñ‚ÑŒ',
        'form_submit': 'ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑŒ',
        'form_success_title': 'Ğ—Ğ°ÑĞ²ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ°!',
        'form_success_text': 'ĞœÑ‹ ÑĞ²ÑĞ¶ĞµĞ¼ÑÑ Ñ Ğ²Ğ°Ğ¼Ğ¸ Ğ² Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞµ Ğ²Ñ€ĞµĞ¼Ñ.',
        'form_error': 'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ·Ğ¶Ğµ.'
    },
    'en': {
        'meta_title': 'Gas-Powered Generation for Mining',
        'meta_description': 'Own gas-powered generation for cryptocurrency mining. 44 GPU units, 14.08 MW effective capacity, $0.05/kWh electricity tariff.',
        'hero_title': 'Own gas-powered generation for mining',
        'hero_alt': 'Gas-powered generation facility',
        'summary_title': 'Executive Summary',
        'summary_price': 'Contract value: USD 5.3 million',
        'summary_1': '44 gas-piston units (GPU) + 44 automatic oil refill systems for USD 5.3 million',
        'summary_2': 'Total effective operating capacity: 14.08 MW, installed capacity: 17.6 MW',
        'summary_3': 'Effective tariff for generated electricity: 0.05 USD/kWh',
        'summary_4': 'Gas field resource: over 30 years',
        'summary_5': 'Placement and ongoing maintenance are included in the tariff',
        'summary_alt': 'Mining facility overview',
        'problem_title': 'Problem',
        'problem_1': 'Shortage of accessible electrical power',
        'problem_2': 'High grid electricity tariffs',
        'problem_3': 'Limited available capacities',
        'problem_4': 'Hosting providers have unstable business models',
        'problem_5': 'Energy dependence on third-party suppliers',
        'problem_6': 'Limited opportunities for scaling',
        'problem_7': 'Unpredictable long-term operating conditions',
        'solution_title': 'Solution',
        'solution_1': 'Full autonomy from the power grid',
        'solution_2': 'Minimal electricity production cost',
        'solution_3': 'Direct access to natural gas',
        'solution_4': 'Predictable and controlled operating expenses',
        'solution_5': 'Scalable infrastructure',
        'solution_6': 'High level of control over all critical systems',
        'solution_7': 'Maximized mining profitability',
        'equipment_title': 'Equipment (GPU - Gas Piston Unit)',
        'kw': 'kW',
        'units': 'units',
        'equipment_nominal': 'nominal (nameplate) capacity',
        'equipment_effective': 'effective operating capacity',
        'equipment_note': 'The GPU operates at 320 kW (80% of nominal capacity) because, like an automobile engine, it cannot run at peak power continuously without accelerated wear. Operating in an optimal load range ensures stability, efficiency, and long-term durability.',
        'equipment_fuel': 'Low fuel consumption (from 0.27 m3/kWh)',
        'equipment_resource': 'Factory-rated operational resource: 5 years',
        'equipment_maintenance': 'Maintenance interval: up to 2,000 operating hours',
        'infra_title': 'Project Infrastructure',
        'infra_1': 'Operator-provided containers',
        'infra_2': 'Client-owned ASIC equipment',
        'infra_3': 'Equipment maintenance and monitoring',
        'infra_4': '24/7 technical support',
        'infra_5': 'Stable internet connectivity',
        'infra_6': 'Guarded facility',
        'infra_7': 'Land plot owned by the operator',
        'infra_8': 'Secure and protected infrastructure',
        'gas_title': 'Gas Field and Fuel Supply',
        'gas_wells': 'gas wells owned by operator',
        'gas_cost': 'cost of purified gas',
        'gas_years': 'years of gas field reserves',
        'gas_1': 'Gas price indexation regulated by Federal Antimonopoly Service (FAS)',
        'gas_2': 'Official production with full documentation: certificates for field and gas quality provided',
        'agreements_title': 'Required Agreements',
        'agreement_1_title': 'GPU Supply Agreement',
        'agreement_1_text': 'Defines all terms, timelines, and procedures for purchase, production, delivery, and commissioning of gas-piston units.',
        'agreement_2_title': 'GPU Lease Agreement',
        'agreement_2_text': 'After purchasing GPUs, buyer leases equipment to operator. Regulates responsibility for operation, maintenance, and preservation.',
        'agreement_3_title': 'Power Supply Agreement',
        'agreement_3_text': 'Specifies terms for electricity supply, effective tariff, billing procedures, and consumption reporting.',
        'agreement_4_title': 'Hosting Agreement',
        'agreement_4_text': 'Covers placement of ASIC miners: hosting conditions, infrastructure access, uptime standards, security, and technical support.',
        'legal_title': 'Legal Compliance',
        'legal_highlight': 'The GPUs become the property of the buyer and are used to generate electricity.',
        'legal_1': 'Our company operates fully in compliance with the law. We possess all required permits, licenses, contracts, and documentation.',
        'legal_2': 'All equipment has been legally imported, customs-cleared in the Russian Federation, and holds valid legal status in Russia.',
        'legal_3': 'We maintain full legal transparency in all operational and financial processes.',
        'cta_title': 'Ready to discuss?',
        'cta_subtitle': 'Leave a request and we will contact you to discuss the details',
        'cta_button': 'Get in Touch',
        'footer_rights': 'All rights reserved.',
        'form_title': 'Get in Touch',
        'form_name': 'Name',
        'form_name_placeholder': 'How should we address you?',
        'form_phone': 'Phone',
        'form_message': 'Message',
        'form_message_placeholder': 'Describe your request or questions...',
        'form_contact_method': 'Preferred contact method',
        'form_call': 'Call',
        'form_submit': 'Submit',
        'form_success_title': 'Request sent!',
        'form_success_text': 'We will contact you shortly.',
        'form_error': 'Error sending. Please try again later.'
    }
}


@app.route('/mining')
@app.route('/mining/')
def mining_en():
    """Gas-powered generation for mining page (English)"""
    return render_template('mining.html', lang='en', t=MINING_TRANSLATIONS['en'])


@app.route('/mining/ru')
@app.route('/mining/ru/')
def mining_ru():
    """Gas-powered generation for mining page (Russian)"""
    return render_template('mining.html', lang='ru', t=MINING_TRANSLATIONS['ru'])


@app.route('/health', methods=['GET'])
@limiter.exempt  # Health check Ğ½Ğµ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°Ñ‚ÑŒÑÑ - Docker healthcheck ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 30 ÑĞµĞº
def health_check():
    """
    Health check endpoint Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°

    ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚:
    - Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
    - Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ Redis ĞºÑÑˆĞ°
    - Ğ¡Ğ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ session storage
    - Ğ’ĞµÑ€ÑĞ¸Ñ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ

    Returns:
        200 OK ĞµÑĞ»Ğ¸ Ğ²ÑĞµ Ğ² Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞµ
        503 Service Unavailable ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹
    """
    health_status = {
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '2.0.0',  # Ğ’ĞµÑ€ÑĞ¸Ñ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğ¹
        'components': {}
    }

    all_healthy = True

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ĞºÑÑˆĞ°
    try:
        cache_health = property_cache.health_check()
        cache_stats = property_cache.get_stats()
        health_status['components']['redis_cache'] = {
            'status': 'healthy' if cache_health else 'degraded',
            'available': cache_health,
            'stats': cache_stats
        }
        if not cache_health and property_cache.enabled:
            # Ğ•ÑĞ»Ğ¸ ĞºÑÑˆ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½, Ğ½Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ - warning, Ğ½Ğ¾ Ğ½Ğµ critical
            health_status['components']['redis_cache']['status'] = 'degraded'
            health_status['status'] = 'degraded'
    except Exception as e:
        health_status['components']['redis_cache'] = {
            'status': 'unhealthy',
            'error': str(e)
        }
        # ĞšÑÑˆ - Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚
        if health_status['status'] != 'unhealthy':
            health_status['status'] = 'degraded'

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° session storage
    try:
        # ĞŸÑ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ñ‚ĞµÑÑ‚Ğ¾Ğ²ÑƒÑ ÑĞµÑÑĞ¸Ñ
        test_session_id = '_health_check_test'
        session_storage.set(test_session_id, {'test': True})
        test_data = session_storage.get(test_session_id)
        session_storage.delete(test_session_id)

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸Ñ‰Ğ°
        storage_stats = session_storage.get_stats()

        health_status['components']['session_storage'] = {
            'status': 'healthy',
            'type': type(session_storage).__name__,
            'backend': storage_stats['backend'],
            'total_sessions': storage_stats['total_sessions'],
            'hit_rate_percent': storage_stats['hit_rate_percent']
        }
    except Exception as e:
        health_status['components']['session_storage'] = {
            'status': 'unhealthy',
            'error': str(e)
        }
        all_healthy = False
        health_status['status'] = 'unhealthy'

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
    try:
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ°ĞºĞ¸Ğµ Ğ¿Ğ°Ñ€ÑĞµÑ€Ñ‹ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹
        available_parsers = []
        try:
            from src.parsers import CianParser
            available_parsers.append('CianParser')
        except ImportError:
            pass

        try:
            from src.parsers.playwright_parser import PlaywrightParser
            available_parsers.append('PlaywrightParser')
        except ImportError:
            pass

        try:
            from src.parsers.simple_parser import SimpleParser
            available_parsers.append('SimpleParser')
        except ImportError:
            pass

        if available_parsers:
            health_status['components']['parser'] = {
                'status': 'healthy',
                'available_parsers': available_parsers
            }
        else:
            health_status['components']['parser'] = {
                'status': 'unhealthy',
                'error': 'No parsers available'
            }
            all_healthy = False
            health_status['status'] = 'unhealthy'
    except Exception as e:
        # Parser Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ - ÑÑ‚Ğ¾ Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾, ĞµÑÑ‚ÑŒ fallback
        health_status['components']['parser'] = {
            'status': 'degraded',
            'error': str(e)
        }
        # ĞĞ• ÑƒÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ all_healthy = False, Ğ¿Ğ°Ñ€ÑĞµÑ€ Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµĞ½
        if health_status['status'] == 'healthy':
            health_status['status'] = 'degraded'

    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° browser pool
    if browser_pool:
        try:
            pool_stats = browser_pool.get_stats()
            health_status['components']['browser_pool'] = {
                'status': 'healthy',
                'pool_size': pool_stats['pool_size'],
                'max_browsers': pool_stats['max_browsers'],
                'browsers_in_use': pool_stats['browsers_in_use'],
                'browsers_free': pool_stats['browsers_free']
            }
        except Exception as e:
            health_status['components']['browser_pool'] = {
                'status': 'degraded',
                'error': str(e)
            }
            if health_status['status'] != 'unhealthy':
                health_status['status'] = 'degraded'

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ HTTP ÑÑ‚Ğ°Ñ‚ÑƒÑ
    if health_status['status'] == 'healthy':
        http_status = 200
    elif health_status['status'] == 'degraded':
        http_status = 200  # Degraded, Ğ½Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚
    else:
        http_status = 503  # Service Unavailable

    return jsonify(health_status), http_status


@app.route('/api/csrf-token', methods=['GET'])
def get_csrf_token():
    """
    SECURITY: Endpoint Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ CSRF Ñ‚Ğ¾ĞºĞµĞ½Ğ°

    Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ CSRF token Ğ´Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² AJAX Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ…

    Returns:
        JSON Ñ CSRF Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ¼
    """
    from flask_wtf.csrf import generate_csrf
    token = generate_csrf()
    return jsonify({'csrf_token': token})


@app.route('/metrics', methods=['GET'])
@limiter.exempt  # Metrics endpoint Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ° - Ğ½Ğµ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼
def metrics():
    """
    Prometheus-compatible metrics endpoint

    Returns:
        ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ Prometheus
    """
    lines = []

    # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
    lines.append('# HELP housler_up Application is running')
    lines.append('# TYPE housler_up gauge')
    lines.append('housler_up 1')

    # ĞšÑÑˆ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸
    try:
        cache_stats = property_cache.get_stats()
        if cache_stats.get('available'):
            lines.append('# HELP housler_cache_hit_rate Cache hit rate percentage')
            lines.append('# TYPE housler_cache_hit_rate gauge')
            lines.append(f"housler_cache_hit_rate {cache_stats.get('hit_rate', 0)}")

            lines.append('# HELP housler_cache_keys_total Total number of cached keys')
            lines.append('# TYPE housler_cache_keys_total gauge')
            lines.append(f"housler_cache_keys_total {cache_stats.get('total_keys', 0)}")
    except:
        pass

    return '\n'.join(lines) + '\n', 200, {'Content-Type': 'text/plain'}


@app.route('/api/admin/proxy-stats', methods=['GET'])
@limiter.limit("10 per minute")  # Strict limit to prevent API key brute force
def proxy_stats():
    """
    Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸)

    Headers:
        X-Admin-Key: <ADMIN_API_KEY from .env>

    Returns:
        JSON Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¾Ğ¹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ¿Ñ€Ğ¾ĞºÑĞ¸
    """
    # Check admin authentication (same pattern as /api/cache/clear)
    admin_key = os.environ.get('ADMIN_API_KEY')
    provided_key = request.headers.get('X-Admin-Key')

    if not admin_key:
        logger.warning("ADMIN_API_KEY not configured, proxy-stats disabled")
        return jsonify({
            'status': 'error',
            'message': 'Admin API not configured'
        }), 503

    if not provided_key or provided_key != admin_key:
        logger.warning(f"Unauthorized proxy-stats attempt from IP: {request.remote_addr}")
        return jsonify({
            'status': 'error',
            'message': 'Unauthorized'
        }), 401

    if not proxy_rotator:
        return jsonify({
            'enabled': False,
            'message': 'ĞŸÑ€Ğ¾ĞºÑĞ¸ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½'
        })

    stats = proxy_rotator.get_stats()
    return jsonify({
        'enabled': True,
        'stats': stats
    })


@app.route('/calculator')
def calculator():
    """Property calculator - main analysis tool"""
    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ session_id Ğ¸Ğ· query Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ° (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
    session_id = request.args.get('session')
    return render_template('wizard.html', session_id=session_id)


@app.route('/api/parse', methods=['POST'])
@limiter.limit(settings.RATELIMIT_PARSE)  # Expensive operation - Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³
def parse_url():
    """
    API: ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ URL Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° (Ğ­ĞºÑ€Ğ°Ğ½ 1)

    Body:
        {
            "url": "https://www.cian.ru/sale/flat/123/"
        }

    Returns:
        {
            "status": "success",
            "data": {...},
            "session_id": "uuid",
            "missing_fields": ["field1", "field2"]
        }
    """
    try:
        data = request.json
        url = data.get('url')

        if not url:
            return jsonify({'status': 'error', 'message': 'URL Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ĞµĞ½'}), 400

        # SECURITY: Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ URL (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ SSRF)
        try:
            validate_url(url)
        except (URLValidationError, SSRFError) as e:
            logger.warning(f"URL validation failed: {e} (from {request.remote_addr})")
            return jsonify({'status': 'error', 'message': str(e)}), e.http_status

        # ĞĞ²Ñ‚Ğ¾Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ° Ğ¿Ğ¾ URL
        region = detect_region_from_url(url)
        logger.info(f"ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ URL: {url} (Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½: {region})")

        # SECURITY: ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ñ timeout (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ DoS)
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ´Ğ»Ñ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°, ĞµÑĞ»Ğ¸ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ
        
        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ Ğ¸Ğ· Ñ€Ğ¾Ñ‚Ğ°Ñ‚Ğ¾Ñ€Ğ° (ĞµÑĞ»Ğ¸ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½)
        proxy_config = None
        proxy_idx = None
        import time as time_module
        
        if proxy_rotator:
            proxy_info, proxy_idx = proxy_rotator.get_next_proxy()
            proxy_config = proxy_info.to_dict()
            logger.info(f"ğŸ”’ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾ĞºÑĞ¸ #{proxy_idx}: {proxy_info.server}")
        
        try:
            with timeout_context(150, 'ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ·Ğ°Ğ½ÑĞ» ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (>150s)'):
                with get_parser_for_url(url, region=region or 'spb', proxy_config=proxy_config) as parser:
                    start_time = time_module.time()
                    parsed_data = parser.parse_detail_page(url)
                    response_time = time_module.time() - start_time
                    
                    # ĞÑ‚Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ ÑƒÑĞ¿ĞµÑ… Ğ¿Ñ€Ğ¾ĞºÑĞ¸
                    if proxy_rotator and proxy_idx is not None:
                        proxy_rotator.mark_success(proxy_idx, response_time)
        except TimeoutError as e:
            logger.error(f"Parsing timeout for {url}: {e}")
            return jsonify({
                'status': 'error',
                'message': 'Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ÑÑ‚ĞµĞºĞ»Ğ¾. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ·Ğ¶Ğµ Ğ¸Ğ»Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚.'
            }), 408  # Request Timeout

        # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°
        if not region:
            address = parsed_data.get('address', '')
            region = detect_region_from_address(address)
            if region:
                logger.info(f"Ğ ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½ Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ: {region} (Ğ°Ğ´Ñ€ĞµÑ: {address})")
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ² Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°
                parsed_data['region'] = region
            else:
                logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ½Ğ¸ Ğ¿Ğ¾ URL, Ğ½Ğ¸ Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ: {address}")
                # Fallback Ğ½Ğ° ĞœĞ¾ÑĞºĞ²Ñƒ (Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¦Ğ˜ĞĞ)
                region = 'msk'
                parsed_data['region'] = region
        else:
            parsed_data['region'] = region

        # Ğ’ĞĞ›Ğ˜Ğ”ĞĞ¦Ğ˜Ğ¯: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ ÑĞ¿Ğ°Ñ€ÑĞ¸Ğ»Ğ¸ÑÑŒ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ»Ñ
        # Ğ‘ĞµĞ· Ñ†ĞµĞ½Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ±ĞµÑĞ¿Ğ¾Ğ»ĞµĞ·ĞµĞ½ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        price = parsed_data.get('price') or parsed_data.get('price_raw')
        total_area = parsed_data.get('total_area') or parsed_data.get('area')
        address = parsed_data.get('address')

        if not price and not total_area and not address:
            logger.error(f"ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ½Ğµ Ğ²ĞµÑ€Ğ½ÑƒĞ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ URL: {url}")
            return jsonify({
                'status': 'error',
                'error_type': 'parsing_failed',
                'message': 'ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ñ. Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¾ Ğ¸Ğ»Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ° ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹.',
                'details': 'Ğ¦ĞµĞ½Ğ°, Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ¸ Ğ°Ğ´Ñ€ĞµÑ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹'
            }), 422  # Unprocessable Entity

        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        missing_fields = _identify_missing_fields(parsed_data)

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞµÑÑĞ¸Ñ
        session_id = str(uuid.uuid4())
        session_storage.set(session_id, {
            'target_property': parsed_data,
            'comparables': [],
            'created_at': datetime.now().isoformat(),
            'step': 1
        })

        return jsonify({
            'status': 'success',
            'data': parsed_data,
            'session_id': session_id,
            'missing_fields': missing_fields
        })

    except Exception as e:
        # ĞÑ‚Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ Ğ² proxy_rotator
        if proxy_rotator and proxy_idx is not None:
            error_str = str(e).lower()
            if 'captcha' in error_str or 'blocked' in error_str:
                proxy_rotator.mark_captcha(proxy_idx)
            else:
                proxy_rotator.mark_failed(proxy_idx, reason=str(e)[:100])
        
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ°: {e}", exc_info=True)

        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
        error_str = str(e).lower()
        error_type = 'parsing_error'
        user_message = 'ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ñ'

        if 'url' in error_str and ('invalid' in error_str or 'Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½' in error_str):
            error_type = 'invalid_url'
            user_message = 'ĞĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ°Ñ ÑÑÑ‹Ğ»ĞºĞ°. Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ URL Ñ Cian.ru'
        elif 'timeout' in error_str or 'timed out' in error_str:
            error_type = 'timeout'
            user_message = 'ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ·.'
        elif 'not found' in error_str or '404' in error_str:
            error_type = 'no_data'
            user_message = 'ĞĞ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ÑÑÑ‹Ğ»ĞºÑƒ.'
        elif 'captcha' in error_str or 'blocked' in error_str:
            error_type = 'parsing_failed'
            user_message = 'ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¸Ğ½ÑƒÑ‚.'
        elif 'browser' in error_str or 'playwright' in error_str:
            error_type = 'browser_error'
            user_message = 'Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°. ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ.'

        return jsonify({
            'status': 'error',
            'error_type': error_type,
            'message': user_message,
            'technical_details': str(e)
        }), 500


@app.route('/api/create-manual', methods=['POST'])
@limiter.limit(settings.RATELIMIT_PARSE)
def create_manual():
    """
    API: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ±ĞµĞ· Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° (Ğ­ĞºÑ€Ğ°Ğ½ 1)

    Body:
        {
            "address": "Ğ¡Ğ°Ğ½ĞºÑ‚-ĞŸĞµÑ‚ĞµÑ€Ğ±ÑƒÑ€Ğ³, ÑƒĞ»Ğ¸Ñ†Ğ° Ğ›ĞµĞ½Ğ¸Ğ½Ğ°, 10",
            "price_raw": 15000000,
            "total_area": 75.5,
            "rooms": "2",
            "floor": "5/10",
            "living_area": 55.0,
            "kitchen_area": 12.0,
            "repair_level": "ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ",
            "view_type": "ÑƒĞ»Ğ¸Ñ†Ğ°"
        }

    Returns:
        {
            "status": "success",
            "data": {...},
            "session_id": "uuid",
            "missing_fields": []
        }
    """
    try:
        data = request.json
        logger.info(f"[create-manual] ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¾Ñ‚ {request.remote_addr}")
        logger.debug(f"[create-manual] Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ: {data}")

        if not data:
            logger.error("[create-manual] ĞŸÑƒÑÑ‚Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ² Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞµ")
            return jsonify({
                'status': 'error',
                'error_type': 'empty_request',
                'message': 'Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ñ‹'
            }), 400

        # SECURITY: Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‡ĞµÑ€ĞµĞ· Pydantic
        try:
            validated = ManualPropertyInput(**data)
            logger.info(f"[create-manual] Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¹Ğ´ĞµĞ½Ğ°: {validated.address}")
        except PydanticValidationError as e:
            logger.warning(f"[create-manual] Validation error from {request.remote_addr}: {e}")
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
            errors = []
            for error in e.errors():
                field = error['loc'][0]
                msg = error['msg']
                errors.append(f"{field}: {msg}")
            return jsonify({
                'status': 'error',
                'error_type': 'data_validation_error',
                'message': 'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ.',
                'errors': errors
            }), 400

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ½ĞµĞ´Ğ²Ğ¸Ğ¶Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        property_data = {
            'address': validated.address,
            'price_raw': validated.price_raw,
            'price': f"{int(validated.price_raw):,} â‚½".replace(',', ' '),
            'total_area': validated.total_area,
            'area': f"{validated.total_area} Ğ¼Â²",
            'rooms': validated.rooms,
            'floor': validated.floor,
            'living_area': validated.living_area,
            'kitchen_area': validated.kitchen_area,
            'repair_level': validated.repair_level,
            'view_type': validated.view_type,
            'manual_input': True,
            'title': f"{validated.rooms}-ĞºĞ¾Ğ¼Ğ½. ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ°, {validated.total_area} Ğ¼Â²",
            'url': 'manual-input',  # ĞŸĞ»ĞµĞ¹ÑÑ…Ğ¾Ğ»Ğ´ĞµÑ€ Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°
            'metro': [],
            'residential_complex': None,
            'characteristics': {}
        }

        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¸Ğ· Ğ°Ğ´Ñ€ĞµÑĞ° (Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ²ÑĞµÑ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ²)
        region = detect_region_from_address(data['address'])
        if not region:
            logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼: msk")
            region = 'msk'  # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ ĞœĞ¾ÑĞºĞ²Ğ°

        property_data['region'] = region

        logger.info(f"Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ: {property_data['address']} (Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½: {region})")

        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ñ (Ğ´Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ¸Ñ… Ğ¼ĞµĞ½ÑŒÑˆĞµ)
        missing_fields = _identify_missing_fields(property_data)

        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ ÑĞµÑÑĞ¸Ñ
        session_id = str(uuid.uuid4())
        try:
            session_storage.set(session_id, {
                'target_property': property_data,
                'comparables': [],
                'created_at': datetime.now().isoformat(),
                'step': 1
            })
            logger.info(f"[create-manual] Ğ¡ĞµÑÑĞ¸Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ°: {session_id}")
        except Exception as storage_err:
            logger.error(f"[create-manual] ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ ÑĞµÑÑĞ¸Ğ¸: {storage_err}", exc_info=True)
            return jsonify({
                'status': 'error',
                'error_type': 'storage_error',
                'message': 'ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ·.'
            }), 500

        logger.info(f"[create-manual] Ğ£ÑĞ¿ĞµÑˆĞ½Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½ Ğ¾Ğ±ÑŠĞµĞºÑ‚: {property_data['title']}")
        return jsonify({
            'status': 'success',
            'data': property_data,
            'session_id': session_id,
            'missing_fields': missing_fields
        })

    except Exception as e:
        logger.error(f"[create-manual] ĞĞµĞ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': safe_error_message(e)
        }), 500


@app.route('/api/update-target', methods=['POST'])
def update_target():
    """
    API: ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ñ Ğ·Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ¾Ğ»ÑĞ¼Ğ¸ (Ğ­ĞºÑ€Ğ°Ğ½ 1 â†’ 2)

    Body:
        {
            "session_id": "uuid",
            "data": {
                "has_design": true,
                "ceiling_height": 3.2,
                ...
            }
        }

    Returns:
        {
            "status": "success",
            "message": "Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹"
        }
    """
    try:
        payload = request.json
        session_id = payload.get('session_id')
        data = payload.get('data')

        if not session_id or not session_storage.exists(session_id):
            return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        session_data = session_storage.get(session_id)
        session_data['target_property'].update(data)
        session_data['step'] = 2
        session_storage.set(session_id, session_data)

        return jsonify({
            'status': 'success',
            'message': 'Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹'
        })

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': safe_error_message(e)
        }), 500


@app.route('/api/find-similar', methods=['POST'])
@limiter.limit(settings.RATELIMIT_SEARCH)  # Expensive - Ğ¿Ğ¾Ğ¸ÑĞº Ğ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²
def find_similar():
    """
    API: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ñ… Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ² (Ğ­ĞºÑ€Ğ°Ğ½ 2)

    Body:
        {
            "session_id": "uuid",
            "limit": 20,
            "search_type": "building"  // "building" Ğ¸Ğ»Ğ¸ "city"
        }

    Returns:
        {
            "status": "success",
            "comparables": [...],
            "search_type": "building",
            "residential_complex": "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ–Ğš"
        }
    """
    try:
        import time
        request_start = time.time()

        payload = request.json
        session_id = payload.get('session_id')
        limit = payload.get('limit', 50)  # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ½Ğ¾ Ğ´Ğ¾ 50 Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
        search_type = payload.get('search_type', 'building')  # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ Ğ¸Ñ‰ĞµĞ¼ Ğ² Ğ–Ğš

        logger.info(f"ğŸ“ [STEP 2] find-similar request started (session: {session_id}, type: {search_type}, limit: {limit})")

        if not session_id or not session_storage.exists(session_id):
            logger.error(f"âŒ Session not found: {session_id}")
            return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

        session_data = session_storage.get(session_id)
        target = session_data['target_property']

        # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¸ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğµ (Ğ½Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾!)
        # Ğ ĞµĞ³Ğ¸Ğ¾Ğ½ ÑƒĞ¶Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½ Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ Ğ² /api/parse
        region = target.get('region')
        if not region:
            # Fallback: Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿Ğ¾ URL Ğ¸Ğ»Ğ¸ Ğ°Ğ´Ñ€ĞµÑÑƒ
            target_url = target.get('url', '')
            region = detect_region_from_url(target_url)
            if not region:
                address = target.get('address', '')
                region = detect_region_from_address(address)
                if not region:
                    logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback: msk")
                    region = 'msk'

        logger.info(f"ğŸ” Searching for similar properties (session: {session_id}, type: {search_type}, region: {region}, limit: {limit})")

        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ URL Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ°
        target_url = target.get('url', '')
        is_manual_input = target.get('manual_input', False) or target_url == 'manual-input'

        if is_manual_input:
            logger.info(f"ğŸ“ Manual input detected - using citywide search")

        # ĞŸĞ¾Ğ¸ÑĞº Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ ĞºÑÑˆĞµĞ¼ Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ¼
        try:
            logger.info(f"ğŸ” Starting search (type: {search_type}, limit: {limit})")
            # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ URL Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ° (Ğ¸Ğ»Ğ¸ fallback Ğ½Ğ° Ğ¦Ğ˜ĞĞ)
            # Ğ”Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¦Ğ˜ĞĞ ĞºĞ°Ğº Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº
            search_url = 'https://www.cian.ru/' if is_manual_input else (target_url or 'https://www.cian.ru/')
            with get_parser_for_url(search_url, region=region) as parser:
                # Ğ”Ğ»Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ citywide search (Ğ½ĞµÑ‚ Ğ–Ğš)
                if search_type == 'building' and not is_manual_input:
                    # ĞŸĞ¾Ğ¸ÑĞº Ğ² Ñ‚Ğ¾Ğ¼ Ğ¶Ğµ Ğ–Ğš
                    logger.info(f"ğŸ¢ Searching in building: {target.get('residential_complex', 'Unknown')}")
                    similar = parser.search_similar_in_building(target, limit=limit)
                    residential_complex = target.get('residential_complex', 'ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾')
                    logger.info(f"âœ… Found {len(similar)} comparables in building")

                    # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ Ğ¤Ğ˜ĞšĞ¡: Fallback ĞµÑĞ»Ğ¸ building search Ğ²ĞµÑ€Ğ½ÑƒĞ» 0
                    if len(similar) == 0:
                        logger.warning("âš ï¸ Building search returned 0 results! Trying citywide search as fallback...")
                        similar = parser.search_similar(target, limit=limit)
                        residential_complex = None  # Ğ¢.Ğº. Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ñƒ
                        logger.info(f"âœ… Fallback citywide search found {len(similar)} comparables")
                else:
                    # Ğ¨Ğ¸Ñ€Ğ¾ĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ¿Ğ¾ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ñƒ
                    logger.info(f"ğŸŒ† Searching in city: {region}")
                    similar = parser.search_similar(target, limit=limit)
                    residential_complex = None
                    logger.info(f"âœ… Found {len(similar)} comparables in city")
        except Exception as search_error:
            logger.error(f"âŒ Search failed: {search_error}", exc_info=True)
            return jsonify({
                'status': 'error',
                'message': 'search_failed',
                'details': f'ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ¸ÑĞº: {str(search_error)}'
            }), 500

        # Ğ•ÑĞ»Ğ¸ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ñ URL, Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼ Ğ¸Ñ… Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾
        # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ±ĞµĞ· Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (price, total_area, price_per_sqm)
        urls_to_parse = [
            c.get('url') for c in similar
            if c.get('url') and not (c.get('price') and c.get('total_area'))
        ]

        logger.info(f"ğŸ” DEBUG: {len(similar)} comparables found, {len(urls_to_parse)} need detailed parsing")

        if urls_to_parse:
            try:
                from src.parsers.async_parser import parse_multiple_urls_parallel
                logger.info(f"ğŸš€ Starting parallel parsing of {len(urls_to_parse)} URLs...")
                import time
                parse_start = time.time()

                # PATCH 1: Robust parsing with retry + quality metrics
                detailed_results, parse_quality = parse_multiple_urls_parallel(
                    urls=urls_to_parse,
                    headless=True,
                    cache=property_cache,
                    region=region,
                    max_concurrent=3,  # Ğ¡Ğ½Ğ¸Ğ¶ĞµĞ½Ğ¾ Ñ 5 Ğ´Ğ¾ 3 Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ rate limiting
                    max_retries=2
                )

                parse_elapsed = time.time() - parse_start
                logger.info(
                    f"â±ï¸ Parallel parsing took {parse_elapsed:.1f}s for {len(urls_to_parse)} URLs | "
                    f"Success: {parse_quality['successfully_parsed']}, "
                    f"Failed: {parse_quality['parse_failed']}, "
                    f"Retries: {parse_quality['total_retries']}"
                )

                # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼
                if parse_quality['error_breakdown']:
                    logger.warning(f"Parse errors breakdown: {parse_quality['error_breakdown']}")

                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹
                url_to_details = {d['url']: d for d in detailed_results}
                updated_count = 0
                for comparable in similar:
                    url = comparable.get('url')
                    if url in url_to_details:
                        comparable.update(url_to_details[url])
                        updated_count += 1

                logger.info(f"âœ… Enhanced {updated_count}/{len(similar)} comparables with detailed data")

            except Exception as e:
                logger.error(f"âŒ Parallel parsing failed, using basic data: {e}", exc_info=True)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ğ”Ğ•Ğ¢Ğ•ĞšĞ¦Ğ˜Ğ¯ Ğ˜ Ğ£Ğ”ĞĞ›Ğ•ĞĞ˜Ğ• Ğ”Ğ£Ğ‘Ğ›Ğ˜ĞšĞĞ¢ĞĞ’
        # ĞŸÑ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ¿Ğ¾ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼ Ğ¾Ğ´Ğ½Ğ° ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ°
        # Ğ½Ğ° Ğ¦Ğ˜ĞĞ, ĞĞ²Ğ¸Ñ‚Ğ¾, Ğ¯Ğ½Ğ´ĞµĞºÑ.ĞĞµĞ´Ğ²Ğ¸Ğ¶Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğ¼Ğ¸ Ñ†ĞµĞ½Ğ°Ğ¼Ğ¸
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if len(similar) > 0:
            logger.info(f"ğŸ” Checking for duplicates among {len(similar)} comparables...")
            unique_comparables, removed_duplicates = duplicate_detector.deduplicate_list(
                similar,
                keep_best_price=True  # ĞÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚ Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ñ†ĞµĞ½Ğ¾Ğ¹
            )

            if removed_duplicates:
                logger.info(f"âœ“ Removed {len(removed_duplicates)} strict duplicates")
                for dup in removed_duplicates:
                    logger.debug(f"  - Removed: {dup.get('address', 'Unknown')} ({dup.get('price', 0):,.0f} â‚½)")

                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº
                similar = unique_comparables
            else:
                logger.info("âœ“ No strict duplicates found")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Ğ”ĞĞ ĞĞ‘ĞĞ¢ĞšĞ #4: ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ ĞšĞĞ§Ğ•Ğ¡Ğ¢Ğ’Ğ ĞŸĞĞ”ĞĞ‘Ğ ĞĞĞĞ«Ğ¥ ĞĞĞĞ›ĞĞ“ĞĞ’
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        warnings = []

        # ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ°Ñ…
        duplicate_warnings_count = sum(1 for c in similar if c.get('possible_duplicate'))
        if duplicate_warnings_count > 0:
            warnings.append({
                'type': 'warning',
                'title': 'ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹',
                'message': f'ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {duplicate_warnings_count} Ğ¾Ğ±ÑŠĞµĞºÑ‚(Ğ¾Ğ²), ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ°Ğ¼Ğ¸ (Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ Ğ°Ğ´Ñ€ĞµÑĞ° Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹). '
                           'ĞĞ½Ğ¸ Ğ¿Ğ¾Ğ¼ĞµÑ‡ĞµĞ½Ñ‹ ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ·Ğ½Ğ°Ñ‡ĞºĞ¾Ğ¼. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ Ğ¸ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ Ğ½ĞµĞ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ.'
            })

        # PATCH 4: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¾ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°Ñ… Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° (ĞµÑĞ»Ğ¸ Ğ±Ñ‹Ğ»Ğ¸)
        if urls_to_parse and 'parse_quality' in locals():
            parse_failed = parse_quality.get('parse_failed', 0)
            total_found = parse_quality.get('total_found', 0)

            if parse_failed > 0:
                failed_percent = (parse_failed / total_found * 100) if total_found > 0 else 0
                error_breakdown = parse_quality.get('error_breakdown', {})

                error_details = []
                if 'rate_limited' in error_breakdown:
                    error_details.append(f"rate limiting ({error_breakdown['rate_limited']})")
                if 'timeout' in error_breakdown:
                    error_details.append(f"timeout ({error_breakdown['timeout']})")
                if 'captcha' in error_breakdown:
                    error_details.append(f"captcha ({error_breakdown['captcha']})")

                if failed_percent > 50:
                    warnings.append({
                        'type': 'error',
                        'title': 'ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…',
                        'message': f'ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ {parse_failed} Ğ¸Ğ· {total_found} Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² ({failed_percent:.0f}%). ' +
                                   (f'ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹: {", ".join(error_details)}. ' if error_details else '') +
                                   'ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½ĞµÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¶Ğµ Ğ¸Ğ»Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ĞµÑÑŒ Ğ² Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºÑƒ.'
                    })
                elif failed_percent > 20:
                    warnings.append({
                        'type': 'warning',
                        'title': 'ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…',
                        'message': f'ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ {parse_failed} Ğ¸Ğ· {total_found} Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² ({failed_percent:.0f}%). ' +
                                   (f'ĞŸÑ€Ğ¸Ñ‡Ğ¸Ğ½Ñ‹: {", ".join(error_details)}. ' if error_details else '') +
                                   'Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ°.'
                    })

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° 1: Ğ”Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²?
        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸Ğº Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°
        def get_context_tips(target_prop, count, rc_name):
            """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ¼Ğ°Ğ»Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ Ñ‡Ñ‚Ğ¾ Ğ´ĞµĞ»Ğ°Ñ‚ÑŒ"""
            tips = []
            context_reason = None

            rooms = target_prop.get('rooms', '')
            price_sqm = target_prop.get('price_per_sqm', 0)
            region = target_prop.get('region', '')

            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
            if rc_name:
                context_reason = f'Ğ’ Ğ–Ğš Â«{rc_name}Â» ÑĞµĞ¹Ñ‡Ğ°Ñ Ğ¼Ğ°Ğ»Ğ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ Ğ½Ğ° Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ñƒ'
                tips.append('ĞŸĞ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¸Ñ‚Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ–Ğš Ğ½Ğ° Ğ¦Ğ˜ĞĞ')
            elif rooms in ['5', '5+', '6', '7']:
                context_reason = 'ĞšĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ñ‹ Ñ 5+ ĞºĞ¾Ğ¼Ğ½Ğ°Ñ‚Ğ°Ğ¼Ğ¸ â€” Ñ€ĞµĞ´ĞºĞ¸Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚ Ñ€Ñ‹Ğ½ĞºĞ°'
            elif price_sqm and price_sqm > 500000:
                context_reason = 'ĞŸÑ€ĞµĞ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚ Ğ¸Ğ¼ĞµĞµÑ‚ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹'
            elif region and region not in ['msk', 'spb']:
                context_reason = 'Ğ”Ğ»Ñ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ¾Ğ² Ğ±Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ°'

            # ĞĞ±Ñ‰Ğ¸Ğµ ÑĞ¾Ğ²ĞµÑ‚Ñ‹
            tips.append('Ğ”Ğ¾Ğ±Ğ°Ğ²ÑŒÑ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ñ‡ĞµÑ€ĞµĞ· ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Â«Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞµÂ»')
            if count > 0:
                tips.append('ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ Ğ¸Ğ¼ĞµÑÑ‰Ğ¸Ğ¼Ğ¸ÑÑ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ğ¼Ğ¸')

            return context_reason, tips

        if len(similar) == 0:
            context_reason, tips = get_context_tips(target, 0, residential_complex)
            message = 'ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°.'
            if context_reason:
                message += f' {context_reason}.'
            warnings.append({
                'type': 'error',
                'title': 'ĞĞ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹',
                'message': message,
                'tips': tips
            })
        elif len(similar) < 5:
            context_reason, tips = get_context_tips(target, len(similar), residential_complex)
            message = f'ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ²ÑĞµĞ³Ğ¾ {len(similar)} Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³(Ğ¾Ğ²). Ğ”Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ 10-15.'
            if context_reason:
                message += f' {context_reason}.'
            warnings.append({
                'type': 'error',
                'title': 'ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²',
                'message': message,
                'tips': tips
            })
        elif len(similar) < 10:
            context_reason, tips = get_context_tips(target, len(similar), residential_complex)
            message = f'ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(similar)} Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². Ğ”Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ 15-20.'
            if context_reason:
                message += f' {context_reason}.'
            warnings.append({
                'type': 'warning',
                'title': 'ĞœĞ°Ğ»Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²',
                'message': message,
                'tips': tips
            })

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° 2: Ğ Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ Ñ†ĞµĞ½ (ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸)
        if len(similar) >= 3:
            prices_per_sqm = [c.get('price_per_sqm', 0) for c in similar if c.get('price_per_sqm')]

            if len(prices_per_sqm) >= 3:
                import statistics
                median_price = statistics.median(prices_per_sqm)

                if median_price > 0:
                    stdev_price = statistics.stdev(prices_per_sqm)
                    cv = stdev_price / median_price  # ĞšĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ Ğ²Ğ°Ñ€Ğ¸Ğ°Ñ†Ğ¸Ğ¸

                    if cv > 0.5:  # >50% - Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ
                        warnings.append({
                            'type': 'error',
                            'title': 'ĞÑ‡ĞµĞ½ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ Ñ†ĞµĞ½',
                            'message': f'Ğ Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ Ñ†ĞµĞ½ Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ {cv*100:.0f}%. Ğ­Ñ‚Ğ¾ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ - Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ½Ñ‹ Ğ½ĞµĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¸ ÑƒĞ´Ğ°Ğ»Ğ¸Ñ‚Ğµ Ğ½ĞµĞ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰Ğ¸Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹.'
                        })
                    elif cv > 0.3:  # >30% - Ğ±Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ
                        warnings.append({
                            'type': 'warning',
                            'title': 'Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¾Ğ¹ Ñ€Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ Ñ†ĞµĞ½',
                            'message': f'Ğ Ğ°Ğ·Ğ±Ñ€Ğ¾Ñ Ñ†ĞµĞ½ Ñƒ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ {cv*100:.0f}%. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸ ÑƒĞ±ĞµĞ´Ğ¸Ñ‚ÑŒÑÑ, Ñ‡Ñ‚Ğ¾ Ğ²ÑĞµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ ÑĞ¾Ğ¿Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ğ¼Ñ‹.'
                        })

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° 3: Ğ•ÑÑ‚ÑŒ Ğ»Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ñ Ñ†ĞµĞ½Ğ¾Ğ¹ Ğ·Ğ° Ğ¼Â²?
        if len(similar) > 0:
            with_price = sum(1 for c in similar if c.get('price_per_sqm'))
            if with_price == 0:
                warnings.append({
                    'type': 'error',
                    'title': 'ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ñ†ĞµĞ½Ğ°Ñ…',
                    'message': 'ĞĞ¸ Ñƒ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ½ĞµÑ‚ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ñ†ĞµĞ½Ğµ Ğ·Ğ° Ğ¼Â². ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·.'
                })
            elif with_price < len(similar) * 0.5:  # ĞœĞµĞ½ÑŒÑˆĞµ 50% Ñ Ñ†ĞµĞ½Ğ¾Ğ¹
                warnings.append({
                    'type': 'warning',
                    'title': 'ĞĞµĞ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ñ†ĞµĞ½Ğ°Ñ…',
                    'message': f'Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ñƒ {with_price} Ğ¸Ğ· {len(similar)} Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² ĞµÑÑ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ñ†ĞµĞ½Ğµ Ğ·Ğ° Ğ¼Â². Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ ÑĞ½Ğ¸Ğ·Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ†ĞµĞ½ĞºĞ¸.'
                })

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² ÑĞµÑÑĞ¸Ñ
        session_data['comparables'] = similar
        session_data['comparables_warnings'] = warnings  # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ warnings Ğ² ÑĞµÑÑĞ¸Ñ
        session_storage.set(session_id, session_data)

        # Debug logging - trace object count
        request_elapsed = time.time() - request_start
        logger.info(f"ğŸ” Saved {len(similar)} comparables to session {session_id}")
        if warnings:
            logger.warning(f"âš ï¸ Quality warnings: {len(warnings)} issue(s) detected")
            for w in warnings:
                logger.warning(f"  - [{w['type'].upper()}] {w['title']}: {w['message']}")
        logger.info(f"âœ… [STEP 2] find-similar completed in {request_elapsed:.1f}s - returning {len(similar)} comparables")

        return jsonify({
            'status': 'success',
            'comparables': similar,
            'count': len(similar),
            'search_type': search_type,
            'residential_complex': residential_complex,
            'elapsed_time': round(request_elapsed, 1),
            'warnings': warnings  # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ warnings Ğ² Ğ¾Ñ‚Ğ²ĞµÑ‚
        })

    except Exception as e:
        logger.error(f"âŒ [STEP 2] find-similar failed: {e}", exc_info=True)

        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ¹ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸
        error_str = str(e).lower()
        error_type = 'search_error'
        user_message = 'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ¸ÑĞºĞµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²'

        if 'session' in error_str or 'not found' in error_str:
            error_type = 'session_error'
            user_message = 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ° Ğ¸Ğ»Ğ¸ Ğ¸ÑÑ‚ĞµĞºĞ»Ğ°. ĞĞ°Ñ‡Ğ½Ğ¸Ñ‚Ğµ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ°.'
        elif 'timeout' in error_str or 'timed out' in error_str:
            error_type = 'timeout'
            user_message = 'ĞŸÑ€ĞµĞ²Ñ‹ÑˆĞµĞ½Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ ĞµÑ‰Ğµ Ñ€Ğ°Ğ·.'
        elif 'parsing' in error_str or 'parse' in error_str:
            error_type = 'parsing_error'
            user_message = 'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¹. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ·Ğ¶Ğµ.'
        elif 'browser' in error_str or 'playwright' in error_str:
            error_type = 'browser_error'
            user_message = 'Ğ¢ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ±Ñ€Ğ°ÑƒĞ·ĞµÑ€Ğ°. ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ¸Ñ‚Ğµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞµĞºÑƒĞ½Ğ´.'
        elif 'network' in error_str or 'connection' in error_str:
            error_type = 'network_error'
            user_message = 'ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞµÑ‚Ğ¸ Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ.'

        return jsonify({
            'status': 'error',
            'error_type': error_type,
            'message': user_message,
            'technical_details': str(e)
        }), 500


@app.route('/api/multi-source-search', methods=['POST'])
@limiter.limit(settings.RATELIMIT_SEARCH)  # Expensive - Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
def multi_source_search():
    """
    API: ĞœÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² (Ğ¦Ğ˜ĞĞ + Ğ”Ğ¾Ğ¼ĞšĞ»Ğ¸Ğº Ğ¾Ğ´Ğ½Ğ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾)

    Body:
        {
            "session_id": "uuid",
            "sources": ["cian", "domclick"],  // Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ¸ÑĞºĞ°
            "limit_per_source": 15,  // Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ½Ğ° ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº
            "strategy": "citywide",  // "same_building", "same_area", "citywide"
            "parallel": true  // ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
        }

    Returns:
        {
            "status": "success",
            "comparables": [...],
            "total": 30,
            "sources_stats": {
                "cian": 15,
                "domclick": 15
            }
        }
    """
    try:
        import time
        from src.parsers.multi_source_search import search_across_sources

        request_start = time.time()

        payload = request.json
        session_id = payload.get('session_id')
        sources = payload.get('sources', ['cian', 'domclick'])
        limit_per_source = payload.get('limit_per_source', 15)
        strategy = payload.get('strategy', 'citywide')
        parallel = payload.get('parallel', True)

        logger.info(f"ğŸ“ [MULTI-SOURCE] search request started (session: {session_id}, sources: {sources}, strategy: {strategy})")

        if not session_id or not session_storage.exists(session_id):
            logger.error(f"âŒ Session not found: {session_id}")
            return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

        session_data = session_storage.get(session_id)
        target = session_data['target_property']

        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½
        region = target.get('region')
        if not region:
            target_url = target.get('url', '')
            region = detect_region_from_url(target_url)
            if not region:
                address = target.get('address', '')
                region = detect_region_from_address(address)
                if not region:
                    logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ fallback: msk")
                    region = 'msk'

        logger.info(f"ğŸ” Multi-source search (sources: {sources}, region: {region}, strategy: {strategy}, limit: {limit_per_source})")

        # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº
        try:
            search_start = time.time()

            results = search_across_sources(
                target_property=target,
                sources=sources,
                strategy=strategy,
                limit_per_source=limit_per_source,
                parallel=parallel
            )

            search_elapsed = time.time() - search_start
            logger.info(f"â±ï¸ Multi-source search took {search_elapsed:.1f}s, found {len(results)} total results")

            # ĞŸĞ¾Ğ´ÑÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°Ğ¼
            sources_stats = {}
            for result in results:
                source = result.get('source', 'unknown')
                sources_stats[source] = sources_stats.get(source, 0) + 1

            logger.info(f"ğŸ“Š Sources stats: {sources_stats}")

            # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹
            from src.utils.duplicate_detector import DuplicateDetector
            detector = DuplicateDetector()
            unique_results, removed_info = detector.deduplicate_list(results, keep_best_price=True)

            removed_duplicates = len(removed_info) if removed_info else 0
            if removed_duplicates > 0:
                logger.info(f"ğŸ—‘ï¸ Removed {removed_duplicates} duplicates, {len(unique_results)} unique results remain")

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² ÑĞµÑÑĞ¸Ñ
            session_data['comparables'] = unique_results
            session_data['multi_source_used'] = True
            session_data['sources_stats'] = sources_stats
            session_storage.set(session_id, session_data)

            request_elapsed = time.time() - request_start
            logger.info(f"âœ… Multi-source search completed in {request_elapsed:.1f}s")

            return jsonify({
                'status': 'success',
                'comparables': unique_results,
                'total': len(unique_results),
                'sources_stats': sources_stats,
                'strategy': strategy,
                'removed_duplicates': removed_duplicates,
                'search_time': round(search_elapsed, 2)
            })

        except Exception as search_error:
            logger.error(f"âŒ Multi-source search failed: {search_error}", exc_info=True)
            return jsonify({
                'status': 'error',
                'message': 'multi_source_search_failed',
                'details': f'ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº: {str(search_error)}'
            }), 500

    except Exception as e:
        logger.error(f"âŒ Multi-source search error: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': 'Ğ’Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½ÑÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ¼ Ğ¿Ğ¾Ğ¸ÑĞºĞµ',
            'technical_details': str(e)
        }), 500


@app.route('/api/add-comparable', methods=['POST'])
def add_comparable():
    """
    API: Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¿Ğ¾ URL (Ğ­ĞºÑ€Ğ°Ğ½ 2)

    Body:
        {
            "session_id": "uuid",
            "url": "https://www.cian.ru/sale/flat/456/"
        }

    Returns:
        {
            "status": "success",
            "comparable": {...}
        }
    """
    try:
        payload = request.json
        session_id = payload.get('session_id')
        url = payload.get('url')

        if not session_id or not session_storage.exists(session_id):
            return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

        # SECURITY: Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ URL (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ SSRF)
        try:
            validate_url(url)
        except (URLValidationError, SSRFError) as e:
            logger.warning(f"URL validation failed: {e} (from {request.remote_addr})")
            return jsonify({'status': 'error', 'message': str(e)}), e.http_status

        # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°
        session_data = session_storage.get(session_id)
        target = session_data['target_property']
        target_region = target.get('region', 'spb')

        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¿Ğ¾ URL
        region = detect_region_from_url(url)
        logger.info(f"Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°: {url} (Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½: {region}, Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½: {target_region})")

        # SECURITY: ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ñ timeout (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ DoS)
        try:
            logger.info(f"ğŸ” Parsing comparable URL: {url}")
            with timeout_context(120, 'ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ·Ğ°Ğ½ÑĞ» ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ (>120s)'):
                with get_parser_for_url(url, region=region) as parser:
                    comparable_data = parser.parse_detail_page(url)
                    logger.info(f"âœ… Successfully parsed comparable: {comparable_data.get('title', 'Unknown')}")
        except TimeoutError as e:
            logger.error(f"âŒ Parsing timeout for {url}: {e}")
            return jsonify({
                'status': 'error',
                'message': 'parsing_timeout',
                'details': 'Ğ’Ñ€ĞµĞ¼Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¿Ñ€ĞµĞ²Ñ‹ÑĞ¸Ğ»Ğ¾ 2 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹. ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚.'
            }), 408
        except Exception as parse_error:
            logger.error(f"âŒ Failed to parse {url}: {parse_error}", exc_info=True)
            return jsonify({
                'status': 'error',
                'message': 'parsing_error',
                'details': str(parse_error)
            }), 500

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‡Ñ‚Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        if not comparable_data or not comparable_data.get('price') or not comparable_data.get('total_area'):
            logger.warning(f"âš ï¸ Parsed data incomplete: {comparable_data}")
            return jsonify({
                'status': 'error',
                'message': 'parsing_incomplete',
                'details': 'ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ° (Ñ†ĞµĞ½Ğ° Ğ¸Ğ»Ğ¸ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚). ĞŸĞ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞ¹Ñ‚Ğµ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚.'
            }), 400

        # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°
        if not region:
            address = comparable_data.get('address', '')
            region = detect_region_from_address(address)
            if region:
                logger.info(f"âœ“ Ğ ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½ Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ: {region}")
            else:
                logger.warning(f"âš ï¸ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¿Ğ¾ Ğ°Ğ´Ñ€ĞµÑÑƒ: {address}")
                region = 'msk'  # fallback (Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½ Ğ¦Ğ˜ĞĞ)

        # ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ: ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´Ğ°ĞµĞ¼ Ğ¾ Ğ½ĞµÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°
        if region != target_region:
            logger.warning(f"âš ï¸ Ğ’ĞĞ˜ĞœĞĞĞ˜Ğ•: ĞĞ½Ğ°Ğ»Ğ¾Ğ³ Ğ¸Ğ· Ğ´Ñ€ÑƒĞ³Ğ¾Ğ³Ğ¾ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°! Ğ¦ĞµĞ»ĞµĞ²Ğ¾Ğ¹: {target_region}, ĞĞ½Ğ°Ğ»Ğ¾Ğ³: {region}")
            return jsonify({
                'status': 'error',
                'message': 'region_mismatch',
                'details': f'Ğ­Ñ‚Ğ¾Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑÑ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¼ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğµ ({region}), Ğ° Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚ - Ğ² Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğµ {target_region}. Ğ”Ğ»Ñ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸Ğ· Ñ‚Ğ¾Ğ³Ğ¾ Ğ¶Ğµ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°.'
            }), 400

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ½Ğ° Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹
        session_data = session_storage.get(session_id)
        existing_comparables = session_data.get('comparables', [])

        if existing_comparables:
            logger.info(f"ğŸ” Checking if new comparable is duplicate of {len(existing_comparables)} existing ones...")
            duplicates = duplicate_detector.find_duplicates(comparable_data, existing_comparables)

            if duplicates:
                # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ ÑĞ°Ğ¼Ñ‹Ğ¹ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚
                best_match = max(duplicates, key=lambda d: d.confidence)

                if best_match.duplicate_type == 'strict':
                    # Ğ¡Ñ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¹ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚ - Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ÑĞµĞ¼
                    logger.warning(f"âŒ Strict duplicate detected: {best_match.confidence:.0f}% match")
                    existing_obj = existing_comparables[best_match.index]
                    return jsonify({
                        'status': 'error',
                        'message': 'duplicate_object',
                        'details': f'Ğ­Ñ‚Ğ¾Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚ ÑƒĞ¶Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ Ğ² ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². '
                                   f'ĞĞ´Ñ€ĞµÑ: {existing_obj.get("address", "Unknown")}, '
                                   f'Ñ†ĞµĞ½Ğ°: {existing_obj.get("price", 0):,.0f} â‚½. '
                                   f'Ğ¡Ğ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ: {best_match.confidence:.0f}%.'
                    }), 400
                elif best_match.duplicate_type in ['probable', 'possible']:
                    # Ğ’ĞµÑ€Ğ¾ÑÑ‚Ğ½Ñ‹Ğ¹/Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚ - Ğ¿Ğ¾Ğ¼ĞµÑ‡Ğ°ĞµĞ¼ Ñ„Ğ»Ğ°Ğ³Ğ¾Ğ¼
                    logger.info(f"âš ï¸ {best_match.duplicate_type.title()} duplicate: {best_match.confidence:.0f}% match")
                    comparable_data['possible_duplicate'] = True
                    comparable_data['duplicate_confidence'] = best_match.confidence
                    comparable_data['duplicate_type'] = best_match.duplicate_type

        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² ÑĞ¿Ğ¸ÑĞ¾Ğº
        session_data['comparables'].append(comparable_data)
        session_storage.set(session_id, session_data)

        logger.info(f"âœ… Comparable added to session {session_id}, total: {len(session_data['comparables'])}")

        return jsonify({
            'status': 'success',
            'comparable': comparable_data
        })

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': safe_error_message(e)
        }), 500


@app.route('/api/exclude-comparable', methods=['POST'])
def exclude_comparable():
    """
    API: Ğ˜ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ¸Ğ· Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° (Ğ­ĞºÑ€Ğ°Ğ½ 2)

    Body:
        {
            "session_id": "uuid",
            "index": 3
        }

    Returns:
        {
            "status": "success"
        }
    """
    try:
        payload = request.json
        session_id = payload.get('session_id')
        index = payload.get('index')

        if not session_id or not session_storage.exists(session_id):
            return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

        session_data = session_storage.get(session_id)
        comparables = session_data['comparables']

        if 0 <= index < len(comparables):
            comparables[index]['excluded'] = True
            session_storage.set(session_id, session_data)

        return jsonify({'status': 'success'})

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': safe_error_message(e)
        }), 500


@app.route('/api/include-comparable', methods=['POST'])
def include_comparable():
    """
    API: Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· (Ğ­ĞºÑ€Ğ°Ğ½ 2)

    Body:
        {
            "session_id": "uuid",
            "index": 3
        }

    Returns:
        {
            "status": "success"
        }
    """
    try:
        payload = request.json
        session_id = payload.get('session_id')
        index = payload.get('index')

        if not session_id or not session_storage.exists(session_id):
            return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

        session_data = session_storage.get(session_id)
        comparables = session_data['comparables']

        if 0 <= index < len(comparables):
            comparables[index]['excluded'] = False
            session_storage.set(session_id, session_data)

        return jsonify({'status': 'success'})

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‚Ğ°: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': safe_error_message(e)
        }), 500


@app.route('/api/analyze', methods=['POST'])
@limiter.limit(settings.RATELIMIT_ANALYZE)  # ĞĞ½Ğ°Ğ»Ğ¸Ğ· - Ğ¼ĞµĞ½ĞµĞµ expensive
def analyze():
    """
    API: ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· (Ğ­ĞºÑ€Ğ°Ğ½ 3)

    Body:
        {
            "session_id": "uuid",
            "filter_outliers": true,
            "use_median": true
        }

    Returns:
        {
            "status": "success",
            "analysis": {...}
        }
    """
    try:
        payload = request.json
        session_id = payload.get('session_id')
        filter_outliers = payload.get('filter_outliers', True)
        use_median = payload.get('use_median', True)

        if not session_id or not session_storage.exists(session_id):
            return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

        session_data = session_storage.get(session_id)

        logger.info(f"ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ´Ğ»Ñ ÑĞµÑÑĞ¸Ğ¸ {session_id}")

        # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
        try:
            # Ğ˜Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑƒÑ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            from src.models.property import normalize_property_data, validate_property_consistency

            # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞºÑ‚
            normalized_target = normalize_property_data(session_data['target_property'])
            target_property = TargetProperty(**normalized_target)

            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ĞºĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ
            warnings = validate_property_consistency(target_property)
            if warnings:
                logger.warning(f"ĞŸÑ€ĞµĞ´ÑƒĞ¿Ñ€ĞµĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸: {warnings}")

            # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸
            comparables = [
                ComparableProperty(**normalize_property_data(c))
                for c in session_data['comparables']
            ]

            request_model = AnalysisRequest(
                target_property=target_property,
                comparables=comparables,
                filter_outliers=filter_outliers,
                use_median=use_median
            )

        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸: {e}", exc_info=True)
            return jsonify({
                'status': 'error',
                'error_type': 'data_validation_error',
                'message': 'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….',
                'technical_details': str(e)
            }), 400

        # ĞĞ½Ğ°Ğ»Ğ¸Ğ·
        analyzer = RealEstateAnalyzer()
        try:
            result = analyzer.analyze(request_model)
        except ValueError as ve:
            # PATCH 4: Ğ¡Ğ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸ÑĞ¼Ğ¸
            error_str = str(ve).lower()
            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {ve}")

            # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ‚Ğ¸Ğ¿ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
            if 'Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²' in error_str or 'insufficient' in error_str:
                error_type = 'insufficient_comparables'
                user_message = str(ve)
            elif 'Ñ†ĞµĞ½Ğ°' in error_str or 'price' in error_str:
                error_type = 'invalid_price_data'
                user_message = f'ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ Ñ†ĞµĞ½Ğ°Ñ…: {ve}'
            elif 'Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ' in error_str or 'area' in error_str:
                error_type = 'invalid_area_data'
                user_message = f'ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ¾ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¸: {ve}'
            else:
                error_type = 'validation_error'
                user_message = str(ve)

            return jsonify({
                'status': 'error',
                'error_type': error_type,
                'message': user_message,
                'details': str(ve)
            }), 422
        except Exception as analysis_error:
            # PATCH 4: Ğ›ÑĞ±Ñ‹Ğµ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹
            error_str = str(analysis_error)
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {analysis_error}", exc_info=True)

            # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ñ‚Ğ¸Ğ¿ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
            error_type = 'analysis_error'
            if 'pydantic' in error_str.lower() or 'validation' in error_str.lower():
                error_type = 'data_validation_error'
                user_message = 'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ². ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….'
            elif 'division' in error_str.lower() or 'zerodivision' in error_str.lower():
                error_type = 'calculation_error'
                user_message = 'ĞÑˆĞ¸Ğ±ĞºĞ° Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ¾Ğ². Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾, Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.'
            elif 'key' in error_str.lower():
                error_type = 'missing_data_error'
                user_message = 'ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ñƒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ğ± Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ°Ñ….'
            else:
                user_message = f'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {error_str[:200]}'  # ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñƒ

            return jsonify({
                'status': 'error',
                'error_type': error_type,
                'message': user_message,
                'technical_details': error_str
            }), 500

        # ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ² JSON
        try:
            result_dict = result.dict()
        except Exception as dict_error:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ² dict: {dict_error}", exc_info=True)
            return jsonify({
                'status': 'error',
                'error_type': 'serialization_error',
                'message': 'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°'
            }), 500

        # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ° Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¾Ğ¹
        required_fields = ['market_statistics', 'fair_price_analysis', 'price_scenarios',
                          'strengths_weaknesses', 'target_property']
        missing_fields = [field for field in required_fields if not result_dict.get(field)]

        if missing_fields:
            logger.warning(f"Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° Ğ½Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹: {missing_fields}")
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿ÑƒÑÑ‚Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµĞ¹
            for field in missing_fields:
                if field == 'price_scenarios':
                    result_dict[field] = []
                else:
                    result_dict[field] = {}

        # ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
        try:
            metrics = analyzer.get_metrics()
            result_dict['metrics'] = metrics
        except Exception as metrics_error:
            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº: {metrics_error}")
            result_dict['metrics'] = {}

        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ„Ñ„ĞµÑ€ Housler
        try:
            housler_offer = generate_housler_offer(
                analysis=result_dict,
                property_info=session_data.get('target_property', {}),
                recommendations=result_dict.get('recommendations', [])
            )
            if housler_offer:
                result_dict['housler_offer'] = housler_offer
            else:
                result_dict['housler_offer'] = None
                logger.warning("ĞÑ„Ñ„ĞµÑ€ Ğ½Ğµ ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ (Ğ½ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…)")
        except Exception as offer_error:
            logger.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ„Ñ„ĞµÑ€Ğ°: {offer_error}")
            result_dict['housler_offer'] = None

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² ÑĞµÑÑĞ¸Ñ
        session_data['analysis'] = result_dict
        session_data['step'] = 3
        session_storage.set(session_id, session_data)

        return jsonify({
            'status': 'success',
            'analysis': result_dict
        })

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'error_type': 'internal_error',
            'message': str(e)
        }), 500


@app.route('/api/session/<session_id>', methods=['GET'])
def get_session(session_id):
    """
    API: ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµÑÑĞ¸Ğ¸

    Returns:
        {
            "status": "success",
            "data": {...}
        }
    """
    if not session_storage.exists(session_id):
        return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

    return jsonify({
        'status': 'success',
        'data': session_storage.get(session_id)
    })


@app.route('/api/cache/stats', methods=['GET'])
def cache_stats():
    """
    API: Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° ĞºÑÑˆĞ°

    Returns:
        {
            "status": "success",
            "stats": {
                "status": "active|disabled",
                "hit_rate": 85.5,
                "total_keys": 123,
                ...
            }
        }
    """
    try:
        stats = property_cache.get_stats()
        return jsonify({
            'status': 'success',
            'stats': stats
        })
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ĞºÑÑˆĞ°: {e}")
        return jsonify({
            'status': 'error',
            'message': safe_error_message(e)
        }), 500


@app.route('/api/cache/clear', methods=['POST'])
@limiter.limit("10 per minute")  # Strict limit to prevent API key brute force
def cache_clear():
    """
    API: ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° ĞºÑÑˆĞ° (Ğ´Ğ»Ñ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¾Ğ²)

    Headers:
        X-Admin-Key: <ADMIN_API_KEY from .env>

    Body:
        {
            "pattern": "*"  # optional, default: Ğ²ÑĞµ
        }

    Returns:
        {
            "status": "success",
            "deleted": 42
        }
    """
    try:
        # Check admin authentication
        admin_key = os.environ.get('ADMIN_API_KEY')
        provided_key = request.headers.get('X-Admin-Key')

        if not admin_key:
            logger.warning("ADMIN_API_KEY not configured, cache clear disabled")
            return jsonify({
                'status': 'error',
                'message': 'Admin API not configured'
            }), 503

        if not provided_key or provided_key != admin_key:
            logger.warning(f"Unauthorized cache clear attempt from IP: {request.remote_addr}")
            return jsonify({
                'status': 'error',
                'message': 'Unauthorized'
            }), 401

        pattern = request.json.get('pattern', '*') if request.json else '*'
        deleted = property_cache.clear_all(pattern)

        logger.info(f"Cache cleared by admin, pattern: {pattern}, deleted: {deleted}")

        return jsonify({
            'status': 'success',
            'deleted': deleted,
            'pattern': pattern
        })
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ ĞºÑÑˆĞ°: {e}")
        return jsonify({
            'status': 'error',
            'message': safe_error_message(e)
        }), 500


@app.route('/api/export-report/<session_id>', methods=['GET'])
def export_report(session_id):
    """
    API: Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ² PDF (Ğ­ĞºÑ€Ğ°Ğ½ 3)

    Returns:
        PDF Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¼ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ¾Ğ¼
    """
    try:
        if not session_storage.exists(session_id):
            return jsonify({'status': 'error', 'message': 'Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°'}), 404

        session_data = session_storage.get(session_id)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½
        if 'analysis' not in session_data or not session_data['analysis']:
            return jsonify({
                'status': 'error',
                'message': 'ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½. Ğ¡Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğ° ÑˆĞ°Ğ³Ğµ 3.'
            }), 400

        logger.info(f"Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ PDF Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° Ğ´Ğ»Ñ ÑĞµÑÑĞ¸Ğ¸ {session_id}")

        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ PDF Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Playwright
        from datetime import datetime
        import asyncio

        if not PLAYWRIGHT_AVAILABLE:
            # Fallback to markdown if playwright not available
            logger.warning("Playwright Ğ½Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ markdown")
            return _export_markdown_fallback(session_id, session_data)

        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ PDF Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· HTML (Ğ±ĞµĞ· HTTP Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°)
        try:
            # Ğ ĞµĞ½Ğ´ĞµÑ€Ğ¸Ğ¼ HTML ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ
            analysis = session_data['analysis']
            target = session_data.get('target_property', {})
            comparables = session_data.get('comparables', [])
            comparables = [c for c in comparables if not c.get('excluded', False)]

            housler_offer = generate_housler_offer(
                analysis=analysis,
                property_info=target,
                recommendations=analysis.get('recommendations', [])
            )

            template_data = {
                'date': datetime.now().strftime('%Y-%m-%d %H:%M'),
                'property_info': target,
                'comparables': comparables,
                'fair_price_analysis': analysis.get('fair_price_analysis', {}),
                'market_statistics': analysis.get('market_statistics', {}),
                'recommendations': analysis.get('recommendations', []),
                'price_scenarios': analysis.get('price_scenarios', []),
                'time_forecast': analysis.get('time_forecast', {}),
                'attractiveness_index': analysis.get('attractiveness_index', {}),
                'strengths_weaknesses': analysis.get('strengths_weaknesses', {}),
                'housler_offer': housler_offer
            }

            html_content = render_template('report.html', **template_data)
            pdf_bytes = asyncio.run(_generate_pdf_from_html(html_content))

            # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ PDF Ñ„Ğ°Ğ¹Ğ»
            from flask import Response
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"housler_report_{session_id[:8]}_{timestamp}.pdf"

            return Response(
                pdf_bytes,
                mimetype='application/pdf',
                headers={
                    'Content-Disposition': f'attachment; filename="{filename}"',
                    'Content-Type': 'application/pdf'
                }
            )
        except Exception as pdf_error:
            logger.warning(f"PDF Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ ({pdf_error}), Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ÑÑ Ğ½Ğ° markdown")
            return _export_markdown_fallback(session_id, session_data)

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°: {e}", exc_info=True)
        return jsonify({
            'status': 'error',
            'message': f'ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°: {safe_error_message(e)}'
        }), 500


@app.route('/report/<session_id>', methods=['GET'])
def view_report(session_id):
    """
    ĞŸÑ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ HTML Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ° (Ğ´Ğ»Ñ PDF Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸)
    """
    try:
        if not session_storage.exists(session_id):
            return "Ğ¡ĞµÑÑĞ¸Ñ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°", 404

        session_data = session_storage.get(session_id)

        if 'analysis' not in session_data or not session_data['analysis']:
            return "ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½", 400

        analysis = session_data['analysis']
        target = session_data.get('target_property', {})
        comparables = session_data.get('comparables', [])

        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ğ¸ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸
        comparables = [c for c in comparables if not c.get('excluded', False)]

        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ„Ñ„ĞµÑ€ Housler
        housler_offer = generate_housler_offer(
            analysis=analysis,
            property_info=target,
            recommendations=analysis.get('recommendations', [])
        )

        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ°
        template_data = {
            'date': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'property_info': target,
            'comparables': comparables,
            'fair_price_analysis': analysis.get('fair_price_analysis', {}),
            'market_statistics': analysis.get('market_statistics', {}),
            'recommendations': analysis.get('recommendations', []),
            'price_scenarios': analysis.get('price_scenarios', []),
            'time_forecast': analysis.get('time_forecast', {}),
            'attractiveness_index': analysis.get('attractiveness_index', {}),
            'strengths_weaknesses': analysis.get('strengths_weaknesses', {}),
            'housler_offer': housler_offer
        }

        return render_template('report.html', **template_data)

    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°: {e}", exc_info=True)
        return f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¾Ñ‚Ñ‡ĞµÑ‚Ğ°: {safe_error_message(e)}", 500


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONTACT ROUTES - Ğ¿ĞµÑ€ĞµĞ½ĞµÑĞµĞ½Ñ‹ Ğ² src/routes/contacts.py
# ĞœĞ°Ñ€ÑˆÑ€ÑƒÑ‚Ñ‹ /api/contact-request Ğ¸ /api/client-request Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ² contacts_bp
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


async def _generate_pdf_from_html(html_content: str) -> bytes:
    """
    Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ PDF Ğ¸Ğ· HTML ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Playwright (Ğ±ĞµĞ· HTTP Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°)
    """
    from playwright.async_api import async_playwright

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()

        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ HTML Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ
        logger.info("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ HTML Ğ´Ğ»Ñ PDF...")
        await page.set_content(html_content, wait_until='networkidle', timeout=60000)

        # Ğ”Ğ°ĞµĞ¼ Ğ²Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ ÑˆÑ€Ğ¸Ñ„Ñ‚Ğ¾Ğ² Ğ¸ ÑÑ‚Ğ¸Ğ»ĞµĞ¹ Ğ¸Ğ· CDN
        await page.wait_for_timeout(2000)

        # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ PDF
        logger.info("Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ PDF...")
        pdf_bytes = await page.pdf(
            format='A4',
            margin={
                'top': '15mm',
                'right': '12mm',
                'bottom': '15mm',
                'left': '12mm'
            },
            print_background=True,
            prefer_css_page_size=False,
            display_header_footer=False
        )

        await browser.close()
        logger.info(f"PDF ÑĞ³ĞµĞ½ĞµÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½, Ñ€Ğ°Ğ·Ğ¼ĞµÑ€: {len(pdf_bytes)} bytes")
        return pdf_bytes


def _export_markdown_fallback(session_id: str, session_data: dict):
    """
    Fallback Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ğ° Ğ² markdown ĞµÑĞ»Ğ¸ playwright Ğ½Ğµ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½
    """
    from src.analytics.property_tracker import PropertyLog
    from src.analytics.markdown_exporter import MarkdownExporter
    from datetime import datetime
    from flask import Response

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ PropertyLog Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… ÑĞµÑÑĞ¸Ğ¸
    property_log = PropertyLog(
        property_id=session_id,
        url=session_data.get('target_property', {}).get('url'),
        started_at=datetime.now().isoformat(),
        completed_at=datetime.now().isoformat(),
        status='completed'
    )

    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğµ
    target = session_data.get('target_property', {})
    property_log.property_info = {
        'price': target.get('price'),
        'total_area': target.get('total_area'),
        'rooms': target.get('rooms'),
        'floor': target.get('floor'),
        'total_floors': target.get('total_floors'),
        'address': target.get('address')
    }

    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸
    comparables = session_data.get('comparables', [])
    property_log.comparables_data = [
        {
            'price': c.get('price'),
            'total_area': c.get('total_area'),
            'price_per_sqm': c.get('price_per_sqm'),
            'address': c.get('address'),
            'url': c.get('url')
        }
        for c in comparables
        if not c.get('excluded', False)
    ]

    # Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
    analysis = session_data['analysis']
    if 'market_statistics' in analysis:
        property_log.market_stats = analysis['market_statistics']
    if 'fair_price_analysis' in analysis:
        property_log.fair_price_result = analysis['fair_price_analysis']
    if 'price_range' in analysis:
        property_log.price_range = analysis['price_range']
    if 'attractiveness_index' in analysis:
        property_log.attractiveness_index = analysis['attractiveness_index']
    if 'time_forecast' in analysis:
        property_log.time_forecast = analysis['time_forecast']
    if 'price_sensitivity' in analysis:
        property_log.price_sensitivity = analysis['price_sensitivity']
    if 'price_scenarios' in analysis:
        property_log.scenarios = analysis['price_scenarios']
    if 'adjustments_applied' in analysis:
        property_log.adjustments = analysis['adjustments_applied']
    if 'metrics' in analysis:
        property_log.metrics = analysis['metrics']
    if 'recommendations' in analysis:
        property_log.recommendations = analysis['recommendations']

    # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Markdown Ğ¾Ñ‚Ñ‡ĞµÑ‚
    exporter = MarkdownExporter()
    markdown_content = exporter.export_single_property(property_log)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"housler_report_{session_id[:8]}_{timestamp}.md"

    return Response(
        markdown_content,
        mimetype='text/markdown',
        headers={
            'Content-Disposition': f'attachment; filename="{filename}"',
            'Content-Type': 'text/markdown; charset=utf-8'
        }
    )


def _identify_missing_fields(parsed_data: Dict) -> List[Dict]:
    """
    ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ñ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°

    Returns:
        Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹ Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ Ğ½ĞµĞ´Ğ¾ÑÑ‚Ğ°ÑÑ‰Ğ¸Ñ… Ğ¿Ğ¾Ğ»ÑÑ…
    """
    missing = []

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ĞĞĞ’ĞĞ¯ ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ ĞĞĞ¯ Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ ĞŸĞĞ›Ğ•Ğ™ (6 ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ¾Ğ², 20 Ğ¿Ğ¾Ğ»ĞµĞ¹)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    required_fields = [
        # ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ  1: ĞĞ¢Ğ”Ğ•Ğ›ĞšĞ
        {
            'field': 'repair_level',
            'label': 'ğŸ¨ Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¾Ñ‚Ğ´ĞµĞ»ĞºĞ¸',
            'type': 'select',
            'options': ['Ñ‡ĞµÑ€Ğ½Ğ¾Ğ²Ğ°Ñ', 'ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ', 'ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ', 'Ğ¿Ñ€ĞµĞ¼Ğ¸ÑƒĞ¼', 'Ğ»ÑĞºÑ'],
            'description': 'ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ĞºĞ¸ (Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€ Ñ†ĞµĞ½Ñ‹)',
            'default': 'ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ°Ñ'
        },

        # ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ  2: Ğ¥ĞĞ ĞĞšĞ¢Ğ•Ğ Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ˜ ĞšĞ’ĞĞ Ğ¢Ğ˜Ğ Ğ«
        {
            'field': 'ceiling_height',
            'label': 'ğŸ“ Ğ’Ñ‹ÑĞ¾Ñ‚Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğ»ĞºĞ¾Ğ², Ğ¼',
            'type': 'number',
            'description': 'Ğ’Ñ‹ÑĞ¾Ñ‚Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğ»ĞºĞ¾Ğ² Ğ² Ğ¼ĞµÑ‚Ñ€Ğ°Ñ… (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, 2.7)',
            'default': 2.7
        },
        {
            'field': 'bathrooms',
            'label': 'ğŸš¿ ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ°Ğ½ÑƒĞ·Ğ»Ğ¾Ğ²',
            'type': 'select',
            'options': ['1', '2', '3'],
            'description': 'ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ…/ÑĞ°Ğ½ÑƒĞ·Ğ»Ğ¾Ğ²',
            'default': '1'
        },
        {
            'field': 'window_type',
            'label': 'ğŸªŸ Ğ¢Ğ¸Ğ¿ Ğ¾ĞºĞ¾Ğ½',
            'type': 'select',
            'options': ['Ğ´ĞµÑ€ĞµĞ²ÑĞ½Ğ½Ñ‹Ğµ', 'Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸ĞºĞ¾Ğ²Ñ‹Ğµ', 'Ğ¿Ğ°Ğ½Ğ¾Ñ€Ğ°Ğ¼Ğ½Ñ‹Ğµ'],
            'description': 'Ğ¢Ğ¸Ğ¿ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºĞ¾Ğ½',
            'default': 'Ğ¿Ğ»Ğ°ÑÑ‚Ğ¸ĞºĞ¾Ğ²Ñ‹Ğµ'
        },
        {
            'field': 'elevator_count',
            'label': 'ğŸ›— Ğ›Ğ¸Ñ„Ñ‚Ñ‹',
            'type': 'select',
            'options': ['0', '1', '2+'],
            'description': 'ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¸Ñ„Ñ‚Ğ¾Ğ² Ğ² Ğ´Ğ¾Ğ¼Ğµ',
            'default': '1'
        },
        {
            'field': 'living_area',
            'label': 'ğŸ  Ğ–Ğ¸Ğ»Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ, Ğ¼Â²',
            'type': 'number',
            'description': 'Ğ–Ğ¸Ğ»Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ ĞºĞ²Ğ°Ñ€Ñ‚Ğ¸Ñ€Ñ‹ (Ğ±ĞµĞ· ĞºÑƒÑ…Ğ½Ğ¸ Ğ¸ ĞºĞ¾Ñ€Ğ¸Ğ´Ğ¾Ñ€Ğ¾Ğ²)',
            'default': None
        },

        # ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ  3: Ğ’Ğ˜Ğ” Ğ˜ Ğ­Ğ¡Ğ¢Ğ•Ğ¢Ğ˜ĞšĞ
        {
            'field': 'view_type',
            'label': 'ğŸŒ… Ğ’Ğ¸Ğ´ Ğ¸Ğ· Ğ¾ĞºĞ½Ğ°',
            'type': 'select',
            'options': ['Ğ´Ğ¾Ğ¼', 'ÑƒĞ»Ğ¸Ñ†Ğ°', 'Ğ¿Ğ°Ñ€Ğº', 'Ğ²Ğ¾Ğ´Ğ°', 'Ğ³Ğ¾Ñ€Ğ¾Ğ´', 'Ğ·Ğ°ĞºĞ°Ñ‚', 'Ğ¿Ñ€ĞµĞ¼Ğ¸ÑƒĞ¼'],
            'description': 'Ğ§Ñ‚Ğ¾ Ğ²Ğ¸Ğ´Ğ½Ğ¾ Ğ¸Ğ· Ğ¾ĞºĞ¾Ğ½ (Ğ²Ğ»Ğ¸ÑĞµÑ‚ Ğ½Ğ° ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ)',
            'default': 'ÑƒĞ»Ğ¸Ñ†Ğ°'
        },

        # ĞšĞ›ĞĞ¡Ğ¢Ğ•Ğ  4: Ğ Ğ˜Ğ¡ĞšĞ˜ Ğ˜ ĞšĞĞ§Ğ•Ğ¡Ğ¢Ğ’Ğ ĞœĞĞ¢Ğ•Ğ Ğ˜ĞĞ›ĞĞ’
        {
            'field': 'material_quality',
            'label': 'ğŸ“¸ ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ñ„Ğ¾Ñ‚Ğ¾/Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ğ¾Ğ²',
            'type': 'select',
            'options': ['ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ_Ñ„Ğ¾Ñ‚Ğ¾_Ğ²Ğ¸Ğ´ĞµĞ¾', 'ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ_Ñ„Ğ¾Ñ‚Ğ¾', 'Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾_Ñ€ĞµĞ½Ğ´ĞµÑ€Ñ‹', 'Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾_Ğ¿Ğ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°'],
            'description': 'ĞšĞ°ĞºĞ¸Ğµ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ² Ğ¾Ğ±ÑŠÑĞ²Ğ»ĞµĞ½Ğ¸Ğ¸',
            'default': 'ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ_Ñ„Ğ¾Ñ‚Ğ¾'
        },
        {
            'field': 'ownership_status',
            'label': 'ğŸ“‹ Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸',
            'type': 'select',
            'options': ['1_ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸Ğº_Ğ±ĞµĞ·_Ğ¾Ğ±Ñ€ĞµĞ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹', '1+_ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸ĞºĞ¾Ğ²_Ğ±ĞµĞ·_Ğ¾Ğ±Ñ€ĞµĞ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹', 'Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞºĞ°_Ñ€Ğ°ÑÑÑ€Ğ¾Ñ‡ĞºĞ°', 'ĞµÑÑ‚ÑŒ_Ğ¾Ğ±Ñ€ĞµĞ¼ĞµĞ½ĞµĞ½Ğ¸Ñ'],
            'description': 'Ğ®Ñ€Ğ¸Ğ´Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ°',
            'default': '1_ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¸Ğº_Ğ±ĞµĞ·_Ğ¾Ğ±Ñ€ĞµĞ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹'
        },
    ]

    # ĞœĞ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³ Ğ¿Ğ¾Ğ»ĞµĞ¹ Ğ½Ğ° Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸
    characteristics_mapping = {
        'ceiling_height': 'Ğ’Ñ‹ÑĞ¾Ñ‚Ğ° Ğ¿Ğ¾Ñ‚Ğ¾Ğ»ĞºĞ¾Ğ²',
        'build_year': 'Ğ“Ğ¾Ğ´ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸',
        'house_type': 'Ğ¢Ğ¸Ğ¿ Ğ´Ğ¾Ğ¼Ğ°',
        'has_elevator': 'ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¸Ñ„Ñ‚Ğ¾Ğ²',
        'elevator_count': 'ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ»Ğ¸Ñ„Ñ‚Ğ¾Ğ²',
        'living_area': 'Ğ–Ğ¸Ğ»Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ',
        'bathrooms': 'Ğ¡Ğ°Ğ½ÑƒĞ·ĞµĞ»',
        'window_type': 'ĞĞºĞ½Ğ°',
    }

    characteristics = parsed_data.get('characteristics', {})

    for field_info in required_fields:
        field = field_info['field']

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ² ĞºĞ¾Ñ€Ğ½Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        if field in parsed_data and parsed_data[field] is not None:
            continue

        # Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ² characteristics
        char_key = characteristics_mapping.get(field)
        if char_key and char_key in characteristics:
            # ĞŸĞ¾Ğ»Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ² characteristics - Ğ½Ğµ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² missing
            continue

        # ĞŸĞ¾Ğ»Ğµ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ - Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² missing
        missing.append(field_info)

    return missing


# CLEANUP: Shutdown handler Ğ´Ğ»Ñ browser pool
import atexit
import signal

def shutdown_browser_pool():
    """Ğ—Ğ°ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ browser pool Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ"""
    if browser_pool:
        logger.info("Shutting down browser pool...")
        try:
            browser_pool.shutdown()
        except Exception as e:
            logger.error(f"Error shutting down browser pool: {e}")

# Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ
atexit.register(shutdown_browser_pool)

def signal_handler(signum, frame):
    """ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ graceful shutdown"""
    logger.info(f"Received signal {signum}, shutting down...")
    shutdown_browser_pool()
    exit(0)

# Ğ ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ğ¾Ğ²
signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)


if __name__ == '__main__':
    try:
        app.run(debug=True, host='0.0.0.0', port=5002)
    finally:
        shutdown_browser_pool()
