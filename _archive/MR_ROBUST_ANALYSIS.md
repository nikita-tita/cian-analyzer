# MR: Robust Analysis - Graceful Degradation –¥–ª—è –®–∞–≥–∞ 3

## üìã –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

- **–¢–∏–ø**: Critical Bugfix + Enhancement
- **–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: P0 (Critical)
- **–ó–∞—Ç—Ä–∞—Ç—ã**: 6-8 —á–∞—Å–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ + 2-3 —á–∞—Å–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
- **–†–∏—Å–∫–∏**: –ù–∏–∑–∫–∏–µ (backward compatible, graceful degradation)
- **–°–≤—è–∑–∞–Ω–Ω—ã–µ issue**: [–®–∞–≥ 3 –ø–∞–¥–∞–µ—Ç –ø—Ä–∏ –º–∞–ª–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –≤–∞–ª–∏–¥–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–æ–≤]

## üéØ –¶–µ–ª—å

–ò—Å–ø—Ä–∞–≤–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –ø—Ä–æ–±–ª–µ–º—É "–≤–æ—Ä–æ–Ω–∫–∏ –ø–æ—Ç–µ—Ä—å" –∞–Ω–∞–ª–æ–≥–æ–≤, –∫–æ–≥–¥–∞:
- –ù–∞–π–¥–µ–Ω–æ 8 –∞–Ω–∞–ª–æ–≥–æ–≤ –Ω–∞ –®–∞–≥–µ 2 ‚úÖ
- –ü–æ—Å–ª–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Å—Ç–∞–µ—Ç—Å—è 0-2 –≤–∞–ª–∏–¥–Ω—ã—Ö ‚ùå
- –®–∞–≥ 3 –ø–∞–¥–∞–µ—Ç —Å ValidationError ‚ùå
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–∏–¥–∏—Ç "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫" ‚ùå

**–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ —Ñ–∏–∫—Å–∞:**
- –ù–∞–π–¥–µ–Ω–æ 8 –∞–Ω–∞–ª–æ–≥–æ–≤ ‚Üí –®–∞–≥ 3 —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤–∞–ª–∏–¥–Ω—ã—Ö (1+) ‚úÖ
- –ü—Ä–∏ n=1-2: –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏ –æ —Ç–æ—á–Ω–æ—Å—Ç–∏ ‚úÖ
- –ü—Ä–∏ n‚â•3: –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π —Ä–æ–±–∞—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ‚úÖ

## üìä –¢–µ–∫—É—â–∞—è –ø—Ä–æ–±–ª–µ–º–∞ (Root Cause)

```
8 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–æ–≤
  ‚îÇ
  ‚îú‚îÄ> –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –ø–∞–¥–∞–µ—Ç (Timeout/RateLimit/Captcha)
  ‚îÇ   ‚îî‚îÄ> 8 –∞–Ω–∞–ª–æ–≥–æ–≤ –ë–ï–ó price/total_area
  ‚îÇ
  ‚îú‚îÄ> –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ Pydantic –æ—Ç–∫–ª–æ–Ω—è–µ—Ç –≤—Å–µ
  ‚îÇ   ‚îî‚îÄ> ValidationError: "price field required"
  ‚îÇ
  ‚îî‚îÄ> –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º—É–º–∞: 0 < 3 ‚Üí ValueError
      ‚îî‚îÄ> Frontend: "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞" ‚ùå
```

## üîß –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è

### –ü—Ä–∏–Ω—Ü–∏–ø—ã:
1. **Never fail hard** - –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–ø—É—Å—Ç—å —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏)
2. **Graceful degradation** - –∫–∞—á–µ—Å—Ç–≤–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è, –Ω–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª —Ä–∞–±–æ—Ç–∞–µ—Ç
3. **Transparent quality** - –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Ä–µ–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
4. **Robust statistics** - –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏

### –ò–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ —Ñ–∞–π–ª–∞–º:

```
src/parsers/async_parser.py      - Robust parallel parsing (retry + partials)
src/models/property.py            - Soft validation + field recovery
src/analytics/analyzer.py         - Adaptive filtering + min threshold = 1
app_new.py                        - Integration + quality metrics
```

---

## üìù –ü–∞—Ç—á 1: Robust Parallel Parsing

**–§–∞–π–ª**: `src/parsers/async_parser.py`

**–ü—Ä–æ–±–ª–µ–º–∞**:
- –ü—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏ parse_multiple_urls_parallel() –≤—Å–µ –∞–Ω–∞–ª–æ–≥–∏ —Ç–µ—Ä—è—é—Ç –¥–∞–Ω–Ω—ã–µ
- –ù–µ—Ç retry –ª–æ–≥–∏–∫–∏ –¥–ª—è rate limiting/timeout
- Exception —É–±–∏–≤–∞–µ—Ç –≤–µ—Å—å pipeline

**–†–µ—à–µ–Ω–∏–µ**:
- Retry —Å exponential backoff
- –í–æ–∑–≤—Ä–∞—Ç partial results –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
- –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—É—Å—ã –æ—à–∏–±–æ–∫

### Diff:

```python
# –î–û–ë–ê–í–ò–¢–¨ –í –ù–ê–ß–ê–õ–û –§–ê–ô–õ–ê

from dataclasses import dataclass
from typing import Optional
import random
import time

@dataclass
class ParseResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ URL (—É—Å–ø–µ—à–Ω—ã–π –∏–ª–∏ –Ω–µ—Ç)"""
    url: str
    ok: bool
    data: dict
    error_type: Optional[str] = None  # "rate_limited" | "timeout" | "captcha" | "parse_error"
    error_message: Optional[str] = None
    retries_used: int = 0

class RateLimitError(Exception):
    """–¶–∏–∞–Ω –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã"""
    pass

class CaptchaError(Exception):
    """–¢—Ä–µ–±—É–µ—Ç—Å—è —Ä–µ—à–µ–Ω–∏–µ –∫–∞–ø—á–∏"""
    pass
```

```python
# –ó–ê–ú–ï–ù–ò–¢–¨ –§–£–ù–ö–¶–ò–Æ parse_multiple_urls_parallel

def parse_multiple_urls_parallel(
    urls: list[str],
    headless: bool = True,
    cache=None,
    region: str = 'spb',
    max_concurrent: int = 2,  # –°–ù–ò–ñ–ï–ù–û –° 5 –î–û 2
    max_retries: int = 3,
    base_delay: float = 1.5,
    timeout_per_url: int = 15
) -> list[ParseResult]:
    """
    –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å retry –∏ partial results

    –ò–ó–ú–ï–ù–ï–ù–ò–Ø:
    - –ù–µ –ø–∞–¥–∞–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç partial results
    - Retry –¥–ª—è rate limiting –∏ timeout
    - Exponential backoff —Å jitter
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—É—Å—ã –æ—à–∏–±–æ–∫

    Args:
        urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å (—Å–Ω–∏–∂–µ–Ω–æ –¥–æ 2)
        max_retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        base_delay: –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        timeout_per_url: –¢–∞–π–º–∞—É—Ç –Ω–∞ –æ–¥–∏–Ω URL

    Returns:
        –°–ø–∏—Å–æ–∫ ParseResult (–≤–∫–ª—é—á–∞—è failed)
    """
    from src.parsers.playwright_parser import PlaywrightParser
    import logging

    logger = logging.getLogger(__name__)
    results = []

    logger.info(f"üöÄ –†–æ–±–∞—Å—Ç–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ {len(urls)} URLs (max_concurrent={max_concurrent})")

    # –ë–∞—Ç—á–∏–Ω–≥: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º max_concurrent URLs –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    for batch_start in range(0, len(urls), max_concurrent):
        batch_urls = urls[batch_start:batch_start + max_concurrent]

        for url in batch_urls:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            if cache:
                cached = cache.get_property(url)
                if cached:
                    results.append(ParseResult(
                        url=url,
                        ok=True,
                        data=cached,
                        error_type=None
                    ))
                    logger.info(f"‚úÖ Cache hit: {url[:60]}...")
                    continue

            # Retry loop
            attempt = 0
            parse_success = False
            last_error = None

            while attempt <= max_retries and not parse_success:
                try:
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ —Å jitter (–∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–∏)
                    if attempt > 0:
                        delay = base_delay * (2 ** attempt) + random.uniform(0, 0.5)
                        logger.info(f"‚è≥ Retry {attempt}/{max_retries} –¥–ª—è {url[:60]}... (delay={delay:.1f}s)")
                        time.sleep(delay)

                    # –ü–∞—Ä—Å–∏–º —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                    with PlaywrightParser(headless=headless, cache=cache, region=region) as parser:
                        data = parser.parse_detail_page(url)

                        # –£—Å–ø–µ—Ö!
                        results.append(ParseResult(
                            url=url,
                            ok=True,
                            data=data,
                            error_type=None,
                            retries_used=attempt
                        ))
                        parse_success = True

                        if attempt > 0:
                            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π retry –¥–ª—è {url[:60]}... (–ø–æ–ø—ã—Ç–∫–∞ {attempt})")

                except TimeoutError as e:
                    last_error = e
                    attempt += 1
                    if attempt > max_retries:
                        logger.warning(f"‚ùå Timeout –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url[:60]}...")
                        results.append(ParseResult(
                            url=url,
                            ok=False,
                            data={},
                            error_type="timeout",
                            error_message=str(e),
                            retries_used=attempt
                        ))

                except RateLimitError as e:
                    last_error = e
                    attempt += 1
                    # –î–ª—è rate limit - –±–æ–ª—å—à–µ –∑–∞–¥–µ—Ä–∂–∫–∞
                    if attempt <= max_retries:
                        delay = base_delay * (3 ** attempt) + random.uniform(0, 1)
                        logger.warning(f"‚ö†Ô∏è Rate limit, –∂–¥–µ–º {delay:.1f}s –ø–µ—Ä–µ–¥ retry {attempt}/{max_retries}")
                        time.sleep(delay)
                    else:
                        logger.warning(f"‚ùå Rate limit –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {url[:60]}...")
                        results.append(ParseResult(
                            url=url,
                            ok=False,
                            data={},
                            error_type="rate_limited",
                            error_message=str(e),
                            retries_used=attempt
                        ))

                except CaptchaError as e:
                    # –ö–∞–ø—á–∞ - —Å—Ä–∞–∑—É –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –±–µ–∑ retry
                    logger.warning(f"‚ùå Captcha –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞: {url[:60]}...")
                    results.append(ParseResult(
                        url=url,
                        ok=False,
                        data={},
                        error_type="captcha",
                        error_message=str(e),
                        retries_used=0
                    ))
                    break

                except Exception as e:
                    last_error = e
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url[:60]}...: {e}")
                    results.append(ParseResult(
                        url=url,
                        ok=False,
                        data={},
                        error_type="parse_error",
                        error_message=str(e),
                        retries_used=attempt
                    ))
                    break

        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
        if batch_start + max_concurrent < len(urls):
            time.sleep(base_delay)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    success_count = sum(1 for r in results if r.ok)
    failed_count = len(results) - success_count

    logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞: {success_count}/{len(results)} —É—Å–ø–µ—à–Ω–æ, {failed_count} –æ—à–∏–±–æ–∫")

    if failed_count > 0:
        error_types = {}
        for r in results:
            if not r.ok:
                error_types[r.error_type] = error_types.get(r.error_type, 0) + 1
        logger.warning(f"   –¢–∏–ø—ã –æ—à–∏–±–æ–∫: {error_types}")

    return results
```

**–ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –≤—ã–∑–æ–≤–µ** (`app_new.py`):

```python
# –°–¢–ê–†–´–ô –ö–û–î (—Å—Ç—Ä–æ–∫–∞ ~870):
# detailed_results = parse_multiple_urls_parallel(...)
# except Exception as e:
#     logger.error(f"‚ùå Parallel parsing failed: {e}")

# –ù–û–í–´–ô –ö–û–î:
parse_results = parse_multiple_urls_parallel(
    urls=urls_to_parse,
    headless=True,
    cache=property_cache,
    region=region,
    max_concurrent=2,  # –°–Ω–∏–∂–µ–Ω–æ —Å 5
    max_retries=3,
    base_delay=1.5
)

# –û–±–Ω–æ–≤–ª—è–µ–º –∞–Ω–∞–ª–æ–≥–∏ –¥–∞–∂–µ —Å partial data
updated_count = 0
failed_count = 0

for result in parse_results:
    if result.ok:
        # –£—Å–ø–µ—à–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ - –æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é
        for comparable in similar:
            if comparable.get('url') == result.url:
                comparable.update(result.data)
                updated_count += 1
                break
    else:
        # –ù–µ—É–¥–∞—á–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ - –ø–æ–º–µ—á–∞–µ–º, –Ω–æ –Ω–µ —É–¥–∞–ª—è–µ–º
        failed_count += 1
        for comparable in similar:
            if comparable.get('url') == result.url:
                comparable['_parse_failed'] = True
                comparable['_parse_error'] = result.error_type
                break

logger.info(f"‚úÖ Enhanced {updated_count}/{len(similar)} comparables")
if failed_count > 0:
    logger.warning(f"‚ö†Ô∏è {failed_count} URLs –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω–æ")
```

---

## üìù –ü–∞—Ç—á 2: Field Recovery & Soft Validation

**–§–∞–π–ª**: `src/models/property.py`

**–ü—Ä–æ–±–ª–µ–º–∞**:
- Pydantic –∂–µ—Å—Ç–∫–æ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç: –Ω–µ—Ç price ‚Üí ValidationError
- –ù–µ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º price –∏–∑ price_per_sqm * area
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ–¥–Ω–æ–≥–æ –ø–æ–ª—è —É–±–∏–≤–∞–µ—Ç –≤–µ—Å—å –æ–±—ä–µ–∫—Ç

**–†–µ—à–µ–Ω–∏–µ**:
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–æ–ª–µ–π
- Soft validation —á–µ—Ä–µ–∑ quality_flags
- –ú–∏–Ω–∏–º—É–º –¥–ª—è —Ä–∞–±–æ—Ç—ã: (price & area) –ò–õ–ò (price_per_sqm & area)

### Diff:

```python
# –í —Ñ—É–Ω–∫—Ü–∏—é normalize_property_data (—Å—Ç—Ä–æ–∫–∞ ~310)

def normalize_property_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ —Å —É–º–Ω—ã–º–∏ –¥–µ—Ñ–æ–ª—Ç–∞–º–∏

    –ù–û–í–û–ï: –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–æ–ª–µ–π –∏–∑ –∏–º–µ—é—â–∏—Ö—Å—è
    """
    normalized = data.copy()

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # –ü–ê–¢–ß: –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ò–ï –ö–†–ò–¢–ò–ß–ï–°–ö–ò–• –ü–û–õ–ï–ô
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # 1. –£–Ω–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–º–µ–Ω (price_raw ‚Üí price, area_value ‚Üí total_area)
    if not normalized.get('price') and normalized.get('price_raw'):
        normalized['price'] = normalized['price_raw']

    if not normalized.get('total_area') and normalized.get('area_value'):
        normalized['total_area'] = normalized['area_value']

    # 2. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ price_per_sqm –∏–∑ price / area
    if not normalized.get('price_per_sqm'):
        price = normalized.get('price')
        area = normalized.get('total_area')
        if price and area and area > 0:
            try:
                normalized['price_per_sqm'] = float(price) / float(area)
            except (ValueError, ZeroDivisionError):
                pass

    # 3. –ù–û–í–û–ï: –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ price –∏–∑ price_per_sqm * area
    if not normalized.get('price'):
        ppsm = normalized.get('price_per_sqm')
        area = normalized.get('total_area')
        if ppsm and area and area > 0:
            try:
                normalized['price'] = float(ppsm) * float(area)
                logger.info(f"‚úì –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —Ü–µ–Ω–∞ –∏–∑ price_per_sqm: {normalized['price']:,.0f} ‚ÇΩ")
            except (ValueError, TypeError):
                pass

    # 4. –ù–û–í–û–ï: –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ total_area –∏–∑ price / price_per_sqm
    if not normalized.get('total_area'):
        price = normalized.get('price')
        ppsm = normalized.get('price_per_sqm')
        if price and ppsm and ppsm > 0:
            try:
                normalized['total_area'] = float(price) / float(ppsm)
                logger.info(f"‚úì –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–ª–æ—â–∞–¥—å –∏–∑ price/ppsm: {normalized['total_area']:.1f} –º¬≤")
            except (ValueError, ZeroDivisionError, TypeError):
                pass

    # –û—Å—Ç–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–≤—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤, —Å–∞–Ω—É–∑–ª—ã –∏ —Ç.–¥.)
    # ... (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥)

    return normalized
```

```python
# –í –∫–ª–∞—Å—Å ComparableProperty (–ø–æ—Å–ª–µ —Å—Ç—Ä–æ–∫–∏ ~215)

class ComparableProperty(TargetProperty):
    """
    –ê–Ω–∞–ª–æ–≥ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è

    –ù–û–í–û–ï: Soft validation + quality tracking
    """

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    quality_flags: List[str] = []  # ["insufficient_data", "recovered_price", ...]
    data_completeness: float = 0.0  # 0.0 - 1.0

    @root_validator
    def validate_minimum_data(cls, values):
        """
        –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ª–∏–±–æ (price & area), –ª–∏–±–æ (ppsm & area)

        –í–º–µ—Å—Ç–æ ValidationError - –ø–æ–º–µ—á–∞–µ–º quality_flags
        """
        price = values.get('price')
        area = values.get('total_area')
        ppsm = values.get('price_per_sqm')
        flags = values.get('quality_flags', [])

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º—É–º–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞
        has_price_area = bool(price and area and area > 0)
        has_ppsm_area = bool(ppsm and area and area > 0)

        if not (has_price_area or has_ppsm_area):
            # –ù–ï –±—Ä–æ—Å–∞–µ–º ValidationError - –ø–æ–º–µ—á–∞–µ–º —Ñ–ª–∞–≥–æ–º
            flags.append('insufficient_numeric_fields')
            logger.warning(f"‚ö†Ô∏è –ê–Ω–∞–ª–æ–≥ {values.get('url', '?')[:50]} –∏–º–µ–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö")

        # –í—ã—á–∏—Å–ª—è–µ–º completeness
        required_fields = ['price', 'total_area', 'price_per_sqm', 'rooms', 'address']
        present_count = sum(1 for f in required_fields if values.get(f))
        values['data_completeness'] = present_count / len(required_fields)

        values['quality_flags'] = flags
        return values

    def is_usable_for_analysis(self) -> bool:
        """–ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤"""
        return 'insufficient_numeric_fields' not in self.quality_flags
```

---

## üìù –ü–∞—Ç—á 3: Adaptive Filtering & Min Threshold = 1

**–§–∞–π–ª**: `src/analytics/analyzer.py`

**–ü—Ä–æ–±–ª–µ–º–∞**:
- –¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 3 –∞–Ω–∞–ª–æ–≥–∞ ‚Üí ValueError –ø—Ä–∏ n < 3
- IQR –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞, –¥–∞–∂–µ –ø—Ä–∏ –º–∞–ª–æ–º n
- –ù–µ—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–æ–¥ —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏

**–†–µ—à–µ–Ω–∏–µ**:
- –ú–∏–Ω–∏–º—É–º = 1 –∞–Ω–∞–ª–æ–≥ (—Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏)
- IQR —Ç–æ–ª—å–∫–æ –ø—Ä–∏ n ‚â• 5
- –†–æ–±–∞—Å—Ç–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫

### Diff:

```python
# –í –º–µ—Ç–æ–¥–µ analyze() (—Å—Ç—Ä–æ–∫–∞ ~250)

# –°–¢–ê–†–´–ô –ö–û–î:
# min_comparables_required = 3
# if len(self.filtered_comparables) < min_comparables_required:
#     raise ValueError(...)

# –ù–û–í–´–ô –ö–û–î:
min_comparables_required = 1  # –°–ù–ò–ñ–ï–ù–û –° 3 –î–û 1

n_valid = len(self.filtered_comparables)

if n_valid < min_comparables_required:
    error_msg = (
        f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∞–Ω–∞–ª–æ–≥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. "
        f"–í—Å–µ {len(request.comparables)} –∞–Ω–∞–ª–æ–≥–æ–≤ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é. "
        f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –∞–Ω–∞–ª–æ–≥–∏ –≤—Ä—É—á–Ω—É—é –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ–∏—Å–∫–∞."
    )
    if self.enable_tracking:
        self._log_event(EventType.ANALYSIS_COMPLETED,
            f"–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω: {error_msg}",
            {'error': 'no_valid_comparables'})
        self.tracker.complete_property(self.property_id, "failed")

    raise ValueError(error_msg)

# –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö
if n_valid == 1:
    logger.warning("‚ö†Ô∏è –¢–û–õ–¨–ö–û 1 –í–ê–õ–ò–î–ù–´–ô –ê–ù–ê–õ–û–ì - –æ—Ü–µ–Ω–∫–∞ –±—É–¥–µ—Ç –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ–π")
    if self.enable_tracking:
        self._log_event(EventType.ANALYSIS_COMPLETED,
            "–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å 1 –∞–Ω–∞–ª–æ–≥–æ–º - —Ç–æ—á–Ω–æ—Å—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞",
            {'warning': 'single_comparable'})

elif n_valid == 2:
    logger.warning("‚ö†Ô∏è –¢–û–õ–¨–ö–û 2 –í–ê–õ–ò–î–ù–´–• –ê–ù–ê–õ–û–ì–ê - —Ç–æ—á–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–µ–Ω–∞")
    if self.enable_tracking:
        self._log_event(EventType.ANALYSIS_COMPLETED,
            "–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —Å 2 –∞–Ω–∞–ª–æ–≥–∞–º–∏ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –µ—â–µ",
            {'warning': 'few_comparables'})

elif n_valid < 5:
    logger.info(f"‚ÑπÔ∏è {n_valid} –≤–∞–ª–∏–¥–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–æ–≤ - –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –±–∞–∑–æ–≤–æ–π –æ—Ü–µ–Ω–∫–∏")
```

```python
# –í –º–µ—Ç–æ–¥–µ _filter_outliers (–ø–æ—Å–ª–µ IQR —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)

def _filter_outliers(self, comparables: List[ComparableProperty]) -> List[ComparableProperty]:
    """
    –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–±—Ä–æ—Å–æ–≤ (–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è)

    –ù–û–í–û–ï: IQR –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ n >= 5
    """
    if not comparables:
        return []

    n = len(comparables)

    # –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø
    if n < 5:
        logger.info(f"‚ÑπÔ∏è –ü—Ä–æ–ø—É—Å–∫ IQR-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (n={n} < 5)")
        return comparables

    # –°—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ IQR
    prices_per_sqm = [c.price_per_sqm for c in comparables if c.price_per_sqm]

    if len(prices_per_sqm) < 5:
        logger.info("‚ÑπÔ∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è IQR —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        return comparables

    q1 = np.percentile(prices_per_sqm, 25)
    q3 = np.percentile(prices_per_sqm, 75)
    iqr = q3 - q1

    # –û–°–õ–ê–ë–õ–ï–ù–ù–´–ô –ö–û–≠–§–§–ò–¶–ò–ï–ù–¢: 1.5 ‚Üí 2.0
    k = 2.0  # –±—ã–ª–æ 1.5
    lower_bound = q1 - k * iqr
    upper_bound = q3 + k * iqr

    filtered = []
    outliers_count = 0

    for c in comparables:
        if c.price_per_sqm and (c.price_per_sqm < lower_bound or c.price_per_sqm > upper_bound):
            c.excluded = True
            c.exclusion_reason = f"IQR outlier (k={k})"
            outliers_count += 1
            logger.info(f"   –í—ã–±—Ä–æ—Å: {c.price_per_sqm:,.0f} ‚ÇΩ/–º¬≤ (–¥–∏–∞–ø–∞–∑–æ–Ω: {lower_bound:,.0f}-{upper_bound:,.0f})")
        else:
            filtered.append(c)

    if outliers_count > 0:
        logger.info(f"   –ò—Å–∫–ª—é—á–µ–Ω–æ {outliers_count} –≤—ã–±—Ä–æ—Å–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å {len(filtered)}")

    return filtered
```

```python
# –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –†–æ–±–∞—Å—Ç–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫

def calculate_robust_statistics(values: List[float], n_bootstraps: int = 1000) -> Dict[str, float]:
    """
    –†–æ–±–∞—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –¥–ª—è –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–æ–∫

    –î–ª—è n=1: –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ ¬± –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –∫–æ—Ä–∏–¥–æ—Ä
    –î–ª—è n=2: midpoint ¬± —Ä–∞–∑–±—Ä–æ—Å
    –î–ª—è n>=3: –º–µ–¥–∏–∞–Ω–∞ + MAD/IQR
    –î–ª—è n>=10: winsorized mean
    """
    n = len(values)

    if n == 0:
        return {'median': 0, 'mean': 0, 'std': 0, 'mad': 0}

    elif n == 1:
        # –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ —Ç–æ—á–µ—á–Ω—É—é –æ—Ü–µ–Ω–∫—É
        # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª ¬±15% (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞)
        val = values[0]
        return {
            'median': val,
            'mean': val,
            'std': val * 0.15,  # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π CV ~15%
            'mad': val * 0.10,
            'confidence_note': 'single_value_historical_ci'
        }

    elif n == 2:
        # –î–≤–µ —Ç–æ—á–∫–∏ - midpoint
        midpoint = (values[0] + values[1]) / 2
        spread = abs(values[1] - values[0]) / 2
        return {
            'median': midpoint,
            'mean': midpoint,
            'std': spread,
            'mad': spread * 0.67,  # MAD ‚âà 0.67 * SD –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ
            'confidence_note': 'two_values_midpoint'
        }

    elif n < 10:
        # –ú–∞–ª–∞—è –≤—ã–±–æ—Ä–∫–∞ - –º–µ–¥–∏–∞–Ω–∞ + MAD
        median_val = statistics.median(values)
        mad = median_abs_deviation(values)
        mean_val = statistics.mean(values)
        std_val = statistics.stdev(values) if n > 1 else 0

        return {
            'median': median_val,
            'mean': mean_val,
            'std': std_val,
            'mad': mad,
            'confidence_note': 'small_sample_median_mad'
        }

    else:
        # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ - winsorized mean
        sorted_vals = sorted(values)
        k = int(n * 0.1)  # 10% —Å –∫–∞–∂–¥–æ–π —Å—Ç–æ—Ä–æ–Ω—ã
        trimmed = sorted_vals[k:n-k]

        return {
            'median': statistics.median(values),
            'mean': statistics.mean(trimmed),
            'std': statistics.stdev(values),
            'mad': median_abs_deviation(values),
            'confidence_note': 'winsorized_robust'
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ calculate_market_statistics()
```

---

## üìù –ü–∞—Ç—á 4: Quality Metrics & User Warnings

**–§–∞–π–ª**: `app_new.py`

**–ü—Ä–æ–±–ª–µ–º–∞**:
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –≤–∏–¥–∏—Ç, —Å–∫–æ–ª—å–∫–æ –∞–Ω–∞–ª–æ–≥–æ–≤ –≤–∞–ª–∏–¥–Ω—ã
- Generic "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫" –≤–º–µ—Å—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
- –ù–µ—Ç —Ç—Ä–µ–∫–∏–Ω–≥–∞ quality metrics

**–†–µ—à–µ–Ω–∏–µ**:
- –î–æ–±–∞–≤–∏—Ç—å quality_metrics –≤ response
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö
- Warnings –≤–º–µ—Å—Ç–æ errors –ø—Ä–∏ –º–∞–ª–æ–º n

### Diff:

```python
# –ü–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω–∞–ª–æ–≥–æ–≤ (—Å—Ç—Ä–æ–∫–∞ ~1188)

# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –≤–∞–ª–∏–¥–Ω—ã–µ –∏ —á–∞—Å—Ç–∏—á–Ω—ã–µ
comparables_valid = []
comparables_partial = []
validation_errors = []

for i, raw_comparable in enumerate(session_data['comparables']):
    try:
        normalized = normalize_property_data(raw_comparable)
        comp = ComparableProperty(**normalized)

        if comp.is_usable_for_analysis():
            comparables_valid.append(comp)
        else:
            comparables_partial.append(comp)
            logger.warning(f"‚ö†Ô∏è –ê–Ω–∞–ª–æ–≥ {i+1} –Ω–µ –ø—Ä–∏–≥–æ–¥–µ–Ω –¥–ª—è —Ä–∞—Å—á–µ—Ç–æ–≤: {comp.quality_flags}")

    except ValidationError as e:
        validation_errors.append({
            'index': i,
            'url': raw_comparable.get('url', 'N/A')[:60],
            'error': str(e)
        })
        logger.error(f"‚ùå –ê–Ω–∞–ª–æ–≥ {i+1} –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é: {e}")

n_total = len(session_data['comparables'])
n_valid = len(comparables_valid)
n_partial = len(comparables_partial)
n_invalid = len(validation_errors)

logger.info(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {n_valid} –≤–∞–ª–∏–¥–Ω—ã—Ö, {n_partial} —á–∞—Å—Ç–∏—á–Ω—ã—Ö, {n_invalid} –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö")

# –§–æ—Ä–º–∏—Ä—É–µ–º quality metrics
quality_metrics = {
    'total_found': n_total,
    'valid_for_analysis': n_valid,
    'partial_data': n_partial,
    'validation_errors': n_invalid,
    'quality_score': 'high' if n_valid >= 10 else 'medium' if n_valid >= 5 else 'low',
    'confidence_level': 'high' if n_valid >= 10 else 'medium' if n_valid >= 3 else 'low'
}

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º—É–º
if n_valid == 0:
    return jsonify({
        'status': 'error',
        'error_type': 'no_valid_comparables',
        'message': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∞–Ω–∞–ª–æ–≥–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –î–æ–±–∞–≤—å—Ç–µ 1-2 –∞–Ω–∞–ª–æ–≥–∞ –≤—Ä—É—á–Ω—É—é.',
        'quality_metrics': quality_metrics,
        'suggestions': [
            '–î–æ–±–∞–≤—å—Ç–µ –∞–Ω–∞–ª–æ–≥–∏ –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫ –Ω–∞ –¶–∏–∞–Ω',
            '–†–∞—Å—à–∏—Ä—å—Ç–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ–∏—Å–∫–∞ (–±–æ–ª—å—à–∏–π —Ä–∞–¥–∏—É—Å, –¥–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω)',
            '–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π –æ–±—ä–µ–∫—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏'
        ]
    }), 422

# –°–æ–∑–¥–∞–µ–º request —Å –≤–∞–ª–∏–¥–Ω—ã–º–∏ –∞–Ω–∞–ª–æ–≥–∞–º–∏
request_model = AnalysisRequest(
    target_property=target_property,
    comparables=comparables_valid,  # –¢–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ!
    filter_outliers=filter_outliers,
    use_median=use_median
)
```

```python
# –ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ (—Å—Ç—Ä–æ–∫–∞ ~1298)

# –î–æ–±–∞–≤–ª—è–µ–º quality metrics –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
result_dict['quality_metrics'] = quality_metrics
result_dict['data_warnings'] = []

# –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
if n_valid == 1:
    result_dict['data_warnings'].append({
        'level': 'warning',
        'title': '–û—Ü–µ–Ω–∫–∞ –ø–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∞–Ω–∞–ª–æ–≥—É',
        'message': '–¢–æ—á–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞. –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å–µ–≥–º–µ–Ω—Ç—É. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å 2-3 –∞–Ω–∞–ª–æ–≥–∞ –≤—Ä—É—á–Ω—É—é.'
    })
elif n_valid == 2:
    result_dict['data_warnings'].append({
        'level': 'warning',
        'title': '–ú–∞–ª–∞—è –≤—ã–±–æ—Ä–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤',
        'message': '–û—Ü–µ–Ω–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –ø–æ 2 –∞–Ω–∞–ª–æ–≥–∞–º. –î–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –µ—â–µ 3-5 –∞–Ω–∞–ª–æ–≥–æ–≤.'
    })
elif n_valid < 5:
    result_dict['data_warnings'].append({
        'level': 'info',
        'title': '–ë–∞–∑–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞',
        'message': f'–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω –ø–æ {n_valid} –∞–Ω–∞–ª–æ–≥–∞–º. –î–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 10+ –∞–Ω–∞–ª–æ–≥–æ–≤.'
    })

if n_partial > 0:
    result_dict['data_warnings'].append({
        'level': 'info',
        'title': '–ù–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ',
        'message': f'{n_partial} –∞–Ω–∞–ª–æ–≥(–æ–≤) –∏–º–µ—é—Ç –Ω–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –≤ —Ä–∞—Å—á–µ—Ç–µ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–µ—Ç–∞–ª–∏ –Ω–∞ –¶–∏–∞–Ω.'
    })

return jsonify({
    'status': 'success',
    'analysis': result_dict,
    'quality_metrics': quality_metrics,
    'warnings': result_dict['data_warnings']
})
```

---

## ‚úÖ –ß–µ–∫-–ª–∏—Å—Ç —Ä–µ–≥—Ä–µ—Å—Å–∞

### Pre-deployment Testing:

- [ ] **Unit tests**: –í—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç
- [ ] **Regression**: –ö–µ–π—Å—ã —Å n‚â•10 –∞–Ω–∞–ª–æ–≥–æ–≤ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–∞–∫ —Ä–∞–Ω—å—à–µ
- [ ] **Edge case n=1**: –ê–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç–∞–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
- [ ] **Edge case n=2**: –ê–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç–∞–µ—Ç, midpoint –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
- [ ] **Edge case n=0**: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–Ω—è—Ç–Ω—É—é –æ—à–∏–±–∫—É —Å suggestions
- [ ] **Parallel parsing timeout**: –ù–µ –ø–∞–¥–∞–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç partial results
- [ ] **Parallel parsing captcha**: –ü–æ–º–µ—á–∞–µ—Ç URL, –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–∞–±–æ—Ç—É
- [ ] **Field recovery**: price –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –∏–∑ price_per_sqm * area
- [ ] **IQR skip**: –ü—Ä–∏ n<5 —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
- [ ] **Quality metrics**: –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ

### Production Testing Scenarios:

```python
# –¢–µ—Å—Ç-–∫–µ–π—Å 1: –≠–ª–∏—Ç–Ω–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å (—Ä–µ–∞–ª—å–Ω—ã–π –∫–µ–π—Å –∏–∑ –±–∞–≥–∞)
TARGET = {
    'url': 'https://www.cian.ru/sale/flat/305062289/',
    'rooms': 5,
    'total_area': 213.4,
    'price': 520_000_000
}
EXPECTED:
  - –ù–∞–π–¥–µ–Ω–æ: 8 –∞–Ω–∞–ª–æ–≥–æ–≤
  - –í–∞–ª–∏–¥–Ω—ã—Ö: 1-2
  - –†–µ–∑—É–ª—å—Ç–∞—Ç: –ê–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å warning
  - Frontend: –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É + –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ç–æ—á–Ω–æ—Å—Ç–∏
```

```python
# –¢–µ—Å—Ç-–∫–µ–π—Å 2: –û–±—ã—á–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞
TARGET = {
    'url': 'https://www.cian.ru/sale/flat/123/',
    'rooms': 2,
    'total_area': 60,
    'price': 15_000_000
}
EXPECTED:
  - –ù–∞–π–¥–µ–Ω–æ: 15+ –∞–Ω–∞–ª–æ–≥–æ–≤
  - –í–∞–ª–∏–¥–Ω—ã—Ö: 10+
  - –†–µ–∑—É–ª—å—Ç–∞—Ç: –ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
  - IQR: –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
```

```python
# –¢–µ—Å—Ç-–∫–µ–π—Å 3: Rate limit simulation
MOCK: parse_detail_page –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç RateLimitError –¥–ª—è 50% URLs
EXPECTED:
  - Retry —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç (2-3 –ø–æ–ø—ã—Ç–∫–∏ —Å backoff)
  - Partial results –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è
  - –ê–Ω–∞–ª–∏–∑ –Ω–µ –ø–∞–¥–∞–µ—Ç
```

```python
# –¢–µ—Å—Ç-–∫–µ–π—Å 4: –ê–Ω–∞–ª–æ–≥–∏ –±–µ–∑ —Ü–µ–Ω—ã –Ω–∞ –ª–∏—Å—Ç–∏–Ω–≥–µ
COMPARABLES: 8 cards, —É –≤—Å–µ—Ö price=None –Ω–æ –µ—Å—Ç—å URL
EXPECTED:
  - –î–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑–≤–ª–µ–∫–∞–µ—Ç price
  - Field recovery –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ
  - ‚â•50% —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –≤–∞–ª–∏–¥–Ω—ã–º–∏
```

---

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

### Grafana Dashboard "Analysis Quality"

```yaml
–ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ Prometheus:

# –°—á–µ—Ç—á–∏–∫–∏
- analysis_requests_total{status="success|error|warning"}
- analysis_comparables_found_total
- analysis_comparables_valid_total
- analysis_comparables_partial_total

# –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã
- analysis_duration_seconds
- parse_retry_count

# –ì–∞—É–¥–∂–∏
- analysis_quality_score{level="high|medium|low"}
- parse_success_rate

# –õ–æ–≥–∏ –≤ ELK
- –ö–∞–∂–¥–∞—è —Å–µ—Å—Å–∏—è: session_id, found, valid, partial, invalid, warnings
- –ö–∞–∂–¥—ã–π parse failure: url, error_type, retries_used
```

### –ê–ª–µ—Ä—Ç—ã:

```yaml
# Alert 1: –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç parse failures
WHEN: parse_success_rate < 50% for 5m
ACTION: Notify team (–≤–æ–∑–º–æ–∂–Ω–æ, –¶–∏–∞–Ω –±–ª–æ–∫–∏—Ä—É–µ—Ç)

# Alert 2: –ú–Ω–æ–≥–æ —Å–µ—Å—Å–∏–π —Å n_valid < 3
WHEN: rate(analysis_comparables_valid_total{n<3}) > 30% for 10m
ACTION: Investigate data quality

# Alert 3: –£–≤–µ–ª–∏—á–µ–Ω–∏–µ analysis_duration
WHEN: p95(analysis_duration_seconds) > 60s
ACTION: Check parsing performance
```

---

## üöÄ Deployment Plan

### Phase 1: Development (2-3 —á–∞—Å–∞)
- [ ] –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–∞—Ç—á–∏ 1-4
- [ ] –î–æ–±–∞–≤–∏—Ç—å unit tests
- [ ] –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### Phase 2: Staging (1-2 —á–∞—Å–∞)
- [ ] Deploy –Ω–∞ staging
- [ ] –ü—Ä–æ–≥–Ω–∞—Ç—å —Ç–µ—Å—Ç-–∫–µ–π—Å—ã 1-4
- [ ] Smoke testing –≤—Å–µ—Ö —à–∞–≥–æ–≤

### Phase 3: Production (30 –º–∏–Ω—É—Ç)
- [ ] Deploy –Ω–∞ production
- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ (–ø–µ—Ä–≤—ã–µ 1-2 —á–∞—Å–∞)
- [ ] –ë—ã—Å—Ç—Ä—ã–π rollback plan (–µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫)

### Rollback Strategy:

```bash
# –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ —Å–ª–æ–º–∞–ª–æ—Å—å:
git revert <commit-hash>
git push origin main
bash scripts/deploy.sh

# –í—Ä–µ–º—è rollback: ~2-3 –º–∏–Ω—É—Ç—ã
```

---

## üìà Expected Impact

**–î–æ –ø–∞—Ç—á–∞:**
- 8 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö ‚Üí 0 –≤–∞–ª–∏–¥–Ω—ã—Ö ‚Üí ‚ùå –û—à–∏–±–∫–∞
- Success rate: ~40% –¥–ª—è —ç–ª–∏—Ç–Ω–æ–π –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏

**–ü–æ—Å–ª–µ –ø–∞—Ç—á–∞:**
- 8 –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö ‚Üí 1-2 –≤–∞–ª–∏–¥–Ω—ã—Ö ‚Üí ‚úÖ –ê–Ω–∞–ª–∏–∑ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏
- Success rate: ~95% –¥–ª—è –≤—Å–µ—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
- –£–ª—É—á—à–µ–Ω–∏–µ UX: –ø–æ–Ω—è—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ generic errors

---

## üîó Related Issues

- #123: "–®–∞–≥ 3 –ø–∞–¥–∞–µ—Ç —Å ValidationError"
- #124: "Rate limiting –æ—Ç –¶–∏–∞–Ω"
- #125: "–≠–ª–∏—Ç–Ω–∞—è –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –Ω–µ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è"

---

## üë• Reviewers

- @backend-lead - Code review
- @qa-engineer - Testing checklist
- @product-owner - UX messaging approval
