"""
Log Analyzer
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã
"""

import re
import logging
from collections import defaultdict
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)


class LogAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏ –∏ –æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∞–Ω–æ–º–∞–ª–∏–∏"""

    def __init__(self, log_file: str = "/home/user/cian-analyzer/app.log"):
        self.log_file = log_file
        self.error_patterns = [
            r'ERROR',
            r'CRITICAL',
            r'Exception',
            r'Traceback',
            r'failed',
            r'timeout',
            r'connection.*error',
            r'–Ω–µ —É–¥–∞–ª–æ—Å—å',
            r'–æ—à–∏–±–∫–∞',
        ]

        self.warning_patterns = [
            r'WARNING',
            r'WARN',
            r'deprecated',
            r'–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ',
        ]

    def analyze_recent_logs(self, hours: int = 1) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–≥–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤"""
        logger.info(f"üìä Analyzing logs for the last {hours} hour(s)...")

        cutoff_time = datetime.now() - timedelta(hours=hours)

        results = {
            'period_hours': hours,
            'analysis_time': datetime.now().isoformat(),
            'errors': [],
            'warnings': [],
            'error_count': 0,
            'warning_count': 0,
            'error_types': defaultdict(int),
            'error_endpoints': defaultdict(int),
            'critical_issues': [],
        }

        try:
            if not Path(self.log_file).exists():
                results['message'] = f'Log file not found: {self.log_file}'
                return results

            with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    # –ü–∞—Ä—Å–∏–º timestamp –∏–∑ –ª–æ–≥–∞
                    log_time = self._extract_timestamp(line)
                    if log_time and log_time < cutoff_time:
                        continue

                    # –ò—â–µ–º –æ—à–∏–±–∫–∏
                    if self._is_error_line(line):
                        results['errors'].append({
                            'time': log_time.isoformat() if log_time else 'unknown',
                            'message': line.strip()
                        })
                        results['error_count'] += 1

                        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
                        error_type = self._classify_error(line)
                        results['error_types'][error_type] += 1

                        # –ò–∑–≤–ª–µ–∫–∞–µ–º endpoint –µ—Å–ª–∏ –µ—Å—Ç—å
                        endpoint = self._extract_endpoint(line)
                        if endpoint:
                            results['error_endpoints'][endpoint] += 1

                    # –ò—â–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
                    elif self._is_warning_line(line):
                        results['warnings'].append({
                            'time': log_time.isoformat() if log_time else 'unknown',
                            'message': line.strip()
                        })
                        results['warning_count'] += 1

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            results['critical_issues'] = self._identify_critical_issues(results)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º defaultdict –≤ –æ–±—ã—á–Ω—ã–π dict –¥–ª—è JSON
            results['error_types'] = dict(results['error_types'])
            results['error_endpoints'] = dict(results['error_endpoints'])

        except Exception as e:
            logger.error(f"Failed to analyze logs: {e}", exc_info=True)
            results['message'] = f'Analysis failed: {str(e)}'

        return results

    def _extract_timestamp(self, line: str) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç timestamp –∏–∑ —Å—Ç—Ä–æ–∫–∏ –ª–æ–≥–∞"""
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ timestamp –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        patterns = [
            r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})',  # 2025-11-10 15:30:45
            r'(\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})',  # 10/11/2025 15:30:45
        ]

        for pattern in patterns:
            match = re.search(pattern, line)
            if match:
                try:
                    timestamp_str = match.group(1)
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞
                    for fmt in ['%Y-%m-%d %H:%M:%S', '%d/%m/%Y %H:%M:%S']:
                        try:
                            return datetime.strptime(timestamp_str, fmt)
                        except ValueError:
                            continue
                except Exception:
                    pass

        return None

    def _is_error_line(self, line: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –æ—à–∏–±–∫–æ–π"""
        line_lower = line.lower()
        return any(re.search(pattern, line_lower) for pattern in self.error_patterns)

    def _is_warning_line(self, line: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º"""
        line_lower = line.lower()
        return any(re.search(pattern, line_lower) for pattern in self.warning_patterns)

    def _classify_error(self, line: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –æ—à–∏–±–∫—É –ø–æ —Ç–∏–ø—É"""
        line_lower = line.lower()

        if 'connection' in line_lower or 'timeout' in line_lower:
            return 'connection_error'
        elif 'parse' in line_lower or 'parser' in line_lower:
            return 'parsing_error'
        elif 'validation' in line_lower or '–≤–∞–ª–∏–¥–∞—Ü' in line_lower:
            return 'validation_error'
        elif 'session' in line_lower or '—Å–µ—Å—Å–∏—è' in line_lower:
            return 'session_error'
        elif 'redis' in line_lower or 'cache' in line_lower:
            return 'cache_error'
        elif 'analyz' in line_lower or '–∞–Ω–∞–ª–∏–∑' in line_lower:
            return 'analysis_error'
        elif 'rate limit' in line_lower:
            return 'rate_limit_error'
        else:
            return 'other_error'

    def _extract_endpoint(self, line: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç endpoint –∏–∑ —Å—Ç—Ä–æ–∫–∏ –ª–æ–≥–∞"""
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã API endpoints
        match = re.search(r'/api/[\w-]+', line)
        if match:
            return match.group(0)
        return None

    def _identify_critical_issues(self, results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        issues = []

        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        if results['error_types'].get('connection_error', 0) > 5:
            issues.append({
                'severity': 'high',
                'type': 'connection_errors',
                'count': results['error_types']['connection_error'],
                'message': 'High number of connection errors detected',
                'recommendation': 'Check network connectivity and CIAN availability'
            })

        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
        if results['error_types'].get('parsing_error', 0) > 5:
            issues.append({
                'severity': 'high',
                'type': 'parsing_errors',
                'count': results['error_types']['parsing_error'],
                'message': 'High number of parsing errors detected',
                'recommendation': 'CIAN HTML structure may have changed, update parser'
            })

        # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if results['error_types'].get('validation_error', 0) > 3:
            issues.append({
                'severity': 'medium',
                'type': 'validation_errors',
                'count': results['error_types']['validation_error'],
                'message': 'Multiple validation errors detected',
                'recommendation': 'Check data quality and validation rules'
            })

        # –ï—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–π endpoint —á–∞—Å—Ç–æ –ø–∞–¥–∞–µ—Ç
        for endpoint, count in results['error_endpoints'].items():
            if count > 5:
                issues.append({
                    'severity': 'high',
                    'type': 'endpoint_failures',
                    'endpoint': endpoint,
                    'count': count,
                    'message': f'Endpoint {endpoint} failing frequently',
                    'recommendation': f'Investigate {endpoint} handler and dependencies'
                })

        # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫ –≤ —Ü–µ–ª–æ–º
        if results['error_count'] > 20:
            issues.append({
                'severity': 'critical',
                'type': 'high_error_rate',
                'count': results['error_count'],
                'message': 'Critically high error rate',
                'recommendation': 'Immediate investigation required - system may be unstable'
            })

        return issues

    def get_error_trends(self, days: int = 7) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç—Ä–µ–Ω–¥—ã –æ—à–∏–±–æ–∫ –∑–∞ N –¥–Ω–µ–π"""
        # TODO: –ú–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        return {
            'days': days,
            'message': 'Trend analysis not yet implemented',
            'recent_analysis': self.analyze_recent_logs(hours=24)
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
log_analyzer = LogAnalyzer()
