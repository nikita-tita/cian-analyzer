"""
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –æ–±—ä–µ–∫—Ç–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio –∏ Playwright async API –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
"""

import asyncio
import logging
from typing import List, Dict, Optional, Callable
from datetime import datetime
import time

try:
    from playwright.async_api import async_playwright, Browser, Page, Playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

from src.parsers.base_parser import BaseCianParser

logger = logging.getLogger(__name__)


class AsyncCianParser(BaseCianParser):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Cian.ru

    Features:
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ URL
    - Connection pooling (–Ω–µ—Å–∫–æ–ª—å–∫–æ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤)
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞–º–∏
    - Progress callbacks
    - Error handling —Å retry –ª–æ–≥–∏–∫–æ–π
    """

    def __init__(
        self,
        headless: bool = True,
        delay: float = 1.0,
        max_concurrent: int = 5,
        timeout: int = 30000,
        retry_attempts: int = 3,
        user_agent: str = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞

        Args:
            headless: –ó–∞–ø—É—Å–∫–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –≤ headless —Ä–µ–∂–∏–º–µ
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
            max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            timeout: –¢–∞–π–º–∞—É—Ç –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü (–º—Å)
            retry_attempts: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            user_agent: Custom User-Agent
        """
        if not PLAYWRIGHT_AVAILABLE:
            raise ImportError(
                "Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install playwright && playwright install"
            )

        super().__init__()

        self.headless = headless
        self.delay = delay
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.retry_attempts = retry_attempts
        self.user_agent = user_agent or self._get_random_user_agent()

        self.playwright: Optional[Playwright] = None
        self.browser: Optional[Browser] = None

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_parsed': 0,
            'successful': 0,
            'failed': 0,
            'total_time': 0,
            'avg_time_per_page': 0
        }

    async def __aenter__(self):
        """Async context manager entry"""
        await self._initialize_browser()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self._close_browser()

    async def _initialize_browser(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Playwright –∏ –±—Ä–∞—É–∑–µ—Ä–∞"""
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox'
                ]
            )
            logger.info(f"‚úÖ Async browser launched (max_concurrent={self.max_concurrent})")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            raise

    async def _close_browser(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –±—Ä–∞—É–∑–µ—Ä–∞"""
        try:
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            logger.info("üîå Async browser closed")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –±—Ä–∞—É–∑–µ—Ä–∞: {e}")

    async def _create_page(self) -> Page:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

        Returns:
            –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è Page
        """
        context = await self.browser.new_context(
            user_agent=self.user_agent,
            viewport={'width': 1920, 'height': 1080},
            locale='ru-RU',
            timezone_id='Europe/Moscow'
        )

        page = await context.new_page()
        page.set_default_timeout(self.timeout)

        return page

    async def _parse_single_page(
        self,
        url: str,
        retry_count: int = 0
    ) -> Optional[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            retry_count: –¢–µ–∫—É—â–∞—è –ø–æ–ø—ã—Ç–∫–∞ (–¥–ª—è retry)

        Returns:
            –†–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        page = None
        start_time = time.time()

        try:
            page = await self._create_page()

            logger.debug(f"üï∑Ô∏è Parsing: {url}")

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = await page.goto(url, wait_until='networkidle')

            if response.status != 200:
                logger.warning(f"‚ö†Ô∏è HTTP {response.status}: {url}")
                if retry_count < self.retry_attempts:
                    await asyncio.sleep(self.delay * 2)
                    return await self._parse_single_page(url, retry_count + 1)
                return None

            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            await page.wait_for_load_state('domcontentloaded')
            await asyncio.sleep(0.5)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è JS

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON-LD
            json_ld_data = await self._extract_json_ld_async(page)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º HTML –¥–∞–Ω–Ω—ã–µ
            html = await page.content()
            html_data = self._parse_html_content(html)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            parsed_data = self._merge_parsed_data(json_ld_data, html_data)
            parsed_data['url'] = url

            duration = time.time() - start_time
            logger.debug(f"‚úÖ Parsed in {duration:.2f}s: {url}")

            self.stats['successful'] += 1
            self.stats['total_time'] += duration

            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(self.delay)

            return parsed_data

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {e}")

            # Retry –ª–æ–≥–∏–∫–∞
            if retry_count < self.retry_attempts:
                logger.info(f"üîÑ Retry {retry_count + 1}/{self.retry_attempts}: {url}")
                await asyncio.sleep(self.delay * 2)
                return await self._parse_single_page(url, retry_count + 1)

            self.stats['failed'] += 1
            return None

        finally:
            if page:
                await page.close()
                await page.context.close()

    async def _extract_json_ld_async(self, page: Page) -> Dict:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON-LD –¥–∞–Ω–Ω—ã—Ö

        Args:
            page: Playwright Page

        Returns:
            –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        try:
            json_ld_content = await page.locator('script[type="application/ld+json"]').first.inner_text()
            if json_ld_content:
                import json
                data = json.loads(json_ld_content)
                return self._extract_from_json_ld(data)

        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON-LD: {e}")

        return {}

    async def parse_urls(
        self,
        urls: List[str],
        progress_callback: Optional[Callable] = None
    ) -> List[Dict]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ URL

        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            progress_callback: Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (url, index, total, data)

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        if not self.browser:
            await self._initialize_browser()

        self.stats['total_parsed'] = len(urls)
        self.stats['successful'] = 0
        self.stats['failed'] = 0
        self.stats['total_time'] = 0

        logger.info(f"üöÄ Starting async parsing of {len(urls)} URLs (max_concurrent={self.max_concurrent})")

        # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def parse_with_semaphore(url: str, index: int):
            async with semaphore:
                result = await self._parse_single_page(url)

                if progress_callback:
                    progress_callback(url, index, len(urls), result)

                return result

        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        start_time = time.time()
        tasks = [parse_with_semaphore(url, i) for i, url in enumerate(urls)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        total_duration = time.time() - start_time
        self.stats['total_time'] = total_duration

        # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        valid_results = [
            r for r in results
            if r is not None and not isinstance(r, Exception)
        ]

        self.stats['avg_time_per_page'] = (
            total_duration / len(urls) if urls else 0
        )

        logger.info(
            f"‚úÖ Async parsing completed: {len(valid_results)}/{len(urls)} successful "
            f"in {total_duration:.2f}s (avg: {self.stats['avg_time_per_page']:.2f}s/page)"
        )

        return valid_results

    async def search_similar_async(
        self,
        target_property: Dict,
        limit: int = 20
    ) -> List[Dict]:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤

        Args:
            target_property: –¶–µ–ª–µ–≤–æ–π –æ–±—ä–µ–∫—Ç
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–∞–ª–æ–≥–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –∞–Ω–∞–ª–æ–≥–æ–≤
        """
        if not self.browser:
            await self._initialize_browser()

        # –°—Ç—Ä–æ–∏–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        search_url = self._build_search_url(target_property)

        logger.info(f"üîç Async search: {search_url}")

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –∞–Ω–∞–ª–æ–≥–æ–≤
        page = None
        try:
            page = await self._create_page()
            await page.goto(search_url, wait_until='networkidle')
            await asyncio.sleep(1)

            html = await page.content()
            comparable_urls = self._extract_listing_urls(html, limit=limit)

            logger.info(f"üìã Found {len(comparable_urls)} comparable URLs")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []

        finally:
            if page:
                await page.close()
                await page.context.close()

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–∞—Ä—Å–∏–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ URL
        comparables = await self.parse_urls(comparable_urls)

        return comparables

    def get_stats(self) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        return self.stats.copy()


# Convenience functions –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

def parse_urls_sync(
    urls: List[str],
    headless: bool = True,
    delay: float = 1.0,
    max_concurrent: int = 5,
    progress_callback: Optional[Callable] = None
) -> List[Dict]:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞

    Args:
        urls: –°–ø–∏—Å–æ–∫ URL
        headless: Headless —Ä–µ–∂–∏–º
        delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        progress_callback: Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞

    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    async def _async_parse():
        async with AsyncCianParser(
            headless=headless,
            delay=delay,
            max_concurrent=max_concurrent
        ) as parser:
            return await parser.parse_urls(urls, progress_callback)

    return asyncio.run(_async_parse())


def search_similar_async_sync(
    target_property: Dict,
    limit: int = 20,
    headless: bool = True
) -> List[Dict]:
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤

    Args:
        target_property: –¶–µ–ª–µ–≤–æ–π –æ–±—ä–µ–∫—Ç
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–∞–ª–æ–≥–æ–≤
        headless: Headless —Ä–µ–∂–∏–º

    Returns:
        –°–ø–∏—Å–æ–∫ –∞–Ω–∞–ª–æ–≥–æ–≤
    """
    async def _async_search():
        async with AsyncCianParser(headless=headless) as parser:
            return await parser.search_similar_async(target_property, limit)

    return asyncio.run(_async_search())
