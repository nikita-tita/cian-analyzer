"""
–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –æ–±—ä–µ–∫—Ç–æ–≤

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç asyncio + playwright –¥–ª—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ 5-10 –æ–±—ä–µ–∫—Ç–æ–≤
–í—Ä–µ–º—è –ø–∞—Ä—Å–∏–Ω–≥–∞ 10 –∞–Ω–∞–ª–æ–≥–æ–≤: ~50s ‚Üí ~8s (6x —É—Å–∫–æ—Ä–µ–Ω–∏–µ)
"""

import asyncio
import logging
import random
from typing import List, Dict, Optional
from dataclasses import dataclass, field
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
import time

from .base_parser import BaseCianParser, ParsingError

logger = logging.getLogger(__name__)


@dataclass
class ParseResult:
    """
    –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ URL

    –ü–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —É—Å–ø–µ—Ö/–Ω–µ—É–¥–∞—á—É –∏ –ø—Ä–∏—á–∏–Ω—É –æ—à–∏–±–∫–∏
    """
    url: str
    ok: bool
    data: dict
    error_type: Optional[str] = None  # "rate_limited" | "timeout" | "captcha" | "network" | "parse_error"
    error_message: Optional[str] = None
    retries_used: int = 0


class AsyncPlaywrightParser(BaseCianParser):
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π Playwright –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
    - –û–¥–∏–Ω –±—Ä–∞—É–∑–µ—Ä, –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (–∏–∑–æ–ª—è—Ü–∏—è)
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ 10 URL –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
    - Shared cache –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
    - Graceful degradation –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    """

    def __init__(
        self,
        headless: bool = True,
        delay: float = 1.0,
        block_resources: bool = True,
        cache=None,
        region: str = 'spb',
        max_concurrent: int = 5
    ):
        """
        Args:
            headless: –ó–∞–ø—É—Å–∫–∞—Ç—å –±—Ä–∞—É–∑–µ—Ä –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
            delay: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫)
            block_resources: –ë–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫–∏/—à—Ä–∏—Ñ—Ç—ã
            cache: PropertyCache instance
            region: –†–µ–≥–∏–æ–Ω ('spb' –∏–ª–∏ 'msk')
            max_concurrent: –ú–∞–∫—Å–∏–º—É–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        super().__init__(delay, cache=cache)
        self.headless = headless
        self.block_resources = block_resources
        self.max_concurrent = max_concurrent

        # –ú–∞–ø–ø–∏–Ω–≥ —Ä–µ–≥–∏–æ–Ω–æ–≤
        self.region_codes = {'spb': '2', 'msk': '1'}
        self.region = region
        self.region_code = self.region_codes.get(region, '2')

        # Async —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.playwright = None
        self.browser: Optional[Browser] = None
        self._contexts: List[BrowserContext] = []
        self._semaphore = None

        logger.info(f"AsyncParser initialized: region={region}, max_concurrent={max_concurrent}")

    async def __aenter__(self):
        """Async context manager –≤—Ö–æ–¥"""
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager –≤—ã—Ö–æ–¥"""
        await self.close()

    async def start(self):
        """–ó–∞–ø—É—Å–∫ –±—Ä–∞—É–∑–µ—Ä–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ semaphore –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞"""
        if self.browser:
            logger.warning("Browser already started")
            return

        try:
            logger.info("üöÄ Starting async Playwright browser...")
            self.playwright = await async_playwright().start()

            self.browser = await self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-gpu',
                ]
            )

            # Semaphore –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
            self._semaphore = asyncio.Semaphore(self.max_concurrent)

            logger.info(f"‚úì Browser started (max concurrent: {self.max_concurrent})")

        except Exception as e:
            logger.error(f"Failed to start browser: {e}")
            await self.close()
            raise

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏ –±—Ä–∞—É–∑–µ—Ä–∞"""
        errors = []

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –≤—Å–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        for context in self._contexts:
            try:
                await context.close()
            except Exception as e:
                errors.append(f"Context: {e}")

        self._contexts.clear()

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä
        if self.browser:
            try:
                await self.browser.close()
            except Exception as e:
                errors.append(f"Browser: {e}")
            finally:
                self.browser = None

        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º playwright
        if self.playwright:
            try:
                await self.playwright.stop()
            except Exception as e:
                errors.append(f"Playwright: {e}")
            finally:
                self.playwright = None

        if errors:
            logger.warning(f"Errors closing browser: {', '.join(errors)}")
        else:
            logger.info("Browser closed")

    async def _create_context(self) -> BrowserContext:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            locale='ru-RU',
            timezone_id='Europe/Moscow',
        )

        # –ê–Ω—Ç–∏–¥–µ—Ç–µ–∫—Ç
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            window.chrome = { runtime: {} };
        """)

        # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
        if self.block_resources:
            await context.route(
                "**/*.{png,jpg,jpeg,gif,svg,webp,woff,woff2,ttf,mp4,mp3,pdf}",
                lambda route: route.abort()
            )

        self._contexts.append(context)
        return context

    async def _fetch_page_content(self, url: str, context: BrowserContext) -> Optional[str]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ HTML —á–µ—Ä–µ–∑ Playwright

        Args:
            url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            context: Browser context

        Returns:
            HTML content –∏–ª–∏ None
        """
        page: Page = await context.new_page()

        try:
            logger.debug(f"Fetching: {url[:60]}...")

            await page.goto(url, wait_until='domcontentloaded', timeout=60000)

            # –ñ–¥–µ–º –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            try:
                await page.wait_for_selector(
                    'h1, [data-mark="OfferTitle"], script[type="application/ld+json"]',
                    timeout=10000
                )
            except Exception as e:
                logger.debug(f"Selectors not found, continuing anyway: {e}")

            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
            await asyncio.sleep(0.5)

            html = await page.content()

            if not html or len(html) < 1000:
                raise ValueError(f"Empty or too short HTML ({len(html) if html else 0} chars)")

            logger.debug(f"‚úì Page loaded: {len(html)} chars")
            return html

        except Exception as e:
            logger.warning(f"Error fetching {url[:60]}: {e}")
            raise

        finally:
            await page.close()
            await asyncio.sleep(self.delay)

    def _classify_error(self, error_msg: str) -> str:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –æ—à–∏–±–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ retry

        Args:
            error_msg: –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ

        Returns:
            –¢–∏–ø –æ—à–∏–±–∫–∏: "rate_limited", "timeout", "captcha", "network", "parse_error"
        """
        error_lower = error_msg.lower()

        if any(x in error_lower for x in ['429', 'rate limit', 'too many requests', '—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ']):
            return 'rate_limited'
        elif any(x in error_lower for x in ['timeout', 'timed out', '–ø—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è']):
            return 'timeout'
        elif any(x in error_lower for x in ['captcha', '–∫–∞–ø—á–∞', 'robot', 'verification']):
            return 'captcha'
        elif any(x in error_lower for x in ['network', 'connection', 'dns', 'socket']):
            return 'network'
        else:
            return 'parse_error'

    async def _parse_with_retry(
        self,
        url: str,
        max_retries: int = 3,
        base_delay: float = 2.0
    ) -> ParseResult:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Å retry –ª–æ–≥–∏–∫–æ–π –∏ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–º backoff

        Args:
            url: URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            max_retries: –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ (0 = —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –ø–æ–ø—ã—Ç–∫–∞)
            base_delay: –ë–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è backoff (—Å–µ–∫—É–Ω–¥—ã)

        Returns:
            ParseResult —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± —É—Å–ø–µ—Ö–µ/–Ω–µ—É–¥–∞—á–µ
        """
        last_error = None
        last_error_type = None

        for attempt in range(max_retries + 1):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–µ—Ä–≤–æ–π –ø–æ–ø—ã—Ç–∫–µ
                if attempt == 0 and self.cache:
                    cached_data = self.cache.get_property(url)
                    if cached_data:
                        self.stats['cache_hits'] += 1
                        logger.debug(f"‚úÖ Cache HIT: {url[:60]}")

                        # –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö: –∑–∞–ø–æ–ª–Ω—è–µ–º total_area –∏–∑ characteristics
                        if not cached_data.get('total_area') and cached_data.get('characteristics'):
                            self._promote_key_fields(cached_data)
                            if cached_data.get('total_area'):
                                self.cache.set_property(url, cached_data, ttl_hours=24)
                                logger.debug(f"Cache migrated: total_area={cached_data.get('total_area')}")

                        return ParseResult(
                            url=url,
                            ok=True,
                            data=cached_data,
                            retries_used=0
                        )
                    else:
                        self.stats['cache_misses'] += 1

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º semaphore –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
                async with self._semaphore:
                    # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏
                    context = await self._create_context()

                    # –ó–∞–≥—Ä—É–∂–∞–µ–º HTML
                    html = await self._fetch_page_content(url, context)
                    if not html:
                        raise ParsingError(f"Failed to fetch content: {url}")

                    # –ü–∞—Ä—Å–∏–Ω–≥ HTML
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'lxml')

                    data = {
                        'url': url,
                        'title': None,
                        'price': None,
                        'price_raw': None,
                    }

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON-LD (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
                    json_ld = self._extract_json_ld(soup)
                    if json_ld:
                        data['title'] = json_ld.get('name')
                        offers = json_ld.get('offers', {})
                        if offers:
                            data['price_raw'] = offers.get('price')
                            data['currency'] = offers.get('priceCurrency')
                            if data['price_raw']:
                                data['price'] = data['price_raw']

                    # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ HTML
                    self._extract_basic_info(soup, data)
                    data['characteristics'] = self._extract_characteristics(soup)
                    data['images'] = self._extract_images(soup)

                    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è –∏–∑ characteristics –≤ –∫–æ—Ä–µ–Ω—å
                    self._promote_key_fields(data)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                    if self.cache:
                        self.cache.set_property(url, data, ttl_hours=24)

                    logger.debug(f"‚úì Parsed: {data.get('title', 'No title')[:50]}")
                    return ParseResult(
                        url=url,
                        ok=True,
                        data=data,
                        retries_used=attempt
                    )

            except Exception as e:
                self.stats['errors'] += 1
                last_error = str(e)
                last_error_type = self._classify_error(last_error)

                logger.warning(
                    f"‚ùå Attempt {attempt + 1}/{max_retries + 1} failed for {url[:60]}: "
                    f"{last_error_type} - {last_error}"
                )

                # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É
                if attempt >= max_retries:
                    break

                # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π backoff —Å jitter
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)

                # –î–ª—è rate limiting - —É–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                if last_error_type == 'rate_limited':
                    delay *= 2

                logger.debug(f"‚è≥ Waiting {delay:.2f}s before retry...")
                await asyncio.sleep(delay)

        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        return ParseResult(
            url=url,
            ok=False,
            data={'url': url, 'title': '–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞'},
            error_type=last_error_type,
            error_message=last_error,
            retries_used=max_retries
        )

    async def parse_detail_page_async(self, url: str) -> Dict:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Args:
            url: URL –æ–±—ä—è–≤–ª–µ–Ω–∏—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if self.cache:
            cached_data = self.cache.get_property(url)
            if cached_data:
                self.stats['cache_hits'] += 1
                logger.debug(f"‚úÖ Cache HIT: {url[:60]}")

                # –ú–∏–≥—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö: –∑–∞–ø–æ–ª–Ω—è–µ–º total_area –∏–∑ characteristics
                if not cached_data.get('total_area') and cached_data.get('characteristics'):
                    self._promote_key_fields(cached_data)
                    if cached_data.get('total_area'):
                        self.cache.set_property(url, cached_data, ttl_hours=24)
                        logger.debug(f"Cache migrated: total_area={cached_data.get('total_area')}")

                return cached_data
            else:
                self.stats['cache_misses'] += 1

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º semaphore –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞
        async with self._semaphore:
            try:
                # –°–æ–∑–¥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏
                context = await self._create_context()

                # –ó–∞–≥—Ä—É–∂–∞–µ–º HTML
                html = await self._fetch_page_content(url, context)
                if not html:
                    raise ParsingError(f"Failed to fetch content: {url}")

                # –ü–∞—Ä—Å–∏–Ω–≥ HTML (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞)
                # TODO: –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–æ–ª–Ω–æ—Å—Ç—å—é async, –Ω–æ –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º sync BeautifulSoup
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html, 'lxml')

                data = {
                    'url': url,
                    'title': None,
                    'price': None,
                    'price_raw': None,
                }

                # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON-LD (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
                json_ld = self._extract_json_ld(soup)
                if json_ld:
                    data['title'] = json_ld.get('name')
                    offers = json_ld.get('offers', {})
                    if offers:
                        data['price_raw'] = offers.get('price')
                        data['currency'] = offers.get('priceCurrency')
                        if data['price_raw']:
                            data['price'] = data['price_raw']

                # –î–æ–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ HTML
                self._extract_basic_info(soup, data)
                data['characteristics'] = self._extract_characteristics(soup)
                data['images'] = self._extract_images(soup)

                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è –∏–∑ characteristics –≤ –∫–æ—Ä–µ–Ω—å
                self._promote_key_fields(data)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                if self.cache:
                    self.cache.set_property(url, data, ttl_hours=24)

                logger.debug(f"‚úì Parsed: {data.get('title', 'No title')[:50]}")
                return data

            except Exception as e:
                self.stats['errors'] += 1
                logger.error(f"Error parsing {url}: {e}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–º–µ—Å—Ç–æ –ø—Ä–æ–≤–∞–ª–∞
                return {
                    'url': url,
                    'title': '–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞',
                    'price': None,
                    'error': str(e)
                }

    async def parse_multiple_async(
        self,
        urls: List[str],
        timeout_per_url: int = 45,
        max_retries: int = 2
    ) -> List[ParseResult]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ URL —Å retry –ª–æ–≥–∏–∫–æ–π

        Args:
            urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
            timeout_per_url: Timeout –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL (—Å–µ–∫—É–Ω–¥—ã)
            max_retries: –ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL

        Returns:
            –°–ø–∏—Å–æ–∫ ParseResult –æ–±—ä–µ–∫—Ç–æ–≤ (–≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL)
        """
        if not urls:
            return []

        logger.info(
            f"üöÄ Starting parallel parsing of {len(urls)} URLs "
            f"(timeout: {timeout_per_url}s, max_retries: {max_retries})..."
        )
        start_time = time.time()

        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å timeout –¥–ª—è –∫–∞–∂–¥–æ–π
        async def parse_with_timeout(url):
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ —Å retry
                result = await asyncio.wait_for(
                    self._parse_with_retry(url, max_retries=max_retries),
                    timeout=timeout_per_url
                )
                return result
            except asyncio.TimeoutError:
                logger.error(f"‚è±Ô∏è Timeout ({timeout_per_url}s) parsing {url}")
                return ParseResult(
                    url=url,
                    ok=False,
                    data={'url': url, 'title': 'Timeout –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ'},
                    error_type='timeout',
                    error_message=f'–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({timeout_per_url}s)',
                    retries_used=max_retries
                )
            except Exception as e:
                logger.error(f"‚ùå Error parsing {url}: {e}")
                return ParseResult(
                    url=url,
                    ok=False,
                    data={'url': url, 'title': '–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞'},
                    error_type='parse_error',
                    error_message=str(e),
                    retries_used=max_retries
                )

        tasks = [parse_with_timeout(url) for url in urls]

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks, return_exceptions=False)

        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        successful = sum(1 for r in results if r.ok)
        failed = len(results) - successful
        total_retries = sum(r.retries_used for r in results)

        elapsed = time.time() - start_time
        avg_time = elapsed / len(urls) if urls else 0

        logger.info(
            f"‚úì Parallel parsing complete: {successful} OK, {failed} failed "
            f"(total retries: {total_retries}) in {elapsed:.1f}s (avg: {avg_time:.2f}s per URL)"
        )

        return results

    def _get_page_content(self, url: str) -> Optional[str]:
        """
        Sync –º–µ—Ç–æ–¥ (—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å BaseCianParser)

        Note: –í async —Ä–µ–∂–∏–º–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        """
        raise NotImplementedError("Use async methods for AsyncPlaywrightParser")


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# SYNC WRAPPER –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ sync –∫–æ–¥–µ
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def parse_multiple_urls_parallel(
    urls: List[str],
    headless: bool = True,
    cache=None,
    region: str = 'spb',
    max_concurrent: int = 3,
    max_retries: int = 2
) -> tuple[List[Dict], Dict]:
    """
    Sync –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Flask)

    Args:
        urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        headless: Headless —Ä–µ–∂–∏–º
        cache: Cache instance
        region: –†–µ–≥–∏–æ–Ω
        max_concurrent: –ú–∞–∫—Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (—Å–Ω–∏–∂–µ–Ω–æ –¥–æ 3 –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è rate limiting)
        max_retries: –ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ URL

    Returns:
        Tuple: (—Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞, –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞)
    """
    async def _run():
        async with AsyncPlaywrightParser(
            headless=headless,
            cache=cache,
            region=region,
            max_concurrent=max_concurrent
        ) as parser:
            return await parser.parse_multiple_async(urls, max_retries=max_retries)

    # –ó–∞–ø—É—Å–∫–∞–µ–º async –∫–æ–¥, –æ–±—Ö–æ–¥—è –ø—Ä–æ–±–ª–µ–º—É —Å running event loop
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã–π event loop
        asyncio.get_running_loop()
        # –ï—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π loop - –∑–∞–ø—É—Å–∫–∞–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, _run())
            parse_results: List[ParseResult] = future.result()
    except RuntimeError:
        # –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ loop - –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å asyncio.run() –Ω–∞–ø—Ä—è–º—É—é
        parse_results: List[ParseResult] = asyncio.run(_run())

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º ParseResult –≤ dict –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    results_data = []
    for pr in parse_results:
        data = pr.data.copy()
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –ø–∞—Ä—Å–∏–Ω–≥–µ
        if not pr.ok:
            data['parse_failed'] = True
            data['parse_error_type'] = pr.error_type
            data['parse_retries'] = pr.retries_used
        results_data.append(data)

    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    quality_metrics = {
        'total_found': len(urls),
        'successfully_parsed': sum(1 for pr in parse_results if pr.ok),
        'parse_failed': sum(1 for pr in parse_results if not pr.ok),
        'total_retries': sum(pr.retries_used for pr in parse_results),
        'error_breakdown': {}
    }

    # –ü–æ–¥—Å—á–µ—Ç –æ—à–∏–±–æ–∫ –ø–æ —Ç–∏–ø–∞–º
    for pr in parse_results:
        if not pr.ok and pr.error_type:
            quality_metrics['error_breakdown'][pr.error_type] = \
                quality_metrics['error_breakdown'].get(pr.error_type, 0) + 1

    return results_data, quality_metrics
