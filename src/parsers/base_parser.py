"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –ø–∞—Ä—Å–µ—Ä–∞ —Å –æ–±—â–µ–π –ª–æ–≥–∏–∫–æ–π
"""

import json
import time
import logging
import re
from typing import Optional, Dict, List
from abc import ABC, abstractmethod
from bs4 import BeautifulSoup
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import requests

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ParsingError(Exception):
    """–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""


class BaseCianParser(ABC):
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ Cian

    –°–æ–¥–µ—Ä–∂–∏—Ç –æ–±—â—É—é –ª–æ–≥–∏–∫—É:
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON-LD
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    - Retry –º–µ—Ö–∞–Ω–∏–∑–º
    - –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    - Redis –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    """

    def __init__(self, delay: float = 2.0, cache=None):
        """
        Args:
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            cache: PropertyCache instance (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        self.delay = delay
        self.base_url = "https://www.cian.ru"
        self.cache = cache
        self.stats = {
            'requests': 0,
            'errors': 0,
            'retries': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }

    @abstractmethod
    def _get_page_content(self, url: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        –î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ –ø–æ–¥–∫–ª–∞—Å—Å–∞—Ö
        """

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((requests.RequestException, TimeoutError)),
        before_sleep=lambda retry_state: logger.warning(
            f"Retry {retry_state.attempt_number}/3 after {retry_state.outcome.exception()}"
        )
    )
    def _make_request_with_retry(self, url: str, **kwargs) -> requests.Response:
        """
        HTTP –∑–∞–ø—Ä–æ—Å —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏

        Args:
            url: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è requests

        Returns:
            Response –æ–±—ä–µ–∫—Ç

        Raises:
            requests.RequestException: –ü–æ—Å–ª–µ 3 –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        """
        self.stats['requests'] += 1
        response = requests.get(url, timeout=30, **kwargs)
        response.raise_for_status()
        time.sleep(self.delay)
        return response

    def _extract_json_ld(self, soup: BeautifulSoup) -> Optional[Dict]:
        """
        –ò–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ JSON-LD

        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None
        """
        try:
            json_ld_script = soup.find('script', type='application/ld+json')
            if json_ld_script and json_ld_script.string:
                return json.loads(json_ld_script.string)
        except (json.JSONDecodeError, AttributeError) as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å JSON-LD: {e}")
        return None

    def _extract_basic_info(self, soup: BeautifulSoup, data: Dict) -> Dict:
        """
        –ò–∑–≤–ª–µ—á—å –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ HTML

        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç
            data: –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è

        Returns:
            –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        if not data.get('title'):
            title_elem = soup.find('h1', {'data-mark': 'OfferTitle'}) or soup.find('h1')
            if title_elem:
                data['title'] = title_elem.get_text(strip=True)

        # –ê–¥—Ä–µ—Å
        if not data.get('address'):
            # –ú–µ—Ç–æ–¥ 1: GeoLabel (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
            address_elem = soup.find('a', {'data-name': 'GeoLabel'})
            if address_elem:
                data['address'] = address_elem.get_text(strip=True)

            # –ú–µ—Ç–æ–¥ 2: AddressItem (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç) - –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ç–µ–≥–µ <a>
            if not data.get('address'):
                address_item = soup.find('a', {'data-name': 'AddressItem'})
                if address_item:
                    data['address'] = address_item.get_text(strip=True)

            # –ú–µ—Ç–æ–¥ 3: –ò–∑ breadcrumbs –≤ OfferBreadcrumbs
            if not data.get('address'):
                breadcrumbs_div = soup.find('div', {'data-name': 'OfferBreadcrumbs'})
                if breadcrumbs_div:
                    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ —Å /geo/ –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–π (–≥–æ—Ä–æ–¥–∞)
                    geo_links = breadcrumbs_div.find_all('a', href=lambda h: h and '/geo/' in h)
                    if geo_links:
                        address_parts = []
                        for link in geo_links:
                            text = link.get_text(strip=True)
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥–æ—Ä–æ–¥ –∏ –æ–±—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                            if text not in ['–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', '–ú–æ—Å–∫–≤–∞', '–ü—Ä–æ–¥–∞–∂–∞', '–ö—É–ø–∏—Ç—å'] and '–∫–æ–º–Ω–∞—Ç' not in text.lower():
                                address_parts.append(text)
                        if address_parts:
                            data['address'] = ', '.join(address_parts)

            # –ù–û–í–û–ï: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ street_url –∏–∑ breadcrumbs –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–Ω–∞–ª–æ–≥–æ–≤
            # URL –≤–∏–¥–∞: /kupit-1-komnatnuyu-kvartiru-moskva-proizvodstvennaya-ulica-021905
            # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ç–∏–ø—ã: ulica, prospekt, pereulok, bulvar, shosse, naberezhnaya, ploshad, proezd, alleya
            if not data.get('street_url'):
                breadcrumbs_div = soup.find('div', {'data-name': 'OfferBreadcrumbs'})
                if breadcrumbs_div:
                    street_types = ['-ulica-', '-prospekt-', '-pereulok-', '-bulvar-', '-shosse-',
                                    '-naberezhnaya-', '-ploshad-', '-proezd-', '-alleya-', '-tupik-']
                    for link in breadcrumbs_div.find_all('a', href=True):
                        href = link.get('href', '')
                        # –ò—â–µ–º —Å—Å—ã–ª–∫—É —Å —É–ª–∏—Ü–µ–π (—Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–∏–ø —É–ª–∏—Ü—ã –∏ geo-id)
                        has_street_type = any(st in href for st in street_types)
                        has_geo_id = re.search(r'-\d{5,}$', href)
                        if has_street_type and has_geo_id:
                            street_url = href if href.startswith('http') else f"https://www.cian.ru{href}"
                            data['street_url'] = street_url
                            logger.info(f"‚úì –ù–∞–π–¥–µ–Ω street_url –∏–∑ breadcrumbs: {street_url[:80]}...")
                            break

        # –ñ–ö (–ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å)
        if not data.get('residential_complex'):
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ñ–ö

            # –ú–µ—Ç–æ–¥ 1: –ò–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ñ–ö (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π!)
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ —Ç–∏–ø–∞: https://zhk-–Ω–∞–∑–≤–∞–Ω–∏–µ.cian.ru/ –∏–ª–∏ /zhiloy-kompleks-–Ω–∞–∑–≤–∞–Ω–∏–µ/
            zhk_links = soup.find_all('a', href=True)
            for link in zhk_links:
                href = link.get('href')
                text = link.get_text(strip=True)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Å—ã–ª–æ–∫ –Ω–∞ –ñ–ö
                if 'zhk-' in href and '.cian.ru' in href:
                    # –ù–∞–ø—Ä–∏–º–µ—Ä: https://zhk-moiseenko-10.cian.ru/
                    data['residential_complex'] = text.replace('–ñ–ö ', '').replace('¬´', '').replace('¬ª', '').strip()
                    data['residential_complex_url'] = href
                    logger.info(f"‚úì –ù–∞–π–¥–µ–Ω –ñ–ö –ø–æ —Å—Å—ã–ª–∫–µ: {text} ‚Üí {href[:50]}")
                    break
                elif '/zhiloy-kompleks-' in href or '/kupit-kvartiru-zhiloy-kompleks-' in href:
                    # –ù–∞–ø—Ä–∏–º–µ—Ä: /kupit-kvartiru-zhiloy-kompleks-moiseenko-10-5094487/
                    data['residential_complex'] = text.replace('–ñ–ö ', '').replace('¬´', '').replace('¬ª', '').strip()
                    data['residential_complex_url'] = href if href.startswith('http') else f"https://www.cian.ru{href}"
                    logger.info(f"‚úì –ù–∞–π–¥–µ–Ω –ñ–ö –ø–æ —Å—Å—ã–ª–∫–µ: {text} ‚Üí {href[:50]}")
                    break

            # –ú–µ—Ç–æ–¥ 2: –ò–∑ breadcrumbs
            if not data.get('residential_complex'):
                breadcrumbs = soup.find('div', {'data-name': 'Breadcrumbs'}) or soup.find('nav', class_='breadcrumbs')
                if breadcrumbs:
                    links = breadcrumbs.find_all('a')
                    for link in links:
                        text = link.get_text(strip=True)
                        href = link.get('href', '')
                        # –ñ–ö –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ /zhk-–Ω–∞–∑–≤–∞–Ω–∏–µ/
                        if '/zhk-' in href or '–ñ–ö' in text:
                            data['residential_complex'] = text.replace('–ñ–ö ', '').replace('¬´', '').replace('¬ª', '').strip()
                            break

            # –ú–µ—Ç–æ–¥ 3: –ò–∑ –∞–¥—Ä–µ—Å–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å "–ñ–ö")
            if not data.get('residential_complex') and data.get('address'):
                # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "–ñ–ö –ù–∞–∑–≤–∞–Ω–∏–µ"
                match = re.search(r'–ñ–ö\s+([–ê-–Ø–∞-—è—ë–Å\s\-\d]+?)(?:,|$)', data['address'])
                if match:
                    data['residential_complex'] = match.group(1).strip()

            # –ú–µ—Ç–æ–¥ 4: –ò–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if not data.get('residential_complex') and data.get('title'):
                match = re.search(r'–ñ–ö\s+([–ê-–Ø–∞-—è—ë–Å\s\-\d]+?)(?:,|–≤|‚Äî|$)', data['title'])
                if match:
                    data['residential_complex'] = match.group(1).strip()

            # –ú–µ—Ç–æ–¥ 5: –ò–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ (–µ—Å–ª–∏ —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã)
            chars = data.get('characteristics', {})
            if not data.get('residential_complex') and chars:
                for key in ['–ñ–∏–ª–æ–π –∫–æ–º–ø–ª–µ–∫—Å', '–ñ–ö', '–ù–∞–∑–≤–∞–Ω–∏–µ –ñ–ö']:
                    if key in chars:
                        data['residential_complex'] = chars[key]
                        break

        # –ú–µ—Ç—Ä–æ
        if not data.get('metro'):
            # –ú–µ—Ç–æ–¥ 1: UndergroundLabel (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
            metro_elems = soup.find_all('a', {'data-name': 'UndergroundLabel'})
            if metro_elems:
                data['metro'] = [metro.get_text(strip=True) for metro in metro_elems]

            # –ú–µ—Ç–æ–¥ 2: UndergroundItem (–Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç)
            if not data.get('metro') or len(data['metro']) == 0:
                metro_items = soup.find_all('div', {'data-name': 'UndergroundItem'})
                if metro_items:
                    metro_list = []
                    for item in metro_items:
                        # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏ –≤–Ω—É—Ç—Ä–∏
                        text = item.get_text(strip=True)
                        # –£–±–∏—Ä–∞–µ–º –≤—Ä–µ–º—è –≤ –ø—É—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä "5 –º–∏–Ω." –∏–ª–∏ "10 –º–∏–Ω. –ø–µ—à–∫–æ–º")
                        # –£–¥–∞–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤—Ä–æ–¥–µ "5 –º–∏–Ω", "10 –º–∏–Ω. –ø–µ—à–∫–æ–º"
                        text = re.sub(r'\d+\s*–º–∏–Ω\.?\s*(–ø–µ—à–∫–æ–º)?', '', text).strip()
                        if text and text not in metro_list:
                            metro_list.append(text)
                    if metro_list:
                        data['metro'] = metro_list

            # –ú–µ—Ç–æ–¥ 3: –ò–∑ breadcrumbs (–Ω–æ–≤–µ–π—à–∏–π —Ñ–æ—Ä–º–∞—Ç)
            if not data.get('metro') or len(data['metro']) == 0:
                breadcrumbs_div = soup.find('div', {'data-name': 'OfferBreadcrumbs'})
                if breadcrumbs_div:
                    # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º "–º–µ—Ç—Ä–æ"
                    links = breadcrumbs_div.find_all('a')
                    metro_list = []
                    for link in links:
                        text = link.get_text(strip=True)
                        # –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å "–º–µ—Ç—Ä–æ", –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–∞–Ω—Ü–∏–∏
                        if '–º–µ—Ç—Ä–æ' in text.lower():
                            # –£–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–æ "–º–µ—Ç—Ä–æ" –∏ –æ—á–∏—â–∞–µ–º
                            station = text.replace('–º–µ—Ç—Ä–æ', '').replace('–º.', '').strip()
                            if station and station not in metro_list:
                                metro_list.append(station)
                    if metro_list:
                        data['metro'] = metro_list

        # –û–ø–∏—Å–∞–Ω–∏–µ
        if not data.get('description'):
            desc_elem = soup.find('div', {'data-name': 'Description'})
            if desc_elem:
                p_elem = desc_elem.find('p')
                if p_elem:
                    data['description'] = p_elem.get_text(strip=True)
                else:
                    data['description'] = desc_elem.get_text(strip=True)

        return data

    def _extract_characteristics(self, soup: BeautifulSoup) -> Dict[str, str]:
        """
        –ò–∑–≤–ª–µ—á—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑ HTML

        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
        """
        characteristics = {}

        # –ú–µ—Ç–æ–¥ 1: OfferSummaryInfoItem (–æ—Å–Ω–æ–≤–Ω–æ–π –¥–ª—è –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)
        summary_items = soup.find_all('div', {'data-name': 'OfferSummaryInfoItem'})
        for item in summary_items:
            # –ò—â–µ–º –ø–∞—Ä—ã label-value –≤–Ω—É—Ç—Ä–∏ item
            spans = item.find_all('span')
            paragraphs = item.find_all('p')

            # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ spans
            if len(spans) >= 2:
                label = spans[0].get_text(strip=True)
                value = spans[1].get_text(strip=True)
                if label and value:
                    characteristics[label] = value
            # –ò–ª–∏ –∏–∑ paragraphs
            elif len(paragraphs) >= 2:
                label = paragraphs[0].get_text(strip=True)
                value = paragraphs[1].get_text(strip=True)
                if label and value:
                    characteristics[label] = value

        # –ú–µ—Ç–æ–¥ 2: ObjectFactoidsItem (–¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π: –ø–ª–æ—â–∞–¥—å, —ç—Ç–∞–∂, –≥–æ–¥)
        factoid_items = soup.find_all('div', {'data-name': 'ObjectFactoidsItem'})
        for item in factoid_items:
            # –ò—â–µ–º label –∏ value - –æ–Ω–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ —Ä–∞–∑–Ω—ã—Ö span'–∞—Ö
            spans = item.find_all('span')
            if len(spans) >= 2:
                # –ü–µ—Ä–≤—ã–π span - –º–µ—Ç–∫–∞ (—Å–µ—Ä—ã–π —Ç–µ–∫—Å—Ç), –≤—Ç–æ—Ä–æ–π - –∑–Ω–∞—á–µ–Ω–∏–µ (–∂–∏—Ä–Ω—ã–π)
                label = spans[0].get_text(strip=True)
                value = spans[1].get_text(strip=True)
                if label and value:
                    characteristics[label] = value

        # –ú–µ—Ç–æ–¥ 3: OfferFactItem (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)
        char_items = soup.find_all('div', {'data-name': 'OfferFactItem'})
        for item in char_items:
            # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ label –∏ value
            children = list(item.find_all(['span', 'p']))
            if len(children) >= 2:
                label = children[0].get_text(strip=True)
                value = children[1].get_text(strip=True)
                if label and value:
                    characteristics[label] = value

        # –ú–µ—Ç–æ–¥ 4: –ò–∑ —Å–ø–∏—Å–∫–æ–≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        char_lists = soup.find_all('ul', class_=lambda x: x and 'item' in str(x).lower())
        for ul in char_lists:
            items = ul.find_all('li')
            for item in items:
                text = item.get_text(strip=True)
                if ':' in text and len(text) < 200:
                    parts = text.split(':', 1)
                    if len(parts) == 2:
                        key, value = parts[0].strip(), parts[1].strip()
                        if key and value:
                            characteristics[key] = value

        return characteristics

    def _extract_images(self, soup: BeautifulSoup, max_images: int = 30) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç
            max_images: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

        Returns:
            –°–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        images = []

        # –ú–µ—Ç–æ–¥ 1: –ò–∑ –≥–∞–ª–µ—Ä–µ–∏
        img_gallery = soup.find('div', {'data-name': 'GalleryPreviews'})
        if img_gallery:
            img_elems = img_gallery.find_all('img')
            images = [
                img.get('src') or img.get('data-src')
                for img in img_elems
                if img.get('src') or img.get('data-src')
            ]

        # –ú–µ—Ç–æ–¥ 2: –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è CDN
        if not images:
            all_images = soup.find_all('img', src=lambda x: x and 'cdn-p.cian.site' in x)
            images = [img.get('src') for img in all_images]

        return images[:max_images]

    def _extract_seller_info(self, soup: BeautifulSoup) -> Dict[str, str]:
        """
        –ò–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥–∞–≤—Ü–µ

        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–¥–∞–≤—Ü–µ
        """
        seller = {}

        seller_name = soup.find('div', {'data-name': 'OfferAuthorName'})
        if seller_name:
            seller['name'] = seller_name.get_text(strip=True)

        seller_type = soup.find('div', {'data-name': 'OfferAuthorType'})
        if seller_type:
            seller['type'] = seller_type.get_text(strip=True)

        return seller

    def _promote_key_fields(self, data: Dict) -> None:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è –∏–∑ characteristics –≤ –∫–æ—Ä–µ–Ω—å –¥–∞–Ω–Ω—ã—Ö
        –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã —Å API

        Args:
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è (–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è in-place)
        """

        characteristics = data.get('characteristics', {})
        if not characteristics:
            return

        # –ú–∞–ø–ø–∏–Ω–≥: –∫–ª—é—á –≤ characteristics -> –∫–ª—é—á –≤ –∫–æ—Ä–Ω–µ –¥–∞–Ω–Ω—ã—Ö
        mappings = {
            # –ü–ª–æ—â–∞–¥—å (–ò–°–ü–†–ê–í–õ–ï–ù–û: total_area –≤–º–µ—Å—Ç–æ area –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å Pydantic)
            '–û–±—â–∞—è –ø–ª–æ—â–∞–¥—å': ('total_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            '–ü–ª–æ—â–∞–¥—å': ('total_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            '–û–±—â–∞—è, –º¬≤': ('total_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            # –ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å - –†–ê–°–®–ò–†–ï–ù–û: –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∫–ª—é—á–µ–π
            '–ñ–∏–ª–∞—è –ø–ª–æ—â–∞–¥—å': ('living_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            '–ñ–∏–ª–∞—è': ('living_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            '–ñ–∏–ª–∞—è, –º¬≤': ('living_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            # –ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏
            '–ü–ª–æ—â–∞–¥—å –∫—É—Ö–Ω–∏': ('kitchen_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            '–ö—É—Ö–Ω—è': ('kitchen_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            '–ö—É—Ö–Ω—è, –º¬≤': ('kitchen_area', lambda x: float(re.sub(r'[^\d,.]', '', x).replace(',', '.')) if x else None),
            # –≠—Ç–∞–∂ (–∏–∑–≤–ª–µ–∫–∞–µ—Ç floor –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ "5 –∏–∑ 10")
            '–≠—Ç–∞–∂': ('floor', lambda x: int(x.split()[0]) if x and '–∏–∑' in x else (int(x) if x.isdigit() else None)),
            # –ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏
            '–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏': ('build_year', lambda x: int(re.sub(r'[^\d]', '', x)) if x else None),
            '–ü–æ—Å—Ç—Ä–æ–µ–Ω': ('build_year', lambda x: int(re.sub(r'[^\d]', '', x)) if x else None),
        }

        for char_key, (data_key, transform_func) in mappings.items():
            if char_key in characteristics and not data.get(data_key):
                try:
                    value = characteristics[char_key]
                    data[data_key] = transform_func(value) if transform_func else value
                except Exception:
                    pass

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç—Ç–∞–∂–µ–π –∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ "–≠—Ç–∞–∂"
        if '–≠—Ç–∞–∂' in characteristics and not data.get('floor_total'):
            try:
                floor_value = characteristics['–≠—Ç–∞–∂']
                if floor_value and '–∏–∑' in floor_value:
                    data['floor_total'] = int(floor_value.split()[-1])
            except Exception:
                pass

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if not data.get('rooms') and data.get('title'):
            title = data['title']
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã: "1-–∫–æ–º–Ω.", "2-–∫–æ–º–Ω.", "3-–∫–æ–º–Ω.", "—Å—Ç—É–¥–∏—è"
            if '—Å—Ç—É–¥–∏' in title.lower():
                data['rooms'] = '—Å—Ç—É–¥–∏—è'
            else:
                match = re.search(r'(\d+)-–∫–æ–º–Ω', title)
                if match:
                    data['rooms'] = int(match.group(1))

    def _extract_premium_features(self, soup: BeautifulSoup, data: Dict) -> None:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–µ–º–∏—É–º-—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –æ–±—ä–µ–∫—Ç–∞ –∏–∑ HTML –∏ –æ–ø–∏—Å–∞–Ω–∏—è

        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            data: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ (–º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è in-place)
        """

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        page_text = soup.get_text(separator=' ', strip=True).lower()

        # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞
        description = data.get('description', '').lower() if data.get('description') else ''

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è –ø–æ–∏—Å–∫–∞
        full_text = f"{page_text} {description}"

        # === –î–ò–ó–ê–ô–ù–ï–†–°–ö–ê–Ø –û–¢–î–ï–õ–ö–ê ===
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∏–π —Ä–µ–º–æ–Ω—Ç, –¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∞—è –æ—Ç–¥–µ–ª–∫–∞, –∞–≤—Ç–æ—Ä—Å–∫–∏–π –¥–∏–∑–∞–π–Ω
        design_keywords = [
            '–¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫',
            '–∞–≤—Ç–æ—Ä—Å–∫',
            '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω',
            '—ç–∫—Å–∫–ª—é–∑–∏–≤–Ω',
            '–ø—Ä–µ–º–∏–∞–ª—å–Ω',
        ]
        data['–¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∞—è –æ—Ç–¥–µ–ª–∫–∞'] = any(kw in full_text for kw in design_keywords)

        # === –ü–ê–ù–û–†–ê–ú–ù–´–ï –í–ò–î–´ ===
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –ø–∞–Ω–æ—Ä–∞–º–Ω—ã–µ –æ–∫–Ω–∞, –ø–∞–Ω–æ—Ä–∞–º–Ω—ã–π –≤–∏–¥, –≤–∏–¥ –Ω–∞ –≤–æ–¥—É, –≤–∏–¥ –Ω–∞ –ø–∞—Ä–∫
        view_keywords = [
            '–ø–∞–Ω–æ—Ä–∞–º',
            '–≤–∏–¥ –Ω–∞ –≤–æ–¥—É',
            '–≤–∏–¥ –Ω–∞ —Ä–µ–∫—É',
            '–≤–∏–¥ –Ω–∞ –∑–∞–ª–∏–≤',
            '–≤–∏–¥ –Ω–∞ –ø–∞—Ä–∫',
            '–≤–∏–¥ –Ω–∞ –ª–µ—Å',
            '—Å –≤–∏–¥–æ–º –Ω–∞',
        ]
        data['–ø–∞–Ω–æ—Ä–∞–º–Ω—ã–µ –≤–∏–¥—ã'] = any(kw in full_text for kw in view_keywords)

        # === –ü–†–ï–ú–ò–£–ú –õ–û–ö–ê–¶–ò–Ø ===
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é —Ä–∞–π–æ–Ω–∞ / –∞–¥—Ä–µ—Å—É
        premium_locations = [
            '–ø–µ—Ç—Ä–æ–≤—Å–∫',  # –ü–µ—Ç—Ä–æ–≤—Å–∫–∏–π –æ—Å—Ç—Ä–æ–≤
            '–∫—Ä–µ—Å—Ç–æ–≤—Å–∫',  # –ö—Ä–µ—Å—Ç–æ–≤—Å–∫–∏–π –æ—Å—Ç—Ä–æ–≤
            '–≤–∞—Å–∏–ª—å–µ–≤—Å–∫',  # –í–∞—Å–∏–ª—å–µ–≤—Å–∫–∏–π –æ—Å—Ç—Ä–æ–≤ (—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å)
            '–∫–∞–º–µ–Ω–Ω–æ–æ—Å—Ç—Ä–æ–≤',  # –ö–∞–º–µ–Ω–Ω–æ–æ—Å—Ç—Ä–æ–≤—Å–∫–∏–π
            '–ø—Ä–∏–º–æ—Ä—Å–∫',  # –ü—Ä–∏–º–æ—Ä—Å–∫–∏–π —Ä–∞–π–æ–Ω (—ç–ª–∏—Ç–Ω—ã–µ —á–∞—Å—Ç–∏)
            '—Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π —Ä–∞–π–æ–Ω',
            '–∞–¥–º–∏—Ä–∞–ª—Ç–µ–π—Å–∫',
        ]

        address = (data.get('address') or '').lower()
        residential_complex = (data.get('residential_complex') or '').lower()

        data['–ø—Ä–µ–º–∏—É–º –ª–æ–∫–∞—Ü–∏—è'] = any(
            loc in address or loc in residential_complex or loc in full_text
            for loc in premium_locations
        )

        # === –¢–û–õ–¨–ö–û –†–ï–ù–î–ï–†–´ (–ù–ï–¢ –§–û–¢–û) ===
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ —Ç–æ–º, —á—Ç–æ –æ–±—ä–µ–∫—Ç –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
        # –∏–ª–∏ –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        renders_keywords = [
            '–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü',
            '—Ä–µ–Ω–¥–µ—Ä',
            '—Å—Ç—Ä–æ–∏—Ç',
            '—Å–¥–∞—á–∞',
            '–ø–ª–∞–Ω–∏—Ä—É–µ–º',
        ]

        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏ - –µ—Å–ª–∏ –±—É–¥—É—â–∏–π, —Ç–æ —Ä–µ–Ω–¥–µ—Ä—ã
        build_year = data.get('build_year')
        from datetime import datetime
        current_year = datetime.now().year

        is_under_construction = build_year and build_year > current_year
        has_render_keywords = any(kw in full_text for kw in renders_keywords)

        data['—Ç–æ–ª—å–∫–æ —Ä–µ–Ω–¥–µ—Ä—ã'] = is_under_construction or has_render_keywords

        # === –ü–ê–†–ö–û–í–ö–ê ===
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞
        characteristics = data.get('characteristics', {})

        parking_value = None
        for key in characteristics:
            if '–ø–∞—Ä–∫–∏–Ω–≥' in key.lower() or '–ø–∞—Ä–∫–æ–≤–∫' in key.lower():
                val = characteristics[key].lower()
                if '–ø–æ–¥–∑–µ–º–Ω' in val or '–º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤' in val:
                    parking_value = '–ø–æ–¥–∑–µ–º–Ω–∞—è'
                elif '–∑–∞–∫—Ä—ã—Ç' in val or '–æ—Ö—Ä–∞–Ω—è–µ–º' in val:
                    parking_value = '–∑–∞–∫—Ä—ã—Ç–∞—è'
                elif '–æ—Ç–∫—Ä—ã—Ç' in val or '–Ω–∞ —É–ª–∏—Ü–µ' in val:
                    parking_value = '–Ω–∞ —É–ª–∏—Ü–µ'
                else:
                    parking_value = '–µ—Å—Ç—å'
                break

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
        if not parking_value:
            if '–ø–æ–¥–∑–µ–º–Ω' in full_text and ('–ø–∞—Ä–∫–∏–Ω–≥' in full_text or '–ø–∞—Ä–∫–æ–≤–∫' in full_text):
                parking_value = '–ø–æ–¥–∑–µ–º–Ω–∞—è'
            elif '–º–∞—à–∏–Ω–æ–º–µ—Å—Ç' in full_text or '–ø–∞—Ä–∫–∏–Ω–≥' in full_text:
                parking_value = '–µ—Å—Ç—å'

        data['–ø–∞—Ä–∫–æ–≤–∫–∞'] = parking_value

        # === –í–´–°–û–¢–ê –ü–û–¢–û–õ–ö–û–í ===
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω —Ç–∏–ø–∞ "3.2 –º", "–≤—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤ 3 –º", "–ø–æ—Ç–æ–ª–∫–∏ 3.5–º"
        # –£–õ–£–ß–®–ï–ù–û: –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        ceiling_patterns = [
            r'–ø–æ—Ç–æ–ª–∫[–∞-—è]*[^\d]{0,20}?(\d+(?:[.,]\d+)?)\s*–º',  # "–ø–æ—Ç–æ–ª–∫–∏ 3.2 –º"
            r'–≤—ã—Å–æ—Ç[–∞-—è]*\s*–ø–æ—Ç–æ–ª–∫[–∞-—è]*[^\d]{0,20}?(\d+(?:[.,]\d+)?)\s*–º',  # "–≤—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤ 3 –º"
            r'–≤—ã—Å–æ—Ç[–∞-—è]*[^\d]{0,20}?(\d+(?:[.,]\d+)?)\s*–º',  # "–≤—ã—Å–æ—Ç–∞ 3.5 –º"
        ]

        for pattern in ceiling_patterns:
            ceiling_match = re.search(pattern, full_text)
            if ceiling_match:
                try:
                    ceiling_height = float(ceiling_match.group(1).replace(',', '.'))
                    if 2.3 <= ceiling_height <= 6.0:  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ä–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
                        data['–≤—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤'] = ceiling_height
                        logger.debug(f"–ù–∞–π–¥–µ–Ω–∞ –≤—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤: {ceiling_height}–º (–ø–∞—Ç—Ç–µ—Ä–Ω: {pattern})")
                        break
                except (ValueError, IndexError):
                    continue

        # === –û–•–†–ê–ù–ê 24/7 ===
        security_keywords = [
            '–æ—Ö—Ä–∞–Ω',
            '–∫–æ–Ω—Å—å–µ—Ä–∂',
            'security',
            '24/7',
            '–≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω',
            '–∫–æ–Ω—Ç—Ä–æ–ª—å –¥–æ—Å—Ç—É–ø',
        ]
        data['–æ—Ö—Ä–∞–Ω–∞ 24/7'] = any(kw in full_text for kw in security_keywords)

        # === –¢–ò–ü –î–û–ú–ê ===
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞
        characteristics = data.get('characteristics', {})

        house_type = None
        for key in characteristics:
            if '—Ç–∏–ø –¥–æ–º–∞' in key.lower() or '–∑–¥–∞–Ω–∏–µ' in key.lower():
                val = characteristics[key].lower()
                if '–º–æ–Ω–æ–ª–∏—Ç' in val:
                    house_type = '–º–æ–Ω–æ–ª–∏—Ç'
                elif '–∫–∏—Ä–ø–∏—á' in val:
                    house_type = '–∫–∏—Ä–ø–∏—á'
                elif '–ø–∞–Ω–µ–ª' in val:
                    house_type = '–ø–∞–Ω–µ–ª—å'
                break

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
        if not house_type:
            if '–º–æ–Ω–æ–ª–∏—Ç' in full_text:
                house_type = '–º–æ–Ω–æ–ª–∏—Ç'
            elif '–∫–∏—Ä–ø–∏—á' in full_text and '–¥–æ–º' in full_text:
                house_type = '–∫–∏—Ä–ø–∏—á'
            elif '–ø–∞–Ω–µ–ª' in full_text and '–¥–æ–º' in full_text:
                house_type = '–ø–∞–Ω–µ–ª—å'

        data['house_type'] = house_type

        # === –í–°–ï–ì–û –≠–¢–ê–ñ–ï–ô ===
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –ø–æ–ª—è floor, –∫–æ—Ç–æ—Ä–æ–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ "6/9"
        if not data.get('total_floors'):
            # –ò—â–µ–º –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö
            for key in characteristics:
                if '—ç—Ç–∞–∂' in key.lower():
                    val = characteristics[key]
                    if isinstance(val, str) and '/' in val:
                        try:
                            floor_total = int(val.split('/')[-1])
                            data['total_floors'] = floor_total
                            break
                        except ValueError:
                            pass

        logger.debug("–ò–∑–≤–ª–µ—á–µ–Ω—ã –ø—Ä–µ–º–∏—É–º-—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:")
        logger.debug(f"  –î–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∞—è –æ—Ç–¥–µ–ª–∫–∞: {data.get('–¥–∏–∑–∞–π–Ω–µ—Ä—Å–∫–∞—è –æ—Ç–¥–µ–ª–∫–∞')}")
        logger.debug(f"  –ü–∞–Ω–æ—Ä–∞–º–Ω—ã–µ –≤–∏–¥—ã: {data.get('–ø–∞–Ω–æ—Ä–∞–º–Ω—ã–µ –≤–∏–¥—ã')}")
        logger.debug(f"  –ü—Ä–µ–º–∏—É–º –ª–æ–∫–∞—Ü–∏—è: {data.get('–ø—Ä–µ–º–∏—É–º –ª–æ–∫–∞—Ü–∏—è')}")
        logger.debug(f"  –¢–æ–ª—å–∫–æ —Ä–µ–Ω–¥–µ—Ä—ã: {data.get('—Ç–æ–ª—å–∫–æ —Ä–µ–Ω–¥–µ—Ä—ã')}")
        logger.debug(f"  –ü–∞—Ä–∫–æ–≤–∫–∞: {data.get('–ø–∞—Ä–∫–æ–≤–∫–∞')}")
        logger.debug(f"  –í—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤: {data.get('–≤—ã—Å–æ—Ç–∞ –ø–æ—Ç–æ–ª–∫–æ–≤')}")
        logger.debug(f"  –û—Ö—Ä–∞–Ω–∞ 24/7: {data.get('–æ—Ö—Ä–∞–Ω–∞ 24/7')}")
        logger.debug(f"  –¢–∏–ø –¥–æ–º–∞: {data.get('house_type')}")
        logger.debug(f"  –í—Å–µ–≥–æ —ç—Ç–∞–∂–µ–π: {data.get('total_floors')}")

    def parse_detail_page(self, url: str) -> Dict:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º

        Args:
            url: URL –æ–±—ä—è–≤–ª–µ–Ω–∏—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if self.cache:
            cached_data = self.cache.get_property(url)
            if cached_data:
                self.stats['cache_hits'] += 1
                logger.info(f"‚úÖ Cache HIT: {url[:60]}...")
                return cached_data
            else:
                self.stats['cache_misses'] += 1

        logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")

        try:
            html = self._get_page_content(url)
            if not html:
                raise ParsingError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç: {url}")

            soup = BeautifulSoup(html, 'lxml')

            data = {
                'url': url,
                'title': None,
                'price': None,
                'price_raw': None,
                'currency': None,
                'description': None,
                'address': None,
                'residential_complex': None,
                'residential_complex_url': None,  # –°—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É –ñ–ö
                'metro': [],
                'characteristics': {},
                'images': [],
                'seller': {},
            }

            # JSON-LD –¥–∞–Ω–Ω—ã–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
            json_ld = self._extract_json_ld(soup)
            if json_ld:
                logger.info("‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º JSON-LD –¥–∞–Ω–Ω—ã–µ")
                data['title'] = json_ld.get('name')

                offers = json_ld.get('offers', {})
                if offers:
                    data['price_raw'] = offers.get('price')
                    data['currency'] = offers.get('priceCurrency')
                    if data['price_raw']:
                        data['price'] = data['price_raw']

            # –î–æ–ø–æ–ª–Ω—è–µ–º –∏–∑ HTML
            data = self._extract_basic_info(soup, data)
            data['characteristics'] = self._extract_characteristics(soup)
            data['images'] = self._extract_images(soup)
            data['seller'] = self._extract_seller_info(soup)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è –∏–∑ characteristics –≤ –∫–æ—Ä–µ–Ω—å –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
            self._promote_key_fields(data)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–µ–º–∏—É–º-—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            self._extract_premium_features(soup, data)

            logger.info(f"‚úì –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–µ–Ω: {data.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if self.cache:
                self.cache.set_property(url, data, ttl_hours=24)
                logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫—ç—à: {url[:60]}...")

            return data

        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}", exc_info=True)
            raise ParsingError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}") from e

    def get_stats(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞"""
        return self.stats.copy()

    def reset_stats(self):
        """–°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        self.stats = {
            'requests': 0,
            'errors': 0,
            'retries': 0,
            'cache_hits': 0
        }
