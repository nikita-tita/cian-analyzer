"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è Avito.ru (–ê–≤–∏—Ç–æ –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å)

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ê–≤–∏—Ç–æ:
- –û—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è –∑–∞—â–∏—Ç–∞ (DataDome)
- React SPA –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
- –ú–æ–±–∏–ª—å–Ω–æ–µ API –¥–æ—Å—Ç—É–ø–Ω–æ
- –¢—Ä–µ–±—É–µ—Ç –æ–±—Ö–æ–¥ –∫–∞–ø—á–∏

–°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞:
1. –ú–æ–±–∏–ª—å–Ω–æ–µ API (—á–µ—Ä–µ–∑ curl_cffi —Å Android User-Agent)
2. Nodriver (–æ–±—Ö–æ–¥ DataDome)
3. Proxy rotation + Nodriver (–ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞)
"""

import json
import logging
import re
import time
from typing import Optional, Dict, List, Literal
from bs4 import BeautifulSoup
from urllib.parse import urljoin, parse_qs, urlparse

from .base_real_estate_parser import BaseRealEstateParser, ParserCapabilities, ParsingError
from .field_mapper import get_field_mapper
from .parser_registry import register_parser

logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
try:
    from .strategies.curl_cffi_strategy import CurlCffiStrategy
    CURL_CFFI_AVAILABLE = True
except ImportError:
    CURL_CFFI_AVAILABLE = False

try:
    from .strategies.nodriver_strategy import NodriverStrategy
    NODRIVER_AVAILABLE = True
except ImportError:
    NODRIVER_AVAILABLE = False


@register_parser('avito', [r'avito\.ru', r'www\.avito\.ru'])
class AvitoParser(BaseRealEstateParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è Avito.ru —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç:
    - –ú–æ–±–∏–ª—å–Ω–æ–µ API —á–µ—Ä–µ–∑ curl_cffi (–±—ã—Å—Ç—Ä–æ, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ)
    - Nodriver –¥–ª—è –æ–±—Ö–æ–¥–∞ DataDome
    - Proxy rotation (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """

    def __init__(
        self,
        delay: float = 2.0,
        cache=None,
        region: str = 'spb',
        use_mobile_api: bool = True
    ):
        """
        Args:
            delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            cache: –û–±—ä–µ–∫—Ç –∫—ç—à–∞
            region: –†–µ–≥–∏–æ–Ω ('spb', 'msk')
            use_mobile_api: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–±–∏–ª—å–Ω–æ–µ API (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
        """
        super().__init__(delay, cache)
        self.region = region
        self.use_mobile_api = use_mobile_api

        self.base_url = "https://www.avito.ru"
        self.mobile_api_base = "https://m.avito.ru/api"

        # –ú–∞–ø–ø–∏–Ω–≥ —Ä–µ–≥–∏–æ–Ω–æ–≤
        self.region_codes = {
            'spb': 'sankt-peterburg',
            'msk': 'moskva',
        }
        self.region_slug = self.region_codes.get(region, 'sankt-peterburg')

        # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
        self.curl_cffi: Optional[CurlCffiStrategy] = None
        self.nodriver: Optional[NodriverStrategy] = None

        # –ú–∞–ø–ø–µ—Ä –ø–æ–ª–µ–π
        self.field_mapper = get_field_mapper('avito')

        logger.info(f"‚úì –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω AvitoParser (—Ä–µ–≥–∏–æ–Ω: {region}, mobile_api: {use_mobile_api})")

    def _init_curl_cffi(self):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è curl_cffi"""
        if not CURL_CFFI_AVAILABLE:
            logger.warning("curl_cffi –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return

        if not self.curl_cffi:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Android User-Agent –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ API
            self.curl_cffi = CurlCffiStrategy(
                impersonate='chrome110',  # –ò–º–∏—Ç–∏—Ä—É–µ–º Chrome
                timeout=30
            )
            logger.info("‚úì curl_cffi –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è Avito")

    def _init_nodriver(self):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Nodriver"""
        if not NODRIVER_AVAILABLE:
            logger.warning("Nodriver –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return

        if not self.nodriver:
            self.nodriver = NodriverStrategy(
                headless=False,  # –î–ª—è Avito –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–µ-headless
                timeout=30
            )
            logger.info("‚úì Nodriver –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è Avito")

    # ===== –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ –ü–ê–†–°–ò–ù–ì–ê =====

    def _get_page_content(self, url: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å HTML –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Returns:
            HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ None
        """
        if self.use_mobile_api:
            # –ü—ã—Ç–∞–µ–º—Å—è —á–µ—Ä–µ–∑ –º–æ–±–∏–ª—å–Ω–æ–µ API
            offer_id = self._extract_offer_id(url)
            if offer_id:
                mobile_html = self._get_via_mobile_api(offer_id)
                if mobile_html:
                    return mobile_html

        # Fallback –Ω–∞ Nodriver
        return self._get_via_nodriver(url)

    def _get_via_mobile_api(self, offer_id: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –º–æ–±–∏–ª—å–Ω–æ–µ API

        Args:
            offer_id: ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è

        Returns:
            "HTML" (JSON –¥–∞–Ω–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ HTML-–ø–æ–¥–æ–±–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
        """
        self._init_curl_cffi()

        if not self.curl_cffi:
            return None

        # –ú–æ–±–∏–ª—å–Ω—ã–π API endpoint
        api_url = f"{self.mobile_api_base}/1/items/{offer_id}"

        logger.info(f"üîÑ –ó–∞–ø—Ä–æ—Å –∫ –º–æ–±–∏–ª—å–Ω–æ–º—É API Avito: {api_url}")

        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –º–æ–±–∏–ª—å–Ω–æ–≥–æ API
        headers = {
            'User-Agent': 'Mozilla/5.0 (Linux; Android 10; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Mobile Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ru-RU,ru;q=0.9',
            'X-Requested-With': 'XMLHttpRequest',
            'Referer': f'https://m.avito.ru/{self.region_slug}/kvartiry/{offer_id}',
        }

        try:
            data = self.curl_cffi.fetch_api(api_url, headers=headers)

            if data:
                logger.info(f"‚úì –ú–æ–±–∏–ª—å–Ω–æ–µ API Avito —É—Å–ø–µ—à–Ω–æ: {offer_id}")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
                return json.dumps(data)
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–±–∏–ª—å–Ω–æ–µ API –Ω–µ –≤–µ—Ä–Ω—É–ª–æ –¥–∞–Ω–Ω—ã–µ")
                return None

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ–±–∏–ª—å–Ω–æ–≥–æ API Avito: {e}")
            return None

    def _get_via_nodriver(self, url: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —á–µ—Ä–µ–∑ Nodriver (–æ–±—Ö–æ–¥ DataDome)

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã

        Returns:
            HTML –∫–æ–Ω—Ç–µ–Ω—Ç –∏–ª–∏ None
        """
        self._init_nodriver()

        if not self.nodriver:
            logger.error("Nodriver –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è Avito")
            return None

        try:
            logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ Nodriver (–æ–±—Ö–æ–¥ DataDome): {url}")

            html = self.nodriver.fetch_content(
                url,
                wait_for_selector='[data-marker="item-view"]',
                additional_wait=3  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –¥–ª—è Avito
            )

            return html

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Nodriver: {e}")
            return None

    def _parse_single_property(self, url: str, html: str) -> Dict:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è

        Args:
            url: URL –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç (–∏–ª–∏ JSON —Å—Ç—Ä–æ–∫–∞ –æ—Ç –º–æ–±–∏–ª—å–Ω–æ–≥–æ API)

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        """
        data = {'url': url, 'source': 'avito'}

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ JSON –æ—Ç –º–æ–±–∏–ª—å–Ω–æ–≥–æ API –∏–ª–∏ HTML
        if html and html.strip().startswith('{'):
            # –≠—Ç–æ JSON –æ—Ç –º–æ–±–∏–ª—å–Ω–æ–≥–æ API
            try:
                json_data = json.loads(html)
                logger.info("üì± –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –º–æ–±–∏–ª—å–Ω–æ–≥–æ API Avito")
                data.update(self._parse_from_mobile_api(json_data))
                return data
            except json.JSONDecodeError:
                logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON, –ø—Ä–æ–±—É–µ–º –∫–∞–∫ HTML")

        # –û–±—ã—á–Ω—ã–π HTML –ø–∞—Ä—Å–∏–Ω–≥
        soup = BeautifulSoup(html, 'lxml')

        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ window.__initialData__ –∏–ª–∏ JSON-LD
        initial_data = self._extract_initial_data(html)
        if initial_data:
            data.update(self._parse_from_initial_data(initial_data))

        # JSON-LD
        json_ld = self._extract_json_ld(soup)
        if json_ld:
            data.update(self._parse_from_json_ld(json_ld))

        # Fallback HTML –ø–∞—Ä—Å–∏–Ω–≥
        if not data.get('title'):
            data.update(self._parse_from_html(soup))

        return data

    def _extract_offer_id(self, url: str) -> Optional[str]:
        """
        –ò–∑–≤–ª–µ—á—å ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ URL

        Args:
            url: URL –æ–±—ä—è–≤–ª–µ–Ω–∏—è

        Returns:
            ID –∏–ª–∏ None
        """
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã URL –ê–≤–∏—Ç–æ:
        # https://www.avito.ru/sankt-peterburg/kvartiry/2-k._kvartira_56m_44et._1234567890
        # https://m.avito.ru/sankt-peterburg/kvartiry/1234567890

        patterns = [
            r'/kvartiry/[^/]*_(\d{10,})$',  # Desktop URL
            r'/kvartiry/(\d{10,})$',        # Mobile URL
            r'_(\d{10,})$',                 # Fallback
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID –∏–∑ URL: {url}")
        return None

    def _parse_from_mobile_api(self, api_data: Dict) -> Dict:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –º–æ–±–∏–ª—å–Ω–æ–≥–æ API

        Args:
            api_data: JSON –¥–∞–Ω–Ω—ã–µ –∏–∑ API

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–ø–ø–µ—Ä –ø–æ–ª–µ–π
        return self.field_mapper.transform(api_data)

    def _extract_initial_data(self, html: str) -> Optional[Dict]:
        """
        –ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ window.__initialData__

        Args:
            html: HTML –∫–æ–Ω—Ç–µ–Ω—Ç

        Returns:
            –î–∞–Ω–Ω—ã–µ –∏–ª–∏ None
        """
        pattern = r'window\.__initialData__\s*=\s*"([^"]+)"'

        try:
            match = re.search(pattern, html)
            if match:
                # –î–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã –≤ JSON —Å—Ç—Ä–æ–∫–µ
                json_str = match.group(1)
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                json_str = json_str.encode().decode('unicode_escape')
                data = json.loads(json_str)
                logger.info("‚úì –ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∏–∑ window.__initialData__")
                return data
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è __initialData__: {e}")

        return None

    def _parse_from_initial_data(self, data: Dict) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ __initialData__"""
        result = {}

        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö Avito —Å–ª–æ–∂–Ω–∞—è, –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: data.item (–æ–±—ä—è–≤–ª–µ–Ω–∏–µ)
        item = data.get('item', {})

        if item:
            result.update(self.field_mapper.transform(item))

        return result

    def _extract_json_ld(self, soup: BeautifulSoup) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á—å JSON-LD"""
        try:
            json_ld_script = soup.find('script', type='application/ld+json')
            if json_ld_script and json_ld_script.string:
                return json.loads(json_ld_script.string)
        except Exception as e:
            logger.debug(f"JSON-LD –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
        return None

    def _parse_from_json_ld(self, json_ld: Dict) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ JSON-LD"""
        data = {}

        if json_ld.get('@type') in ['Product', 'RealEstateListing']:
            data['title'] = json_ld.get('name')
            data['description'] = json_ld.get('description')

            if 'offers' in json_ld:
                offers = json_ld['offers']
                if isinstance(offers, dict):
                    data['price'] = offers.get('price')
                    data['currency'] = offers.get('priceCurrency', 'RUB')

        return data

    def _parse_from_html(self, soup: BeautifulSoup) -> Dict:
        """Fallback HTML –ø–∞—Ä—Å–∏–Ω–≥"""
        data = {}

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title_elem = soup.find('h1', {'data-marker': 'item-view/title'}) or soup.find('h1')
        if title_elem:
            data['title'] = title_elem.get_text(strip=True)

        # –¶–µ–Ω–∞
        price_elem = soup.find('[data-marker="item-view/item-price"]')
        if price_elem:
            price_text = price_elem.get_text(strip=True)
            data['price'] = self._extract_number(price_text)

        # –û–ø–∏—Å–∞–Ω–∏–µ
        desc_elem = soup.find('[data-marker="item-view/item-description"]')
        if desc_elem:
            data['description'] = desc_elem.get_text(strip=True)

        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        params = soup.find_all('li', class_='params-paramsList__item')
        characteristics = {}
        for param in params:
            try:
                key = param.get_text(separator='|', strip=True).split('|')[0].strip()
                value = param.get_text(separator='|', strip=True).split('|')[1].strip() if '|' in param.get_text(separator='|') else ''
                if key and value:
                    characteristics[key] = value
            except:
                pass

        data['characteristics'] = characteristics

        return data

    def _extract_number(self, text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        cleaned = re.sub(r'[^\d.]', '', text)
        try:
            return float(cleaned)
        except ValueError:
            return None

    # ===== –ü–û–ò–°–ö –ê–ù–ê–õ–û–ì–û–í =====

    def _search_similar_impl(
        self,
        target_property: Dict,
        limit: int = 20,
        strategy: Literal['same_building', 'same_area', 'citywide'] = 'citywide'
    ) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –Ω–∞ Avito

        Args:
            target_property: –¶–µ–ª–µ–≤–æ–π –æ–±—ä–µ–∫—Ç
            limit: –õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∏—Å–∫–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –∞–Ω–∞–ª–æ–≥–æ–≤
        """
        logger.info(f"üîç –ü–æ–∏—Å–∫ –∞–Ω–∞–ª–æ–≥–æ–≤ –Ω–∞ –ê–≤–∏—Ç–æ (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy})")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
        search_params = self._build_search_params(target_property, strategy)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        try:
            results = self._search_via_url(search_params, limit)
            logger.info(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(results)} –∞–Ω–∞–ª–æ–≥–æ–≤ –Ω–∞ –ê–≤–∏—Ç–æ")
            return results
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ –ê–≤–∏—Ç–æ: {e}")
            return []

    def _build_search_params(self, target: Dict, strategy: str) -> Dict:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è URL –ê–≤–∏—Ç–æ

        Args:
            target: –¶–µ–ª–µ–≤–æ–π –æ–±—ä–µ–∫—Ç
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∏—Å–∫–∞

        Returns:
            –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è URL
        """
        params = {}

        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        price = target.get('price')
        area = target.get('total_area')
        rooms = target.get('rooms')

        if price:
            # ¬±30% –æ—Ç —Ü–µ–Ω—ã
            price_min = int(price * 0.7)
            price_max = int(price * 1.3)
            params['pmin'] = price_min
            params['pmax'] = price_max

        if area:
            # ¬±20% –æ—Ç –ø–ª–æ—â–∞–¥–∏
            area_min = int(area * 0.8)
            area_max = int(area * 1.2)
            params['smin'] = area_min
            params['smax'] = area_max

        if rooms and rooms != '—Å—Ç—É–¥–∏—è':
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
            params['rooms'] = rooms

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–æ–∏—Å–∫–∞
        if strategy == 'same_building' or strategy == 'same_area':
            # –ê–≤–∏—Ç–æ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –ñ–ö –Ω–∞–ø—Ä—è–º—É—é
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥—Ä–µ—Å –∏–ª–∏ —Ä–∞–π–æ–Ω
            address = target.get('address', '')
            if address:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–π–æ–Ω –∏–∑ –∞–¥—Ä–µ—Å–∞
                # –ù–∞–ø—Ä–∏–º–µ—Ä: "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –ù–µ–≤—Å–∫–∏–π —Ä–∞–π–æ–Ω"
                parts = [p.strip() for p in address.split(',')]
                if len(parts) >= 2:
                    params['q'] = parts[1]  # –†–∞–π–æ–Ω

        return params

    def _search_via_url(self, params: Dict, limit: int) -> List[Dict]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ URL —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏

        Args:
            params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
            limit: –õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL –ø–æ–∏—Å–∫–∞
        base_search_url = f"https://www.avito.ru/{self.region_slug}/kvartiry/prodam"

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if params:
            param_str = '&'.join([f"{k}={v}" for k, v in params.items()])
            search_url = f"{base_search_url}?{param_str}"
        else:
            search_url = base_search_url

        logger.info(f"üîç –ü–æ–∏—Å–∫ –ê–≤–∏—Ç–æ: {search_url}")

        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞
        html = self._get_search_page(search_url)

        if not html:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞ –ê–≤–∏—Ç–æ")
            return []

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        results = self._parse_search_results(html, limit)

        return results

    def _get_search_page(self, url: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ–∏—Å–∫–∞

        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞

        Returns:
            HTML –∏–ª–∏ None
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Nodriver –¥–ª—è –æ–±—Ö–æ–¥–∞ DataDome
        return self._get_via_nodriver(url)

    def _parse_search_results(self, html: str, limit: int) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –∏–∑ HTML

        Args:
            html: HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–∏—Å–∫–∞
            limit: –õ–∏–º–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        """
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(html, 'lxml')
        results = []

        # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        # –ê–≤–∏—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç data-marker –¥–ª—è –∫–∞—Ä—Ç–æ—á–µ–∫
        cards = soup.find_all(attrs={'data-marker': 'item'})

        if not cards:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
            cards = soup.find_all('div', class_=re.compile(r'item.*card|iva-item', re.I))

        logger.debug(f"–ù–∞–π–¥–µ–Ω–æ {len(cards)} –∫–∞—Ä—Ç–æ—á–µ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ø–æ–∏—Å–∫–∞")

        for card in cards[:limit]:
            try:
                item = self._parse_search_card(card)
                if item:
                    results.append(item)
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ä—Ç–æ—á–∫–∏: {e}")
                continue

        return results

    def _parse_search_card(self, card) -> Optional[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞

        Args:
            card: BeautifulSoup —ç–ª–µ–º–µ–Ω—Ç –∫–∞—Ä—Ç–æ—á–∫–∏

        Returns:
            –î–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–ª–∏ None
        """
        data = {'source': 'avito'}

        # URL
        link = card.find('a', attrs={'data-marker': 'item-title'})
        if not link:
            link = card.find('a', href=re.compile(r'/kvartiry/'))

        if link and link.get('href'):
            url = link['href']
            if not url.startswith('http'):
                url = f"https://www.avito.ru{url}"
            data['url'] = url
        else:
            return None

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        title_elem = card.find(attrs={'data-marker': 'item-title'})
        if not title_elem:
            title_elem = card.find('h3') or card.find('h2')

        if title_elem:
            data['title'] = title_elem.get_text(strip=True)

        # –¶–µ–Ω–∞
        price_elem = card.find(attrs={'data-marker': 'item-price'})
        if not price_elem:
            price_elem = card.find('span', class_=re.compile(r'price', re.I))

        if price_elem:
            price_text = price_elem.get_text(strip=True)
            data['price'] = self._extract_number(price_text)

        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–ø–ª–æ—â–∞–¥—å, –∫–æ–º–Ω–∞—Ç—ã –∏ —Ç.–¥.)
        params_elem = card.find(attrs={'data-marker': 'item-specific-params'})
        if not params_elem:
            params_elem = card.find('div', class_=re.compile(r'params', re.I))

        if params_elem:
            params_text = params_elem.get_text(strip=True)

            # –ü–ª–æ—â–∞–¥—å
            area_match = re.search(r'(\d+(?:[.,]\d+)?)\s*–º', params_text)
            if area_match:
                data['total_area'] = float(area_match.group(1).replace(',', '.'))

            # –ö–æ–º–Ω–∞—Ç—ã
            if '—Å—Ç—É–¥–∏—è' in params_text.lower():
                data['rooms'] = '—Å—Ç—É–¥–∏—è'
            else:
                rooms_match = re.search(r'(\d+)[- ]?–∫–æ–º–Ω', params_text)
                if rooms_match:
                    data['rooms'] = int(rooms_match.group(1))

            # –≠—Ç–∞–∂
            floor_match = re.search(r'(\d+)/(\d+)\s*—ç—Ç', params_text)
            if floor_match:
                data['floor'] = int(floor_match.group(1))
                data['floor_total'] = int(floor_match.group(2))

        # –ê–¥—Ä–µ—Å
        address_elem = card.find(attrs={'data-marker': 'item-address'})
        if not address_elem:
            address_elem = card.find('div', class_=re.compile(r'address|geo', re.I))

        if address_elem:
            data['address'] = address_elem.get_text(strip=True)

        # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—É –∑–∞ –º¬≤
        if data.get('price') and data.get('total_area'):
            data['price_per_sqm'] = round(data['price'] / data['total_area'], 2)

        return data

    # ===== –í–û–ó–ú–û–ñ–ù–û–°–¢–ò =====

    def get_capabilities(self) -> ParserCapabilities:
        """–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–∞—Ä—Å–µ—Ä–∞"""
        return ParserCapabilities(
            supports_search=True,  # ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ HTML –ø–∞—Ä—Å–∏–Ω–≥
            supports_residential_complex=False,  # –ê–≤–∏—Ç–æ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –ñ–ö
            supports_regions=['msk', 'spb'],
            supports_async=False,
            has_api=True,  # –ú–æ–±–∏–ª—å–Ω–æ–µ API
            requires_browser=True  # Nodriver –¥–ª—è –æ–±—Ö–æ–¥–∞ DataDome
        )

    def get_source_name(self) -> str:
        """–ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return 'avito'
