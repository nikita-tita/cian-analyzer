#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü Cian –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
"""

from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import json
import re
import time

def analyze_detail_page(url: str):
    """
    –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è
    """
    print("=" * 80)
    print(f"–ê–ù–ê–õ–ò–ó –°–¢–†–ê–ù–ò–¶–´: {url}")
    print("=" * 80)

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        # –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ network –∑–∞–ø—Ä–æ—Å—ã
        api_requests = []

        def handle_request(request):
            if 'api' in request.url or 'ajax' in request.url:
                api_requests.append({
                    'url': request.url,
                    'method': request.method,
                    'headers': dict(request.headers)
                })

        page.on("request", handle_request)

        page.goto(url, wait_until='domcontentloaded', timeout=30000)
        time.sleep(2)

        html = page.content()
        browser.close()

    print(f"\nüìÑ –†–∞–∑–º–µ—Ä HTML: {len(html)} —Å–∏–º–≤–æ–ª–æ–≤")

    soup = BeautifulSoup(html, 'lxml')

    # 1. JSON-LD –¥–∞–Ω–Ω—ã–µ
    print("\n" + "=" * 80)
    print("1. JSON-LD –î–ê–ù–ù–´–ï")
    print("=" * 80)

    json_ld_scripts = soup.find_all('script', type='application/ld+json')
    for i, script in enumerate(json_ld_scripts, 1):
        try:
            data = json.loads(script.string)
            print(f"\nJSON-LD –±–ª–æ–∫ {i}:")
            print(f"  @type: {data.get('@type')}")
            if data.get('@type') == 'Apartment':
                print(f"  name: {data.get('name', 'N/A')[:100]}")
                print(f"  address: {data.get('address', 'N/A')}")
                if 'geo' in data:
                    print(f"  coordinates: {data['geo']}")
                if 'offers' in data:
                    print(f"  price: {data['offers'].get('price')}")
        except:
            pass

    # 2. NextData (–¥–∞–Ω–Ω—ã–µ –¥–ª—è React)
    print("\n" + "=" * 80)
    print("2. NEXT DATA (React State)")
    print("=" * 80)

    next_data = soup.find('script', id='__NEXT_DATA__')
    if next_data:
        try:
            data = json.loads(next_data.string)
            print("\n‚úÖ –ù–∞–π–¥–µ–Ω __NEXT_DATA__")

            # –ò—â–µ–º –¥–∞–Ω–Ω—ã–µ –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏
            props = data.get('props', {})
            initial_state = props.get('initialState', {})

            # –í—ã–≤–æ–¥–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            print(f"\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:")
            for key in initial_state.keys():
                print(f"  - {key}")

            # –ò—â–µ–º offerData –∏–ª–∏ –ø–æ—Ö–æ–∂–µ–µ
            if 'offerData' in initial_state:
                offer = initial_state['offerData']
                print(f"\nüìç offerData –Ω–∞–π–¥–µ–Ω!")
                print(f"  ID: {offer.get('id', 'N/A')}")
                print(f"  –¢–∏–ø: {offer.get('offerType', 'N/A')}")

                # –ì–µ–æ–¥–∞–Ω–Ω—ã–µ
                if 'geo' in offer:
                    geo = offer['geo']
                    print(f"\nüåç –ì–µ–æ–¥–∞–Ω–Ω—ã–µ:")
                    print(f"  coordinates: {geo.get('coordinates', 'N/A')}")
                    print(f"  address: {geo.get('address', 'N/A')}")
                    if 'underground' in geo:
                        print(f"  –º–µ—Ç—Ä–æ: {geo.get('underground', [])[:2]}")

                # –ó–¥–∞–Ω–∏–µ
                if 'building' in offer:
                    building = offer['building']
                    print(f"\nüè¢ –ó–¥–∞–Ω–∏–µ:")
                    print(f"  ID: {building.get('id', 'N/A')}")
                    print(f"  floorsCount: {building.get('floorsCount', 'N/A')}")
                    print(f"  buildYear: {building.get('buildYear', 'N/A')}")
                    print(f"  materialType: {building.get('materialType', 'N/A')}")

                # –ñ–ö
                if 'newbuilding' in offer:
                    nb = offer['newbuilding']
                    print(f"\nüèóÔ∏è –ñ–ö (Newbuilding):")
                    print(f"  ID: {nb.get('id', 'N/A')}")
                    print(f"  name: {nb.get('name', 'N/A')}")
                    print(f"  fullName: {nb.get('fullName', 'N/A')}")

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                with open('offer_data_full.json', 'w', encoding='utf-8') as f:
                    json.dump(offer, f, ensure_ascii=False, indent=2)
                print(f"\nüíæ –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ offer_data_full.json")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ __NEXT_DATA__: {e}")
    else:
        print("‚ö†Ô∏è __NEXT_DATA__ –Ω–µ –Ω–∞–π–¥–µ–Ω")

    # 3. –ú–µ—Ç–∞-—Ç–µ–≥–∏
    print("\n" + "=" * 80)
    print("3. –ú–ï–¢–ê-–¢–ï–ì–ò")
    print("=" * 80)

    meta_tags = {
        'og:title': soup.find('meta', property='og:title'),
        'og:url': soup.find('meta', property='og:url'),
        'og:description': soup.find('meta', property='og:description'),
        'cian:offer_id': soup.find('meta', {'name': 'cian:offer_id'}),
    }

    for name, tag in meta_tags.items():
        if tag and tag.get('content'):
            content = tag['content']
            print(f"  {name}: {content[:100]}")

    # 4. Data –∞—Ç—Ä–∏–±—É—Ç—ã
    print("\n" + "=" * 80)
    print("4. DATA –ê–¢–†–ò–ë–£–¢–´")
    print("=" * 80)

    # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å data-* –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏
    data_attrs = set()
    for elem in soup.find_all(attrs={'data-name': True}):
        data_attrs.add(elem.get('data-name'))

    print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(data_attrs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö data-name:")
    for attr in sorted(data_attrs)[:20]:
        print(f"  - {attr}")

    # 5. API –∑–∞–ø—Ä–æ—Å—ã
    print("\n" + "=" * 80)
    print("5. API –ó–ê–ü–†–û–°–´")
    print("=" * 80)

    if api_requests:
        print(f"\n–ü–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–æ {len(api_requests)} API –∑–∞–ø—Ä–æ—Å–æ–≤:")
        for req in api_requests[:10]:
            print(f"\n  {req['method']} {req['url'][:100]}")
    else:
        print("‚ö†Ô∏è API –∑–∞–ø—Ä–æ—Å—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã")

    # 6. Breadcrumbs
    print("\n" + "=" * 80)
    print("6. BREADCRUMBS (—Ö–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏)")
    print("=" * 80)

    breadcrumbs = soup.find('div', {'data-name': 'Breadcrumbs'})
    if breadcrumbs:
        links = breadcrumbs.find_all('a')
        print(f"\n–ù–∞–π–¥–µ–Ω–æ {len(links)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤:")
        for link in links:
            text = link.get_text(strip=True)
            href = link.get('href', '')
            print(f"  - {text} ‚Üí {href[:80]}")
    else:
        print("‚ö†Ô∏è Breadcrumbs –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    print("\n" + "=" * 80)
    print("–ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 80)


def test_search_by_address():
    """
    –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –ø–æ–∏—Å–∫–∞ –ø–æ –∞–¥—Ä–µ—Å—É
    """
    print("\n\n")
    print("=" * 80)
    print("–¢–ï–°–¢: –ü–æ–∏—Å–∫ –ø–æ –∞–¥—Ä–µ—Å—É")
    print("=" * 80)

    test_addresses = [
        "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –°–≤–µ—Ç–ª–∞–Ω–æ–≤—Å–∫–∏–π –ø—Ä–æ—Å–ø–µ–∫—Ç, 60",
        "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥, –ù–µ–≤—Å–∫–∏–π –ø—Ä–æ—Å–ø–µ–∫—Ç",
        "—É–ª–∏—Ü–∞ –ê—Ä—Ö–∏–≤–Ω–∞—è, 3",
    ]

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)

        for address in test_addresses[:1]:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            print(f"\n\nüîç –ü–æ–∏—Å–∫: {address}")
            print("-" * 80)

            # –í–∞—Ä–∏–∞–Ω—Ç 1: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
            import urllib.parse
            encoded = urllib.parse.quote(address)
            url = f"https://www.cian.ru/cat.php?deal_type=sale&offer_type=flat&engine_version=2&region=2&text={encoded}"

            print(f"\nURL: {url[:120]}...")

            page = browser.new_page()
            page.goto(url, wait_until='domcontentloaded', timeout=30000)
            time.sleep(2)

            html = page.content()
            soup = BeautifulSoup(html, 'lxml')

            # –°—á–∏—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            cards = soup.find_all('article', {'data-name': 'CardComponent'})
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(cards)}")

            if cards:
                print(f"\n–ü–µ—Ä–≤—ã–µ 3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞:")
                for i, card in enumerate(cards[:3], 1):
                    title_elem = card.find('span', {'data-mark': 'OfferTitle'})
                    title = title_elem.get_text(strip=True) if title_elem else "N/A"

                    geo_labels = card.find_all('a', {'data-name': 'GeoLabel'})
                    addr = ', '.join([g.get_text(strip=True) for g in geo_labels]) if geo_labels else "N/A"

                    print(f"\n  {i}. {title[:60]}")
                    print(f"     –ê–¥—Ä–µ—Å: {addr[:100]}")

            page.close()

        browser.close()


if __name__ == '__main__':
    # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–π URL –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    test_url = "https://spb.cian.ru/sale/flat/309818461/"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π

    print("\nüöÄ –ù–ê–ß–ê–õ–û –ê–ù–ê–õ–ò–ó–ê –°–¢–†–£–ö–¢–£–†–´ CIAN\n")

    # 1. –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±—ä—è–≤–ª–µ–Ω–∏—è
    analyze_detail_page(test_url)

    # 2. –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞
    test_search_by_address()

    print("\n\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
    print("\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª offer_data_full.json –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
